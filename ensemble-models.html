<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>10 Ensemble Models | MATH482: Statistical Learning</title>
  <meta name="description" content="10 Ensemble Models | MATH482: Statistical Learning" />
  <meta name="generator" content="bookdown 0.44 and GitBook 2.6.7" />

  <meta property="og:title" content="10 Ensemble Models | MATH482: Statistical Learning" />
  <meta property="og:type" content="book" />
  
  
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="10 Ensemble Models | MATH482: Statistical Learning" />
  
  
  

<meta name="author" content="David P. Hofmeyr" />



  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="nonlinearity2.html"/>

<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { display: inline-block; text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<style type="text/css">
  
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
</style>

</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li class="chapter" data-level="1" data-path="introduction-to-r.html"><a href="introduction-to-r.html"><i class="fa fa-check"></i><b>1</b> Introduction to R</a>
<ul>
<li class="chapter" data-level="1.1" data-path="introduction-to-r.html"><a href="introduction-to-r.html#background-on-r"><i class="fa fa-check"></i><b>1.1</b> Background on R</a></li>
<li class="chapter" data-level="1.2" data-path="introduction-to-r.html"><a href="introduction-to-r.html#getting-started-with-rstudio"><i class="fa fa-check"></i><b>1.2</b> Getting started with RStudio</a></li>
<li class="chapter" data-level="1.3" data-path="introduction-to-r.html"><a href="introduction-to-r.html#statements-expressions-and-objects"><i class="fa fa-check"></i><b>1.3</b> Statements, expressions and objects</a></li>
<li class="chapter" data-level="1.4" data-path="introduction-to-r.html"><a href="introduction-to-r.html#basic-plots"><i class="fa fa-check"></i><b>1.4</b> Basic plots</a></li>
<li class="chapter" data-level="1.5" data-path="introduction-to-r.html"><a href="introduction-to-r.html#r-scripts"><i class="fa fa-check"></i><b>1.5</b> R scripts</a></li>
<li class="chapter" data-level="1.6" data-path="introduction-to-r.html"><a href="introduction-to-r.html#r-markdown"><i class="fa fa-check"></i><b>1.6</b> R markdown</a></li>
<li class="chapter" data-level="1.7" data-path="introduction-to-r.html"><a href="introduction-to-r.html#functions"><i class="fa fa-check"></i><b>1.7</b> Functions</a></li>
<li class="chapter" data-level="1.8" data-path="introduction-to-r.html"><a href="introduction-to-r.html#s:0help"><i class="fa fa-check"></i><b>1.8</b> Getting Help</a></li>
<li class="chapter" data-level="1.9" data-path="introduction-to-r.html"><a href="introduction-to-r.html#loops-and-flow"><i class="fa fa-check"></i><b>1.9</b> Loops and Flow</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="working-with-data-in-r.html"><a href="working-with-data-in-r.html"><i class="fa fa-check"></i><b>2</b> Working with Data in R</a>
<ul>
<li class="chapter" data-level="2.1" data-path="working-with-data-in-r.html"><a href="working-with-data-in-r.html#data-frames"><i class="fa fa-check"></i><b>2.1</b> Data Frames</a></li>
<li class="chapter" data-level="2.2" data-path="working-with-data-in-r.html"><a href="working-with-data-in-r.html#importing-and-exporting-data"><i class="fa fa-check"></i><b>2.2</b> Importing and Exporting Data</a></li>
<li class="chapter" data-level="2.3" data-path="working-with-data-in-r.html"><a href="working-with-data-in-r.html#data-plotting"><i class="fa fa-check"></i><b>2.3</b> Data Plotting</a></li>
<li class="chapter" data-level="2.4" data-path="working-with-data-in-r.html"><a href="working-with-data-in-r.html#summary-2"><i class="fa fa-check"></i><b>2.4</b> Summary</a></li>
<li class="chapter" data-level="2.5" data-path="working-with-data-in-r.html"><a href="working-with-data-in-r.html#exercises-3"><i class="fa fa-check"></i><b>2.5</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="background.html"><a href="background.html"><i class="fa fa-check"></i><b>3</b> Statistical Background</a>
<ul>
<li class="chapter" data-level="3.1" data-path="background.html"><a href="background.html#probability-basics"><i class="fa fa-check"></i><b>3.1</b> Probability Basics</a></li>
<li class="chapter" data-level="3.2" data-path="background.html"><a href="background.html#random-variables"><i class="fa fa-check"></i><b>3.2</b> Random Variables</a></li>
<li class="chapter" data-level="3.3" data-path="background.html"><a href="background.html#samples-and-statistical-modelling"><i class="fa fa-check"></i><b>3.3</b> Samples and Statistical Modelling</a></li>
<li class="chapter" data-level="3.4" data-path="background.html"><a href="background.html#statistical-estimation"><i class="fa fa-check"></i><b>3.4</b> Statistical Estimation</a></li>
<li class="chapter" data-level="3.5" data-path="background.html"><a href="background.html#multivariate-random-variables-and-dependence"><i class="fa fa-check"></i><b>3.5</b> Multivariate Random Variables and Dependence</a></li>
<li class="chapter" data-level="3.6" data-path="background.html"><a href="background.html#summary-3"><i class="fa fa-check"></i><b>3.6</b> Summary</a></li>
<li class="chapter" data-level="3.7" data-path="background.html"><a href="background.html#exercises-4"><i class="fa fa-check"></i><b>3.7</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="fundamentals1.html"><a href="fundamentals1.html"><i class="fa fa-check"></i><b>4</b> The Fundamentals of Predictive Modelling I</a>
<ul>
<li class="chapter" data-level="4.1" data-path="fundamentals1.html"><a href="fundamentals1.html#two-archetypal-problems"><i class="fa fa-check"></i><b>4.1</b> Two Archetypal Problems</a></li>
<li class="chapter" data-level="4.2" data-path="fundamentals1.html"><a href="fundamentals1.html#some-preliminaries"><i class="fa fa-check"></i><b>4.2</b> Some Preliminaries</a></li>
<li class="chapter" data-level="4.3" data-path="fundamentals1.html"><a href="fundamentals1.html#model-training"><i class="fa fa-check"></i><b>4.3</b> Model Training</a></li>
<li class="chapter" data-level="4.4" data-path="fundamentals1.html"><a href="fundamentals1.html#summary-4"><i class="fa fa-check"></i><b>4.4</b> Summary</a></li>
<li class="chapter" data-level="4.5" data-path="fundamentals1.html"><a href="fundamentals1.html#exercises-5"><i class="fa fa-check"></i><b>4.5</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="fundamentals2.html"><a href="fundamentals2.html"><i class="fa fa-check"></i><b>5</b> The Fundamentals of Predictive Modelling II</a>
<ul>
<li class="chapter" data-level="5.1" data-path="fundamentals2.html"><a href="fundamentals2.html#a-quick-recap"><i class="fa fa-check"></i><b>5.1</b> A Quick Recap</a></li>
<li class="chapter" data-level="5.2" data-path="fundamentals2.html"><a href="fundamentals2.html#overfitting"><i class="fa fa-check"></i><b>5.2</b> Overfitting</a></li>
<li class="chapter" data-level="5.3" data-path="fundamentals2.html"><a href="fundamentals2.html#prediction-error-and-generalisation"><i class="fa fa-check"></i><b>5.3</b> Prediction Error and Generalisation</a></li>
<li class="chapter" data-level="5.4" data-path="fundamentals2.html"><a href="fundamentals2.html#estimating-expected-prediction-error"><i class="fa fa-check"></i><b>5.4</b> Estimating (Expected) Prediction Error</a></li>
<li class="chapter" data-level="5.5" data-path="fundamentals2.html"><a href="fundamentals2.html#summary-5"><i class="fa fa-check"></i><b>5.5</b> Summary</a></li>
<li class="chapter" data-level="5.6" data-path="fundamentals2.html"><a href="fundamentals2.html#exercises-6"><i class="fa fa-check"></i><b>5.6</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="linear.html"><a href="linear.html"><i class="fa fa-check"></i><b>6</b> Linear Regression: Ordinary Least Squares and Regularised Variants</a>
<ul>
<li class="chapter" data-level="6.1" data-path="linear.html"><a href="linear.html#the-linear-model"><i class="fa fa-check"></i><b>6.1</b> The Linear Model</a></li>
<li class="chapter" data-level="6.2" data-path="linear.html"><a href="linear.html#ordinary-least-squares"><i class="fa fa-check"></i><b>6.2</b> Ordinary Least Squares</a></li>
<li class="chapter" data-level="6.3" data-path="linear.html"><a href="linear.html#regularisation-for-linear-models"><i class="fa fa-check"></i><b>6.3</b> Regularisation for Linear Models</a></li>
<li class="chapter" data-level="6.4" data-path="linear.html"><a href="linear.html#summary-6"><i class="fa fa-check"></i><b>6.4</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="glms.html"><a href="glms.html"><i class="fa fa-check"></i><b>7</b> Generalised Linear Models</a>
<ul>
<li class="chapter" data-level="7.1" data-path="glms.html"><a href="glms.html#generalised-predictive-modelling"><i class="fa fa-check"></i><b>7.1</b> Generalised Predictive Modelling</a></li>
<li class="chapter" data-level="7.2" data-path="glms.html"><a href="glms.html#classification-and-logistic-regression"><i class="fa fa-check"></i><b>7.2</b> Classification and Logistic Regression</a></li>
<li class="chapter" data-level="7.3" data-path="glms.html"><a href="glms.html#summary-7"><i class="fa fa-check"></i><b>7.3</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="nonlinear1.html"><a href="nonlinear1.html"><i class="fa fa-check"></i><b>8</b> Nonlinearity Part I</a>
<ul>
<li class="chapter" data-level="8.1" data-path="nonlinear1.html"><a href="nonlinear1.html#basis-expansions"><i class="fa fa-check"></i><b>8.1</b> Basis Expansions</a></li>
<li class="chapter" data-level="8.2" data-path="nonlinear1.html"><a href="nonlinear1.html#kernels-and-reproducing-kernel-hilbert-spaces"><i class="fa fa-check"></i><b>8.2</b> Kernels and Reproducing Kernel Hilbert Spaces</a></li>
<li class="chapter" data-level="8.3" data-path="nonlinear1.html"><a href="nonlinear1.html#support-vector-machines"><i class="fa fa-check"></i><b>8.3</b> Support Vector Machines</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="nonlinearity2.html"><a href="nonlinearity2.html"><i class="fa fa-check"></i><b>9</b> Nonlinearity Part II</a>
<ul>
<li class="chapter" data-level="9.1" data-path="nonlinearity2.html"><a href="nonlinearity2.html#smoothing-as-local-averaging"><i class="fa fa-check"></i><b>9.1</b> Smoothing as Local Averaging</a></li>
<li class="chapter" data-level="9.2" data-path="nonlinearity2.html"><a href="nonlinearity2.html#nearest-neighbours"><i class="fa fa-check"></i><b>9.2</b> Nearest Neighbours</a></li>
<li class="chapter" data-level="9.3" data-path="nonlinearity2.html"><a href="nonlinearity2.html#decision-trees"><i class="fa fa-check"></i><b>9.3</b> Decision Trees</a></li>
<li class="chapter" data-level="9.4" data-path="nonlinearity2.html"><a href="nonlinearity2.html#summary-8"><i class="fa fa-check"></i><b>9.4</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="ensemble-models.html"><a href="ensemble-models.html"><i class="fa fa-check"></i><b>10</b> Ensemble Models</a>
<ul>
<li class="chapter" data-level="10.1" data-path="ensemble-models.html"><a href="ensemble-models.html#bagging"><i class="fa fa-check"></i><b>10.1</b> Bagging</a></li>
<li class="chapter" data-level="10.2" data-path="ensemble-models.html"><a href="ensemble-models.html#boosting"><i class="fa fa-check"></i><b>10.2</b> Boosting</a></li>
<li class="chapter" data-level="10.3" data-path="ensemble-models.html"><a href="ensemble-models.html#variable-importance"><i class="fa fa-check"></i><b>10.3</b> Variable Importance</a></li>
<li class="chapter" data-level="10.4" data-path="ensemble-models.html"><a href="ensemble-models.html#summary-9"><i class="fa fa-check"></i><b>10.4</b> Summary</a></li>
</ul></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">MATH482: Statistical Learning</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="ensemble-models" class="section level1 hasAnchor" number="10">
<h1><span class="header-section-number">10</span> Ensemble Models<a href="ensemble-models.html#ensemble-models" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<p><span class="math display">\[
\def\x{\mathbf{x}}
\def\hg{\hat g}
\def\F{\mathcal{F}}
\def\bbeta{\boldsymbol{\beta}}
\newcommand{\argmin}{\mathop{\rm argmin}}
\newcommand{\argmax}{\mathop{\rm argmax}}
\]</span></p>
<p>In this final chapter we will be looking at some of the most influential
groups of models of the last twenty to thirty years. <em>Ensemble models</em>
are seen by many as the <em>state of the art</em> when it comes to predictive
modelling on so-called “tabular data”. Although neural nets have risen
to prominence in problems involving “unstructured data”, such as textual
data, as well as “highly structured data”, such as images, in the
“vector of covariates aligned with response variables” (this is what we
mean by “tabular” data) context ensemble models are very often the most
performant.</p>
<p>The basic idea behind ensemble modelling is to combine the
outputs/predictions of multiple (sometimes a very large number of)
“weaker” models (sometimes called <em>base learners</em>) in order to produce a
final prediction. We will look at what are very likely the two most
fundamental ensemble approaches, known as <em>bagging</em> (bootstrap
aggregating) and <em>boosting</em>. Whereas bagging fits the base learners
independently of one another, boosting is a sequential approach in which
each base learner in the ensemble is fit to improve on what the ensemble
comprising the base learners fit so far are predicting.</p>
<p>Although ensemble models are heralded for their performance in terms of
accuracy, there is a sacrifice on the side of interpretability. In
addition because a large number of individual models needs to be fit for
each ensemble they are considered computationally intensive methods.</p>
<div id="bagging" class="section level2 hasAnchor" number="10.1">
<h2><span class="header-section-number">10.1</span> Bagging<a href="ensemble-models.html#bagging" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>As alluded to above bagging is a bootstrap based procedure, but unlike
in the standard bootstrap procedure we saw before where we used the
bootstrap in order to understand the sampling distribution of an
estimator, bagging is used to actually construct the estimator itself.
Returning to the notation we used in Chapter <a href="fundamentals2.html#fundamentals2">5</a> where
<span class="math inline">\(\hg^T\)</span> refers to a particular model trained on a training set <span class="math inline">\(T\)</span>, the
main steps in a bagging procedure are:</p>
<ul>
<li><p>For <span class="math inline">\(b = 1, 2, ..., B\)</span> do:</p>
<ul>
<li><p>Resample from the training set
<span class="math inline">\(T = \{(y_1, \x_1), ..., (y_n, \x_n)\}\)</span>, to obtain a bootstrap
sample <span class="math inline">\(T_b\)</span></p></li>
<li><p>Fit a base learner <span class="math inline">\(\hg^{T_b}\)</span> on the bootstrap sample</p></li>
</ul></li>
<li><p>Combine the base learners for final prediction by defining <span class="math display">\[
\hg^T_{bag}(\x) = \frac{1}{B}\sum_{b=1}^B \hg^{T_b}(\x)
\]</span></p></li>
</ul>
<p><strong>A Quick Aside</strong></p>
<p><em>In the above we have expressed the bagged prediction as an average of
the predictions from the base learners. This formulation makes the most
immediate sense for the regression setting, however in the
classification setting we know it does not make sense to average class
labels (recall that the average of a cat and a goat is not a dog!). We
will come back to this point a little later on, but for now we will work
with the averaging framework above.</em></p>
<div id="variance-reduction-through-model-averaging" class="section level3 hasAnchor" number="10.1.1">
<h3><span class="header-section-number">10.1.1</span> Variance Reduction through Model Averaging<a href="ensemble-models.html#variance-reduction-through-model-averaging" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>The fundamental reason for performing bagging is that averaging reduces
variance. We are well aware that the variance and bias of a model are
ultimately what drive its success as a predictive model, and strategies
for reducing the variance (regularisation strategies) can be very
beneficial if the increase in bias is much less than the reduction in
variance. So let’s examine these a little, and how they compare with
simply fitting our base learner to the entire training set to obtain
<span class="math inline">\(\hg^T\)</span>.</p>
<p>It is relatively straightforward to show that the variance of the bagged
prediction (as it is expressed above as an average) satisfies</p>
<p><span class="math display">\[
Var\left(\hg^T_{bag}(\x)\right) = Var\left(\hg^{T_1}(\x)\right)\left(\rho + \frac{1-\rho}{B}\right),
\]</span> where the quantity <span class="math inline">\(\rho\)</span> is the correlation between <span class="math inline">\(\hg^{T_1}(\x)\)</span>
and <span class="math inline">\(\hg^{T_2}(\x)\)</span>.</p>
<ul>
<li>Why have we used specifically bootstrap samples <span class="math inline">\(T_1\)</span> and <span class="math inline">\(T_2\)</span> in
the above? Those who have done some statistics before may be
familiar with the reason, and it is simply because the joint
distribution of <span class="math inline">\(\hg^{T_i}\)</span> and <span class="math inline">\(\hg^{T_j}\)</span> for any <span class="math inline">\(i \not = j\)</span> is
the same. This means that <span class="math inline">\(Var(\hg^{T_i}(\x))\)</span> is equal for all <span class="math inline">\(i\)</span>
and <span class="math inline">\(Cor(\hg^{T_i}(\x), \hg^{T_j}(\x))\)</span> is the same for all <span class="math inline">\(i, j\)</span>
as long as <span class="math inline">\(i \not = j\)</span>. So the answer is that we could have used
<em>any</em> pair, not necessarily the first and second bootstrap samples.</li>
</ul>
<p>Let’s take a minute to unpack the expression above. The first thing to
notice is that in principle we can always increase <span class="math inline">\(B\)</span>, and so the term
<span class="math inline">\((1-\rho)/B\)</span> doesn’t matter much except that for <span class="math inline">\(B\)</span> to be <em>very</em> large
it may take a lot of computational effort to fit so many models. From a
statistical perspective the <em>dominant</em> term is
<span class="math inline">\(\rho Var(\hg^{T_1}(\x))\)</span>. Typically the variance of <span class="math inline">\(\hg^{T_1}(\x)\)</span>
will be higher than that of <span class="math inline">\(\hg^T(\x)\)</span> since there is less information
in a bootstrap sample than there is in the training set, as some of the
observations will be repeated and not all observations will be present.
The amount by which <span class="math inline">\(Var(\hg^{T_1}(\x))\)</span> exceeds <span class="math inline">\(Var(\hg^T(\x))\)</span> will
depend on the base learner, but typically it is not a massive
difference. What is more crucial is whether or not <span class="math inline">\(\rho\)</span> is close to
one or whether there are base learners for which it is actually much
smaller. Firstly, the reason why <span class="math inline">\(\rho\)</span> is not simply zero, is that
pairs bootstrap samples <em>overlap</em>, and a rough estimate is that about
two thirds of the information in a bootstrap sample will overlap with
another. Herein lies the beauty of bagging. At a high level we may think
of <span class="math inline">\(\rho\)</span> as capturing how similar the predictions tend to be from base
learners fit to two bootstrap samples. Inflexible (low variance) models
will typically be fairly “stable”, in that the two-thirds overlap in two
bootstrap samples will make the predictions from the associated base
learners quite similar (i.e. high correlation). It is actually the more
flexible, high variance models, which typically have lower correlation
because they fit closer to the specific variations in their samples, and
so the one-third non-overlapping part actually influences the
predictions from the model more. So the reduction in variance from
bagging is typically more pronounced from particularly flexible, high
variance models. What this means is that we can exploit the low bias,
high flexibility models with relative impunity when using them as base
learners within a <em>bagged ensemble</em>. We benefit from their low bias, but
don’t suffer so much from their high variance.</p>
<p>The kernelised linear models (including SVMs) as well as nonparametric
smoothing models (plus some models we have not covered in this module)
can all be made extremely flexible, however decision trees have
undeniably become the most successful models to use within a bagged
ensemble, to the extent that bagging is often categorised as a decision
tree based approach.</p>
</div>
<div id="why-bag-decision-trees" class="section level3 hasAnchor" number="10.1.2">
<h3><span class="header-section-number">10.1.2</span> Why Bag Decision Trees?<a href="ensemble-models.html#why-bag-decision-trees" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>There is a number of reasons why decision trees have become almost
ubiquitous within bagging.</p>
<ol style="list-style-type: decimal">
<li>Flexibility: They are <em>universal approximators</em> meaning that with
enough splits that can fit arbitrarily close to any function.
Although there are other models which are also universal
approximators, decision trees are often able to capture extremely
complex interactions between covariates with comparative “ease”.</li>
<li>Computation: Fitting decision trees is extremely computationally
efficient, especially compared with other highly flexible models.
This is very important since within bagged ensembles we typically
use of the order of <em>hundreds</em> of base learners.</li>
<li>Instability: Whereas the comparatively high variance of deep
decision trees, for a similarly low bias to other flexible models,
is usually a disadvantage, within the context of bagging it actually
leads to lower correlation between the predictions from trees fit on
bootstrapped samples and is overall usually an advantage in this
context.</li>
</ol>
<div id="bagging-classifiers" class="section level4 hasAnchor" number="10.1.2.1">
<h4><span class="header-section-number">10.1.2.1</span> Bagging Classifiers<a href="ensemble-models.html#bagging-classifiers" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>When it comes to making predictions from a combination of regression
models, taking the average is sensible. When predicting combining the
outputs of classification models, however, taking the average of the
predicted labels is clearly not sensible. Typically instead we use what
is called <em>majority voting</em>, and assign the final predicted class label
to be that which is predicted most often by the ensemble. Specifically,
we have <span class="math display">\[
\hg^T_{bag}(\x) = \argmax_k \sum_{b=1}^B I(\hg^{T_b}(\x) = k),
\]</span> where the function <span class="math inline">\(I\)</span> is called the <em>indicator function</em> and takes
the value one if its argument is true and zero otherwise. It is worth
noting that the majority voting formulation can be expressed as an average, and so if you find the discussion related to the variance reduction from averaging particularly persuasive, you can rest assured it applies here too.</p>
</div>
<div id="out-of-bag-oob-error" class="section level4 hasAnchor" number="10.1.2.2">
<h4><span class="header-section-number">10.1.2.2</span> Out-Of-Bag (OOB) Error<a href="ensemble-models.html#out-of-bag-oob-error" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>One of the additional benefits of bagging is that we get a few things “for free”. Recall that within each bootstrap sample there will be some observations which are excluded. In fact each observation is “out of bag” in roughly one third of the bootstrap samples. What this means is that for, say the <span class="math inline">\(i\)</span>-th observation, we actually have an ensemble of roughly <span class="math inline">\(B/3\)</span> models which were not trained using <span class="math inline">\((y_i, \x_i)\)</span> and so this point can act sort of like a validation point for this ensemble. Perhaps more precisely, for each observation we can obtain a prediction from an ensemble of roughly <span class="math inline">\(B/3\)</span> models which didn’t have access to the observation for training.
By comparing the actual responses in the training data with these <em>out-of-bag predictions</em> gives a fairly reliable estimate of the generalisation performance, and can be used as a much more computationally efficient alternative to cross validation in bagging.</p>
<p><strong>Bagged Decision Trees in R</strong></p>
<p>The <code>bagging</code> function in the <code>ipred</code> package allows us to fit bagged
predictive models within R. Although standard bagged tree models can be
linked using <code>caret</code>, this particular implementation does not allow for
hyperparameter tuning.</p>
<p>Let’s use the <code>satimage</code> data set again, for a comparison with the single decision trees we fit in the previous chapter.</p>
<div class="sourceCode" id="cb505"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb505-1"><a href="ensemble-models.html#cb505-1" tabindex="-1"></a><span class="do">### load our libraries</span></span>
<span id="cb505-2"><a href="ensemble-models.html#cb505-2" tabindex="-1"></a><span class="fu">library</span>(ipred)</span>
<span id="cb505-3"><a href="ensemble-models.html#cb505-3" tabindex="-1"></a><span class="fu">library</span>(pmlbr)</span>
<span id="cb505-4"><a href="ensemble-models.html#cb505-4" tabindex="-1"></a><span class="fu">library</span>(caret)</span>
<span id="cb505-5"><a href="ensemble-models.html#cb505-5" tabindex="-1"></a><span class="fu">library</span>(rpart)</span>
<span id="cb505-6"><a href="ensemble-models.html#cb505-6" tabindex="-1"></a></span>
<span id="cb505-7"><a href="ensemble-models.html#cb505-7" tabindex="-1"></a><span class="do">### Fetch the satimage data set and convert response to a factor variable</span></span>
<span id="cb505-8"><a href="ensemble-models.html#cb505-8" tabindex="-1"></a>satimage <span class="ot">&lt;-</span> <span class="fu">fetch_data</span>(<span class="st">&quot;satimage&quot;</span>)</span></code></pre></div>
<pre><code>## Download successful.</code></pre>
<div class="sourceCode" id="cb507"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb507-1"><a href="ensemble-models.html#cb507-1" tabindex="-1"></a>satimage<span class="sc">$</span>target <span class="ot">&lt;-</span> <span class="fu">as.factor</span>(satimage<span class="sc">$</span>target)</span>
<span id="cb507-2"><a href="ensemble-models.html#cb507-2" tabindex="-1"></a></span>
<span id="cb507-3"><a href="ensemble-models.html#cb507-3" tabindex="-1"></a><span class="do">### Create training/test split</span></span>
<span id="cb507-4"><a href="ensemble-models.html#cb507-4" tabindex="-1"></a>train_ix <span class="ot">&lt;-</span> <span class="fu">createDataPartition</span>(satimage<span class="sc">$</span>target, <span class="at">p =</span> <span class="fl">0.7</span>, <span class="at">list =</span> <span class="cn">FALSE</span>)</span>
<span id="cb507-5"><a href="ensemble-models.html#cb507-5" tabindex="-1"></a></span>
<span id="cb507-6"><a href="ensemble-models.html#cb507-6" tabindex="-1"></a>satimage.tr <span class="ot">&lt;-</span> satimage[train_ix,]</span>
<span id="cb507-7"><a href="ensemble-models.html#cb507-7" tabindex="-1"></a>satimage.te <span class="ot">&lt;-</span> satimage[<span class="sc">-</span>train_ix,]</span>
<span id="cb507-8"><a href="ensemble-models.html#cb507-8" tabindex="-1"></a></span>
<span id="cb507-9"><a href="ensemble-models.html#cb507-9" tabindex="-1"></a><span class="do">### The bagging function links to rpart and so we need to provide</span></span>
<span id="cb507-10"><a href="ensemble-models.html#cb507-10" tabindex="-1"></a><span class="do">### an rpart.control object. Since we know that bagging can effectively</span></span>
<span id="cb507-11"><a href="ensemble-models.html#cb507-11" tabindex="-1"></a><span class="do">### &quot;take care of&quot; overfitting we can use very deep trees. For instance</span></span>
<span id="cb507-12"><a href="ensemble-models.html#cb507-12" tabindex="-1"></a><span class="do">### it is common to set the minbucket parameter to a value between 1 and</span></span>
<span id="cb507-13"><a href="ensemble-models.html#cb507-13" tabindex="-1"></a><span class="do">### 5</span></span>
<span id="cb507-14"><a href="ensemble-models.html#cb507-14" tabindex="-1"></a></span>
<span id="cb507-15"><a href="ensemble-models.html#cb507-15" tabindex="-1"></a>control <span class="ot">&lt;-</span> <span class="fu">rpart.control</span>(<span class="at">minbucket =</span> <span class="dv">3</span>, <span class="at">cp =</span> <span class="dv">0</span>)</span>
<span id="cb507-16"><a href="ensemble-models.html#cb507-16" tabindex="-1"></a></span>
<span id="cb507-17"><a href="ensemble-models.html#cb507-17" tabindex="-1"></a><span class="do">### Now we can fit the bagged model</span></span>
<span id="cb507-18"><a href="ensemble-models.html#cb507-18" tabindex="-1"></a><span class="do">### The coob argument tells ipred whether or not</span></span>
<span id="cb507-19"><a href="ensemble-models.html#cb507-19" tabindex="-1"></a><span class="do">### we want to estimate the prediction error using</span></span>
<span id="cb507-20"><a href="ensemble-models.html#cb507-20" tabindex="-1"></a><span class="do">### out of bag error</span></span>
<span id="cb507-21"><a href="ensemble-models.html#cb507-21" tabindex="-1"></a>tree_bag <span class="ot">&lt;-</span> <span class="fu">bagging</span>(target<span class="sc">~</span>., satimage.tr, <span class="at">nbagg =</span> <span class="dv">100</span>,</span>
<span id="cb507-22"><a href="ensemble-models.html#cb507-22" tabindex="-1"></a>                    <span class="at">method =</span> <span class="st">&quot;standard&quot;</span>,</span>
<span id="cb507-23"><a href="ensemble-models.html#cb507-23" tabindex="-1"></a>                    <span class="at">coob =</span> <span class="cn">TRUE</span>,</span>
<span id="cb507-24"><a href="ensemble-models.html#cb507-24" tabindex="-1"></a>                    <span class="at">control =</span> control)</span>
<span id="cb507-25"><a href="ensemble-models.html#cb507-25" tabindex="-1"></a></span>
<span id="cb507-26"><a href="ensemble-models.html#cb507-26" tabindex="-1"></a><span class="do">### Recall that the multinomial regression model and the single</span></span>
<span id="cb507-27"><a href="ensemble-models.html#cb507-27" tabindex="-1"></a><span class="do">### decision tree had accuracy approximately 0.85</span></span>
<span id="cb507-28"><a href="ensemble-models.html#cb507-28" tabindex="-1"></a><span class="fu">confusionMatrix</span>(<span class="fu">predict</span>(tree_bag, satimage.te),</span>
<span id="cb507-29"><a href="ensemble-models.html#cb507-29" tabindex="-1"></a>                satimage.te<span class="sc">$</span>target)<span class="sc">$</span>overall[<span class="st">&#39;Accuracy&#39;</span>]</span></code></pre></div>
<pre><code>##  Accuracy 
## 0.9050337</code></pre>
<div class="sourceCode" id="cb509"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb509-1"><a href="ensemble-models.html#cb509-1" tabindex="-1"></a><span class="do">### The bagged model achieves roughly 5% more correct classifications</span></span>
<span id="cb509-2"><a href="ensemble-models.html#cb509-2" tabindex="-1"></a><span class="do">### which is very far from negligible. Although this will depend on the</span></span>
<span id="cb509-3"><a href="ensemble-models.html#cb509-3" tabindex="-1"></a><span class="do">### specific train/test split, on a data set of this size there is</span></span>
<span id="cb509-4"><a href="ensemble-models.html#cb509-4" tabindex="-1"></a><span class="do">### little variation in the preformance across training/test</span></span>
<span id="cb509-5"><a href="ensemble-models.html#cb509-5" tabindex="-1"></a><span class="do">### splits. We can also check the out of bag error from the model</span></span>
<span id="cb509-6"><a href="ensemble-models.html#cb509-6" tabindex="-1"></a><span class="do">### to see if it accurately represents the test error</span></span>
<span id="cb509-7"><a href="ensemble-models.html#cb509-7" tabindex="-1"></a><span class="do">### Printing the tree_bag object will show the out of bag error</span></span>
<span id="cb509-8"><a href="ensemble-models.html#cb509-8" tabindex="-1"></a>tree_bag  </span></code></pre></div>
<pre><code>## 
## Bagging classification trees with 100 bootstrap replications 
## 
## Call: bagging.data.frame(formula = target ~ ., data = satimage.tr, 
##     nbagg = 100, method = &quot;standard&quot;, coob = TRUE, control = control)
## 
## Out-of-bag estimate of misclassification error:  0.1009</code></pre>
<div class="sourceCode" id="cb511"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb511-1"><a href="ensemble-models.html#cb511-1" tabindex="-1"></a><span class="co"># Indeed the OOB error approximately of 0.1 closely matches</span></span>
<span id="cb511-2"><a href="ensemble-models.html#cb511-2" tabindex="-1"></a><span class="co"># the test error of 1-accuracy</span></span></code></pre></div>
</div>
</div>
<div id="random-forests" class="section level3 hasAnchor" number="10.1.3">
<h3><span class="header-section-number">10.1.3</span> Random Forests<a href="ensemble-models.html#random-forests" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Random Forests (RFs) have ultimately eclipsed the “standard” bagged
decision trees. RFs are themselves bagged ensembles of decision trees,
however there is a slight difference in how each of the individual trees
is fit. At a high level the steps taken to fit each tree are the same,
in that the greedy optimisation of the CART algorithm we saw in Chapter
<a href="nonlinearity2.html#nonlinearity2">9</a> is applied. However, whenever a node in a tree is
being split, rather than scanning <em>all</em> of the covariates to find the
best split, only a random subset of the covariates is scanned. This
randomisation can be seen as disrupting the way in which the trees are
fit, and leads to less similarity (and hence correlation) between the
trees fit on different bootstrap samples. Although this additional
randomisation slightly impacts on the quality (bias and variance) of
each of the trees, the reduction in the correlation typically overall
improves performance, and sometimes substantially so. The hyperparameter
used to specify the number of covariates available at each split in a
tree is typically called <code>mtry</code>, and any “standard” RF implementation
can be turned into a standard bagged ensemble of trees by simply setting
<code>mtry</code> equal to the number of covariates.</p>
<p>Because of their remarkable success, RFs have been implemented in a
large number of R packages. The <code>caret</code> package alone links to a
plethora of implementations, but we will be using the <code>randomForestSRC</code>
package, as a computationally efficient and accurate option.</p>
<div class="sourceCode" id="cb512"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb512-1"><a href="ensemble-models.html#cb512-1" tabindex="-1"></a><span class="do">### Load the randomForestSRC library</span></span>
<span id="cb512-2"><a href="ensemble-models.html#cb512-2" tabindex="-1"></a><span class="fu">library</span>(randomForestSRC)</span>
<span id="cb512-3"><a href="ensemble-models.html#cb512-3" tabindex="-1"></a></span>
<span id="cb512-4"><a href="ensemble-models.html#cb512-4" tabindex="-1"></a><span class="do">### Let&#39;s simply use the default settings</span></span>
<span id="cb512-5"><a href="ensemble-models.html#cb512-5" tabindex="-1"></a>rf <span class="ot">&lt;-</span> <span class="fu">rfsrc</span>(target<span class="sc">~</span>., <span class="at">data =</span> satimage.tr)</span>
<span id="cb512-6"><a href="ensemble-models.html#cb512-6" tabindex="-1"></a></span>
<span id="cb512-7"><a href="ensemble-models.html#cb512-7" tabindex="-1"></a><span class="do">### The predict function applied to an rfsrc is a list</span></span>
<span id="cb512-8"><a href="ensemble-models.html#cb512-8" tabindex="-1"></a><span class="do">### with fields $predicted which for regression is the final</span></span>
<span id="cb512-9"><a href="ensemble-models.html#cb512-9" tabindex="-1"></a><span class="do">### prediction of the response and for classification</span></span>
<span id="cb512-10"><a href="ensemble-models.html#cb512-10" tabindex="-1"></a><span class="do">### contains the proportion of trees which &quot;voted&quot; for</span></span>
<span id="cb512-11"><a href="ensemble-models.html#cb512-11" tabindex="-1"></a><span class="do">### each of the classes. To obtain the class predictions</span></span>
<span id="cb512-12"><a href="ensemble-models.html#cb512-12" tabindex="-1"></a><span class="do">### directly use the field $class</span></span>
<span id="cb512-13"><a href="ensemble-models.html#cb512-13" tabindex="-1"></a>rf_pred <span class="ot">&lt;-</span> <span class="fu">predict</span>(rf, satimage.te)<span class="sc">$</span>class</span>
<span id="cb512-14"><a href="ensemble-models.html#cb512-14" tabindex="-1"></a></span>
<span id="cb512-15"><a href="ensemble-models.html#cb512-15" tabindex="-1"></a><span class="do">### Now let&#39;s check the test accuracy</span></span>
<span id="cb512-16"><a href="ensemble-models.html#cb512-16" tabindex="-1"></a><span class="fu">confusionMatrix</span>(rf_pred, satimage.te<span class="sc">$</span>target)<span class="sc">$</span>overall[<span class="st">&#39;Accuracy&#39;</span>]</span></code></pre></div>
<pre><code>##  Accuracy 
## 0.9159315</code></pre>
<div class="sourceCode" id="cb514"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb514-1"><a href="ensemble-models.html#cb514-1" tabindex="-1"></a><span class="do">### To obtain the OOB error we can extract the OOB class predictions</span></span>
<span id="cb514-2"><a href="ensemble-models.html#cb514-2" tabindex="-1"></a><span class="do">### from the model and compare these with the actual class labels</span></span>
<span id="cb514-3"><a href="ensemble-models.html#cb514-3" tabindex="-1"></a><span class="fu">confusionMatrix</span>(rf<span class="sc">$</span>class.oob, satimage.tr<span class="sc">$</span>target)<span class="sc">$</span>overall[<span class="st">&#39;Accuracy&#39;</span>]</span></code></pre></div>
<pre><code>##  Accuracy 
## 0.9099379</code></pre>
</div>
</div>
<div id="boosting" class="section level2 hasAnchor" number="10.2">
<h2><span class="header-section-number">10.2</span> Boosting<a href="ensemble-models.html#boosting" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Boosting, like bagging, is an ensemble approach in that model
predictions are obtained by combining the predictions from multiple
smaller models (base learners). Like bagging, boosting can be achieved
using base learners other than decision trees, but decision trees have
nonetheless become a very popular choice in this context. Unlike in
bagging, however, in the case of boosting the individual base learners
are fit sequentially, and use information about the others which have
been fit so far. Because of this sequential learning, the base learner
added at the <span class="math inline">\(b\)</span>-th iteration can be fit specifically to improve the
performance of the ensemble comprising the first <span class="math inline">\(b-1\)</span>.</p>
<p>The earliest boosting methods, like the <em>Adaptive Boosting</em> (AdaBoost)
algorithm, were based on adaptively modifying the <em>case weights</em> so that
each base learner would focus more effort on those observations on which
the rest of the ensemble was less accurate. Starting with all
observations having the same weight, after each base learner is added to
the ensemble the weights for those observations on which it is
inaccurate are increased and those on which it is accurate are
decreased. In this way the subsequent base learner will focus more on
those observations where this base learner “failed”. However, more
recently formulations of the sequential ensemble boosting framework have
mainly been based on what is known as <em>gradient boosting</em>.</p>
<div id="gradient-boosting" class="section level3 hasAnchor" number="10.2.1">
<h3><span class="header-section-number">10.2.1</span> Gradient Boosting<a href="ensemble-models.html#gradient-boosting" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Gradient boosting is appealing for its universality, in that it can be
applied for any differentiable loss function. Although it has been shown
that the “reweighting” strategy of AdaBoost can be framed as a gradient
boosting algorithm, the loss function in that case is not the most
natural loss function for the classification problem for which it was
designed.</p>
<p>Before describing the general gradient boosting approach, we will start
with the standard regression problem as this is the most immediately
intuitive. Suppose, as always, that our sample is given by
<span class="math inline">\(\{(y_1, \x_1), ..., (y_n, \x_n)\}\)</span>. Then, for an initial estimate for
<span class="math inline">\(g^*\)</span>, say <span class="math inline">\(\hg\)</span> (typically we just set <span class="math inline">\(\hg(\x) = \bar y\)</span> for all
<span class="math inline">\(\x\)</span>), and for a tuning parameter <span class="math inline">\(0 &lt; \eta &lt; 1\)</span>, do the following for
<span class="math inline">\(b = 1, ..., B\)</span>:</p>
<ul>
<li><p>Find the residuals from the current model:
<span class="math inline">\(r_i \gets y_i - \hg(\x_i); i = 1, ..., n\)</span>.</p></li>
<li><p>Fit a base learner (e.g. a decision tree), say <span class="math inline">\(\hg^{b}\)</span>, to predict
the <span class="math inline">\(r_i\)</span>’s from the <span class="math inline">\(\x_i\)</span>’s.</p>
<ul>
<li>That is we fit a regression model where the responses are
replaced with the residuals <span class="math inline">\(r_1, ..., r_n\)</span></li>
</ul></li>
<li><p>Update the estimate of <span class="math inline">\(g^*\)</span> by adding a “shrunken” version of
<span class="math inline">\(\hg^{b}\)</span>: <span class="math inline">\(\hg \gets \hg + \eta\hg^{b}\)</span></p></li>
</ul>
<p>Why should an approach like this work? We can think of the residuals at
each iteration simply as the “part of the response” which is <em>not yet</em>
predicted by the ensemble. The next base learner is therefore focused
specifically on predicting what the rest of the ensemble is not, and so
whose addition will most improve the overall model.</p>
<p>So, why “shrink” the effect of the base learners (with the parameter
<span class="math inline">\(\eta\)</span>)? The simple answer to this is to avoid overfitting. If the base
learners are able to fit too well to the data (or the modified data
using the residuals rather than the raw <span class="math inline">\(Y\)</span> values), then we face the
standard problem of focusing too heavily on the precise manifest data
observations and not the general trend in the relationships. By limiting
the influence of each of the base learners, through this shrinkage, the
risk of overfitting is substantially reduced. Using a larger number of
base learners, each with a small contribution to the overall prediction,
can also have a variance reducing effect similar to that obtained when
bagging. In fact many of the popular implementations of boosting seek to
further leverage this effect by using decorrelation “tricks”, like
sub-sampling observations and variables (as in random forests) when
fitting each base learner. Nonetheless, it is important to appropriately
tune the value of <span class="math inline">\(\eta\)</span>, the total number of base learners <span class="math inline">\(B\)</span>, as well
as any additional hyperparameters of the individual base learners.
Importantly, because the base learners act collaboratively, each can be
far simpler than when a single decision tree (or bagged ensemble/random
forest) is used.</p>
<p><strong>Functional Gradient Descent</strong></p>
<p>The approach of having the base learners try to predict the residuals
from the “current model”, at each iteration, is intuitive. However, it
is only in regression where the notion of a “residual” has obvious
meaning. Fortunately we can rely on the principles of optimisation, and
specifically gradient descent, to extend this boosting procedure to more
general settings.</p>
<p>First let’s quickly recall the ideas of gradient descent, and apply them
to the problem of training a model. Let’s suppose that the class of
functions from which we are choosing our estimate for <span class="math inline">\(g^*\)</span>, <span class="math inline">\(\F\)</span>, is a
<em>parametric class of functions</em>, meaning that each <span class="math inline">\(g \in \F\)</span> can be
written using some general form <span class="math inline">\(g_\F\)</span> plus specific settings for
parameters, <span class="math inline">\(\theta\)</span>.</p>
<ul>
<li>For example, if <span class="math inline">\(\F\)</span> is the collection of all linear (affine)
functions then we could have <span class="math inline">\(\theta = \bbeta\)</span> and <span class="math display">\[
g_\F(\x|\bbeta) = \beta_0 + \sum_{j=1}^p \beta_j x_j.
\]</span></li>
</ul>
<p>We know that we want to minimise our training error, that is, to find <span class="math display">\[
\hat \theta = \argmin_{\theta} \frac{1}{n}\sum_{i=1}^n (y_i - g_\F(\x_i|\theta))^2 \equiv \argmin_\theta L_{train}\left(g_\F(\cdot | \theta)\right).
\]</span></p>
<p>To find the best <span class="math inline">\(\hat \theta\)</span> using gradient descent, we would start
with an initial estimate and then proceed by iteratively updating;
setting</p>
<p><span class="math display">\[
\hat \theta \gets \hat \theta - \eta \frac{\partial}{\partial
\theta}\frac{1}{n}\sum_{i=1}^n (y_i -  g_\F(\x_i|\hat \theta))^2.
\]</span></p>
<p>That is, with each iteration we shift the parameters a small amount
(<span class="math inline">\(\eta\)</span>) in the direction which decreases the training error the most
(i.e. in the direction of the negative gradient). This sequential
updating is analogous to boosting, since the “current value” for
<span class="math inline">\(\hat \theta\)</span> is what has been achieved so far by previous iterations,
and we are seeking to improve this incrementally.</p>
<p>The idea of <em>functional gradient descent</em> is to imagine that every
single value of the function is its own parameter. In other words we
would start with an initial setting of <span class="math inline">\(\hg\)</span> and then iteratively update
this by setting <span class="math display">\[
\hg \gets \hg - \eta \frac{\partial}{\partial
g}\frac{1}{n}\sum_{i=1}^n (y_i -  \hg(\x_i))^2.
\]</span></p>
<p>But notice that the training error
<span class="math inline">\(\frac{1}{n}\sum_{i=1}^n (y_i -  \hg(\x_i))^2\)</span> depends on <span class="math inline">\(\hg\)</span> only
through the values of <span class="math inline">\(\hg\)</span> evaluated at the <span class="math inline">\(\x_i\)</span>’s. That is, the
<em>function</em>
<span class="math inline">\(\frac{\partial L_{train}(\hg)}{\partial g}:= \frac{\partial}{\partial g} \frac{1}{n}\sum_{i=1}^n (y_i -  \hg(\x_i))^2\)</span>
can be anything which satisfies
<span class="math inline">\(\frac{\partial L_{train}(\hg)}{\partial g}(\x_i) = \frac{1}{n}\frac{\partial}{\partial \hat y}(y_i - \hat y)^2|_{\hat y = \hg(\x_i)}\)</span>
for all <span class="math inline">\(i = 1, ..., n\)</span>. But what is
<span class="math inline">\(\frac{\partial}{\partial \hat y}(y_i-\hat y_i)^2\)</span>? It is simply equal
to <span class="math inline">\(-2(y_i - \hat y_i) = -2 r_i\)</span>, where <span class="math inline">\(r_i\)</span> is the <span class="math inline">\(i\)</span>-th residual. In
other words, to update our model using functional gradient descent we
could simplify this as <span class="math display">\[
\hg \gets \hg + \tilde \eta \tilde g,
\]</span></p>
<p>where <span class="math inline">\(\tilde \eta = \frac{2}{n}\eta\)</span> and <span class="math inline">\(\tilde g\)</span> is any function
satisfying <span class="math inline">\(\tilde g(\x_i) = r_i\)</span>.</p>
<p>Okay, so this is a lot to take in, so let’s quickly take stock:</p>
<ul>
<li><p>We know that gradient descent can be used to perform optimisation</p></li>
<li><p>We know that model training is an optimisation problem</p>
<ul>
<li>Although we have seen a lot about how only minimising the
training error may not be a good idea, and that some form of
regularisation may be needed to improve generalisation, let’s
park that point for now.</li>
</ul></li>
<li><p>We don’t want to be restricted to parametric <span class="math inline">\(\F\)</span>, and so we look at
“nonparametric” gradient descent where we actually treat the
function values themselves as though <em>they</em> are the “parameters”
we’re shifting to improve our objective</p></li>
<li><p>Then we see finally that actually doing this is the same as
iteratively updating our estimate <span class="math inline">\(\hg\)</span> by adding a “shrunken”
version of a function <span class="math inline">\(\tilde g\)</span> which “predicts” the current
residuals. In other words, we found that the boosting idea we
described earlier for regression is actually very closely connected
to training a model by functional gradient descent.</p></li>
</ul>
<p>Now, many of you may be thinking “how do we actually find <span class="math inline">\(\tilde g\)</span>?”,
and the truth is that we don’t. Not only do we not, but actually it is
beneficial only to use an approximation of <span class="math inline">\(\tilde g\)</span>, as this this will
reduce the risk of overfitting. Moreover, we have learnt a lot in this
module about how to approximate (or estimate) functions using
regression. So replacing <span class="math inline">\(\tilde g\)</span> with a regression function fit to
predict the residuals, which is what we described initially in this
section anyway, is likely preferable. So, more correctly than it is
stated above, the regression boosting approach described at the start of
this section may be seen as performing <em>approximate functional gradient
descent</em>, based on minimising the regression training error.</p>
<p>With <em>this</em> description it is far easier to generalise the procedure to
other problems, since we could equally apply the same principles to any
loss function, <span class="math inline">\(L\)</span>. The general gradient boosting method therefore works
as follows: Starting with some initial model <span class="math inline">\(\hg\)</span>, we do the following
for <span class="math inline">\(b = 1, ..., B\)</span>:</p>
<ul>
<li><p>Find the current gradients of the loss:
<span class="math inline">\(\delta_i \gets \frac{\partial}{\partial \hat y} L(y_i, \hat y)|_{\hat y = \hg(\x_i)}; i = 1, ..., n\)</span></p></li>
<li><p>Fit a base learner (regression model, e.g. a regression tree), say
<span class="math inline">\(\hg^b\)</span> to predict the <em>negative gradients</em>
<span class="math inline">\(-\delta_1, ..., -\delta_n\)</span> from <span class="math inline">\(\x_1, ..., \x_n\)</span>.</p></li>
<li><p>Update <span class="math inline">\(\hg\)</span> to include a shrunken version of <span class="math inline">\(\hg^b\)</span>:
<span class="math inline">\(\hg \gets \hg + \eta \hg^b\)</span>.</p></li>
</ul>
<p><strong>Link Functions</strong></p>
<p>As in the case of generalised linear models, when performing gradient
boosting we may require a link function <span class="math inline">\(h\)</span> to connect the outputs of
the function <span class="math inline">\(\hg\)</span> with the predictions for the mean of the response. In
this case we evaluate the loss function on <span class="math inline">\(y_i\)</span> and
<span class="math inline">\(h^{-1}(\hg(\x_i))\)</span>, and not on <span class="math inline">\(y_i\)</span> and <span class="math inline">\(\hg(\x_i)\)</span>. The same overall
procedure is applied, however when computing the gradients <span class="math inline">\(\delta_i\)</span>
with respect to <span class="math inline">\(\hg\)</span> we need to apply the chain rule in evaluating
<span class="math inline">\(\delta_i = \frac{\partial}{\partial \hat y} L(y_i, h^{-1}(\hat y))|_{\hat y = \hg(\x_i)}\)</span>.</p>
<p><strong>Gradient Boosting in R</strong></p>
<p>Gradient boosting can lead to extremely flexible models, and so careful
tuning of both <span class="math inline">\(\eta\)</span> (sometimes called either the learning rate, or the
shrinkage parameter) and <span class="math inline">\(B\)</span> (the total number of “boosting
iterations”), is vital. At a high level we may think of the product of
<span class="math inline">\(\eta\)</span> and <span class="math inline">\(B\)</span> as the “total amount of learning” which is achieved.
However this does not always mean that halving <span class="math inline">\(\eta\)</span> and doubling <span class="math inline">\(B\)</span>
would lead to the same solution, nor the same quality of solution. The
reason for this is that when <span class="math inline">\(\eta\)</span> is relatively large some of the
learning is non-productive, since taking large steps in gradient based
optimisation can overshoot and lead to oscillation around where good
solutions lie. Generally speaking it is preferable from the point of
view of accuracy, to use very small <span class="math inline">\(\eta\)</span> and larger <span class="math inline">\(B\)</span>. However this
comes at the cost of increased computation since we need to fit <span class="math inline">\(B\)</span>
individual base learners. As a general rule of thumb for the standard
gradient boosting algorithm setting <span class="math inline">\(\eta\)</span> to <span class="math inline">\(0.1\)</span> or less is
reasonable, and then tuning over <span class="math inline">\(B\)</span> can be done using cross validation.</p>
<ul>
<li>One important point to note is that when tuning <span class="math inline">\(B\)</span>, if for example
we want to consider models with both <span class="math inline">\(100\)</span> and <span class="math inline">\(200\)</span> base
learners/boosting iterations then we don’t have to fit two separate
models. We can simply fit the model with <span class="math inline">\(B=200\)</span> and then compare
the solutions after the first <span class="math inline">\(100\)</span> and <span class="math inline">\(200\)</span>. In fact we can
compare the solutions after each iteration. However, the
implementation in <code>caret</code> does not allow for this and the <code>gbm</code>
package is preferable in this case.</li>
</ul>
<p>In addition to <span class="math inline">\(\eta\)</span> and <span class="math inline">\(B\)</span>, each of the base learners themselves will
have their own tuning parameters. In the case of decision trees, we can
tune any of the parameters which determine their complexity, such as
their maximum depth and the minimum size of any of the leaf nodes. It is
arguable that tuning all of these is “overkill”, however, and that only
tuning <span class="math inline">\(\eta, B\)</span> and any <em>one</em> of the tuning parameters of the trees is
sufficient.</p>
<div class="sourceCode" id="cb516"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb516-1"><a href="ensemble-models.html#cb516-1" tabindex="-1"></a><span class="do">### Load library</span></span>
<span id="cb516-2"><a href="ensemble-models.html#cb516-2" tabindex="-1"></a><span class="fu">library</span>(caret)</span>
<span id="cb516-3"><a href="ensemble-models.html#cb516-3" tabindex="-1"></a></span>
<span id="cb516-4"><a href="ensemble-models.html#cb516-4" tabindex="-1"></a><span class="do">### Set up to perform five fold cross validation</span></span>
<span id="cb516-5"><a href="ensemble-models.html#cb516-5" tabindex="-1"></a><span class="do">### As the data set is reasonably large five folds is very reasonable</span></span>
<span id="cb516-6"><a href="ensemble-models.html#cb516-6" tabindex="-1"></a>trControl <span class="ot">&lt;-</span> <span class="fu">trainControl</span>(<span class="at">method =</span> <span class="st">&quot;cv&quot;</span>, <span class="at">number =</span> <span class="dv">5</span>)</span>
<span id="cb516-7"><a href="ensemble-models.html#cb516-7" tabindex="-1"></a></span>
<span id="cb516-8"><a href="ensemble-models.html#cb516-8" tabindex="-1"></a></span>
<span id="cb516-9"><a href="ensemble-models.html#cb516-9" tabindex="-1"></a><span class="do">### We will only vary B (called n.trees) and tree depth (called interaction.depth)</span></span>
<span id="cb516-10"><a href="ensemble-models.html#cb516-10" tabindex="-1"></a><span class="do">### However as the gbm method has additional hyperparameters eta (called shrinkage)</span></span>
<span id="cb516-11"><a href="ensemble-models.html#cb516-11" tabindex="-1"></a><span class="do">### and minimum leaf size (called n.minobsinnode) we need to set fixed values for</span></span>
<span id="cb516-12"><a href="ensemble-models.html#cb516-12" tabindex="-1"></a><span class="do">### these as well</span></span>
<span id="cb516-13"><a href="ensemble-models.html#cb516-13" tabindex="-1"></a>tune_grid <span class="ot">&lt;-</span> <span class="fu">expand.grid</span>(<span class="at">n.trees =</span> <span class="fu">c</span>(<span class="dv">100</span>, <span class="dv">200</span>, <span class="dv">300</span>), <span class="at">shrinkage =</span> <span class="fl">0.1</span>, <span class="at">n.minobsinnode =</span> <span class="dv">10</span>,</span>
<span id="cb516-14"><a href="ensemble-models.html#cb516-14" tabindex="-1"></a>                         <span class="at">interaction.depth =</span> <span class="fu">c</span>(<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">3</span>))</span>
<span id="cb516-15"><a href="ensemble-models.html#cb516-15" tabindex="-1"></a></span>
<span id="cb516-16"><a href="ensemble-models.html#cb516-16" tabindex="-1"></a></span>
<span id="cb516-17"><a href="ensemble-models.html#cb516-17" tabindex="-1"></a><span class="do">### We now tune and train the model as always</span></span>
<span id="cb516-18"><a href="ensemble-models.html#cb516-18" tabindex="-1"></a>gb_model <span class="ot">&lt;-</span> <span class="fu">train</span>(target<span class="sc">~</span>., <span class="at">data =</span> satimage.tr, <span class="at">method =</span> <span class="st">&#39;gbm&#39;</span>,</span>
<span id="cb516-19"><a href="ensemble-models.html#cb516-19" tabindex="-1"></a>                  <span class="at">tuneGrid =</span> tune_grid,</span>
<span id="cb516-20"><a href="ensemble-models.html#cb516-20" tabindex="-1"></a>                  <span class="at">trControl =</span> trControl,</span>
<span id="cb516-21"><a href="ensemble-models.html#cb516-21" tabindex="-1"></a>                  <span class="at">verbose =</span> <span class="cn">FALSE</span>) <span class="co"># stops printing of training progress</span></span>
<span id="cb516-22"><a href="ensemble-models.html#cb516-22" tabindex="-1"></a></span>
<span id="cb516-23"><a href="ensemble-models.html#cb516-23" tabindex="-1"></a><span class="do">### We can inspect the results of the cross validation</span></span>
<span id="cb516-24"><a href="ensemble-models.html#cb516-24" tabindex="-1"></a>gb_model<span class="sc">$</span>results</span></code></pre></div>
<pre><code>##   shrinkage interaction.depth n.minobsinnode n.trees  Accuracy     Kappa
## 1       0.1                 1             10     100 0.8704567 0.8396749
## 4       0.1                 2             10     100 0.8848772 0.8576698
## 7       0.1                 3             10     100 0.8968583 0.8725216
## 2       0.1                 1             10     200 0.8779977 0.8492478
## 5       0.1                 2             10     200 0.8979637 0.8738788
## 8       0.1                 3             10     200 0.9024032 0.8793976
## 3       0.1                 1             10     300 0.8839891 0.8566643
## 6       0.1                 2             10     300 0.8997413 0.8760905
## 9       0.1                 3             10     300 0.9026225 0.8796597
##    AccuracySD     KappaSD
## 1 0.005147918 0.006359900
## 4 0.007625226 0.009347375
## 7 0.009026825 0.011066750
## 2 0.005164187 0.006369456
## 5 0.007562932 0.009314294
## 8 0.008225978 0.010157932
## 3 0.008871559 0.010935541
## 6 0.011100185 0.013643020
## 9 0.006896950 0.008477130</code></pre>
<div class="sourceCode" id="cb518"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb518-1"><a href="ensemble-models.html#cb518-1" tabindex="-1"></a><span class="do">### Finally we can make predictions on the test set and the assess the</span></span>
<span id="cb518-2"><a href="ensemble-models.html#cb518-2" tabindex="-1"></a><span class="do">### test accuracy. We see that the performance is similar to the</span></span>
<span id="cb518-3"><a href="ensemble-models.html#cb518-3" tabindex="-1"></a><span class="do">### bagging models</span></span>
<span id="cb518-4"><a href="ensemble-models.html#cb518-4" tabindex="-1"></a>gb_pred <span class="ot">&lt;-</span> <span class="fu">predict</span>(gb_model, satimage.te)</span>
<span id="cb518-5"><a href="ensemble-models.html#cb518-5" tabindex="-1"></a></span>
<span id="cb518-6"><a href="ensemble-models.html#cb518-6" tabindex="-1"></a><span class="fu">confusionMatrix</span>(gb_pred, satimage.te<span class="sc">$</span>target)<span class="sc">$</span>overall[<span class="st">&#39;Accuracy&#39;</span>]</span></code></pre></div>
<pre><code>##  Accuracy 
## 0.9060716</code></pre>
<div class="sourceCode" id="cb520"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb520-1"><a href="ensemble-models.html#cb520-1" tabindex="-1"></a><span class="do">###</span></span>
<span id="cb520-2"><a href="ensemble-models.html#cb520-2" tabindex="-1"></a><span class="fu">library</span>(gbm)</span>
<span id="cb520-3"><a href="ensemble-models.html#cb520-3" tabindex="-1"></a></span>
<span id="cb520-4"><a href="ensemble-models.html#cb520-4" tabindex="-1"></a><span class="do">### Using the gbm package directly allows us to perform cross validation over</span></span>
<span id="cb520-5"><a href="ensemble-models.html#cb520-5" tabindex="-1"></a><span class="do">### a broad range of values for B, since it tracks the validation error</span></span>
<span id="cb520-6"><a href="ensemble-models.html#cb520-6" tabindex="-1"></a><span class="do">### for each iteration</span></span>
<span id="cb520-7"><a href="ensemble-models.html#cb520-7" tabindex="-1"></a></span>
<span id="cb520-8"><a href="ensemble-models.html#cb520-8" tabindex="-1"></a><span class="do">### It should be noted that the multinomial model in gbm has bugs, and a</span></span>
<span id="cb520-9"><a href="ensemble-models.html#cb520-9" tabindex="-1"></a><span class="do">### warning is printed to the screen. For a real modelling problem care should</span></span>
<span id="cb520-10"><a href="ensemble-models.html#cb520-10" tabindex="-1"></a><span class="do">### therefore be taken, and if you happen to love this implementation of</span></span>
<span id="cb520-11"><a href="ensemble-models.html#cb520-11" tabindex="-1"></a><span class="do">### gbm then you can always fit K-1 binary models as described in the</span></span>
<span id="cb520-12"><a href="ensemble-models.html#cb520-12" tabindex="-1"></a><span class="do">### section on generalised linear models, and combine the outputs for</span></span>
<span id="cb520-13"><a href="ensemble-models.html#cb520-13" tabindex="-1"></a><span class="do">### conducting multinomial regression. For illustrative purposes</span></span>
<span id="cb520-14"><a href="ensemble-models.html#cb520-14" tabindex="-1"></a><span class="do">### on how to use the package for tuning, however, we will accept the</span></span>
<span id="cb520-15"><a href="ensemble-models.html#cb520-15" tabindex="-1"></a><span class="do">### risk of inappropriate results</span></span>
<span id="cb520-16"><a href="ensemble-models.html#cb520-16" tabindex="-1"></a>gb_model2 <span class="ot">&lt;-</span> <span class="fu">gbm</span>(target<span class="sc">~</span>., <span class="at">data =</span> satimage.tr, <span class="at">cv.folds =</span> <span class="dv">5</span>,</span>
<span id="cb520-17"><a href="ensemble-models.html#cb520-17" tabindex="-1"></a>                 <span class="at">n.trees =</span> <span class="dv">300</span>, <span class="at">interaction.depth =</span> <span class="dv">6</span>)</span></code></pre></div>
<pre><code>## Distribution not specified, assuming multinomial ...</code></pre>
<div class="sourceCode" id="cb522"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb522-1"><a href="ensemble-models.html#cb522-1" tabindex="-1"></a><span class="do">### The predict method for gbm objects does not directly produce</span></span>
<span id="cb522-2"><a href="ensemble-models.html#cb522-2" tabindex="-1"></a><span class="do">### a class prediction, but rather the probabilities</span></span>
<span id="cb522-3"><a href="ensemble-models.html#cb522-3" tabindex="-1"></a>gb_predict <span class="ot">&lt;-</span> <span class="fu">predict</span>(gb_model2, satimage.te, <span class="at">type =</span> <span class="st">&quot;response&quot;</span>)</span></code></pre></div>
<pre><code>## Using 114 trees...</code></pre>
<div class="sourceCode" id="cb524"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb524-1"><a href="ensemble-models.html#cb524-1" tabindex="-1"></a><span class="do">### To convert these to class predictions we can take the maximum</span></span>
<span id="cb524-2"><a href="ensemble-models.html#cb524-2" tabindex="-1"></a><span class="do">### probability, and to ensure the correct class label (factor level)</span></span>
<span id="cb524-3"><a href="ensemble-models.html#cb524-3" tabindex="-1"></a><span class="do">### is applied we can find these in the names of the columns</span></span>
<span id="cb524-4"><a href="ensemble-models.html#cb524-4" tabindex="-1"></a>gb_pred2 <span class="ot">&lt;-</span> <span class="fu">as.factor</span>(<span class="fu">colnames</span>(gb_predict[,,<span class="dv">1</span>])[<span class="fu">apply</span>(gb_predict[,,<span class="dv">1</span>], <span class="dv">1</span>, which.max)])</span>
<span id="cb524-5"><a href="ensemble-models.html#cb524-5" tabindex="-1"></a></span>
<span id="cb524-6"><a href="ensemble-models.html#cb524-6" tabindex="-1"></a><span class="do">### Again we see the accuracy very similar to the other ensembles</span></span>
<span id="cb524-7"><a href="ensemble-models.html#cb524-7" tabindex="-1"></a><span class="fu">confusionMatrix</span>(gb_pred2, satimage.te<span class="sc">$</span>target)<span class="sc">$</span>overall[<span class="st">&#39;Accuracy&#39;</span>]</span></code></pre></div>
<pre><code>##  Accuracy 
## 0.9128179</code></pre>
<div id="extreme-gradient-boosting" class="section level4 hasAnchor" number="10.2.1.1">
<h4><span class="header-section-number">10.2.1.1</span> eXtreme Gradient Boosting<a href="ensemble-models.html#extreme-gradient-boosting" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>Because of its success gradient boosting has had a number of
improvements, with one of the most successful being what is known as
<em>eXtreme Gradient Boosting</em> (XGBoost). XGBoost uses a few principled
modifications of the standard gradient boosting approach. These include
the addition of a penalty to the training error in order to regularise
the estimation and reduce the risk of overfitting, as well as a more
sophisticated optimisation strategy which uses both the first and second
functional gradients. Because of these XGBoost is often able to achieve
equal or better accuracy than standard gradient boosting using a larger
<span class="math inline">\(\eta\)</span> and smaller <span class="math inline">\(B\)</span>, and hence less overall computation.</p>
<p><strong>XGBoost in R</strong></p>
<p>XGBoost can be implemented using either the <code>caret</code> package or the
<code>xgboost</code> package (as well as some others, which include variations on
the standard XGBoost framework). Because of the additional options in
terms of how the regularisation is applied, XGBoost has more tuning
parameters than standard gradient boosting. We will only vary the
learning rate <span class="math inline">\(\eta\)</span> and the number of iterations <span class="math inline">\(B\)</span> (<code>nrounds</code> below), however details of the other tuning parameters can be accessed by using <code>help(xgboost)</code> after loading the <code>xgboost</code> library.</p>
<div class="sourceCode" id="cb526"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb526-1"><a href="ensemble-models.html#cb526-1" tabindex="-1"></a><span class="do">### Let&#39;s start by setting up our trainControl and tuning grid objects</span></span>
<span id="cb526-2"><a href="ensemble-models.html#cb526-2" tabindex="-1"></a>trControl <span class="ot">&lt;-</span> <span class="fu">trainControl</span>(<span class="at">method =</span> <span class="st">&quot;cv&quot;</span>, <span class="at">number =</span> <span class="dv">5</span>)</span>
<span id="cb526-3"><a href="ensemble-models.html#cb526-3" tabindex="-1"></a></span>
<span id="cb526-4"><a href="ensemble-models.html#cb526-4" tabindex="-1"></a>tune_grid <span class="ot">&lt;-</span> <span class="fu">expand.grid</span>(<span class="at">nrounds =</span> <span class="fu">c</span>(<span class="dv">100</span>, <span class="dv">200</span>, <span class="dv">300</span>), <span class="at">max_depth =</span> <span class="dv">6</span>,</span>
<span id="cb526-5"><a href="ensemble-models.html#cb526-5" tabindex="-1"></a>                         <span class="at">eta =</span> <span class="fu">c</span>(<span class="fl">0.1</span>, <span class="fl">0.2</span>, <span class="fl">0.3</span>), <span class="at">gamma =</span> <span class="dv">0</span>,</span>
<span id="cb526-6"><a href="ensemble-models.html#cb526-6" tabindex="-1"></a>                         <span class="at">colsample_bytree =</span> <span class="fl">0.8</span>, <span class="at">subsample =</span> <span class="fl">0.5</span>,</span>
<span id="cb526-7"><a href="ensemble-models.html#cb526-7" tabindex="-1"></a>                         <span class="at">min_child_weight =</span> <span class="dv">1</span>)</span>
<span id="cb526-8"><a href="ensemble-models.html#cb526-8" tabindex="-1"></a></span>
<span id="cb526-9"><a href="ensemble-models.html#cb526-9" tabindex="-1"></a><span class="do">### Now we can tune and train the model</span></span>
<span id="cb526-10"><a href="ensemble-models.html#cb526-10" tabindex="-1"></a>xgb_model <span class="ot">&lt;-</span> <span class="fu">train</span>(target<span class="sc">~</span>., <span class="at">data =</span> satimage.tr,</span>
<span id="cb526-11"><a href="ensemble-models.html#cb526-11" tabindex="-1"></a>                   <span class="at">trControl =</span> trControl, <span class="at">tuneGrid =</span> tune_grid,</span>
<span id="cb526-12"><a href="ensemble-models.html#cb526-12" tabindex="-1"></a>                   <span class="at">method =</span> <span class="st">&quot;xgbTree&quot;</span>)</span></code></pre></div>
<div class="sourceCode" id="cb527"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb527-1"><a href="ensemble-models.html#cb527-1" tabindex="-1"></a><span class="do">### The best tuning parameters are</span></span>
<span id="cb527-2"><a href="ensemble-models.html#cb527-2" tabindex="-1"></a>xgb_model<span class="sc">$</span>bestTune</span></code></pre></div>
<pre><code>##   nrounds max_depth eta gamma colsample_bytree min_child_weight subsample
## 4     100         6 0.2     0              0.8                1       0.5</code></pre>
<div class="sourceCode" id="cb529"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb529-1"><a href="ensemble-models.html#cb529-1" tabindex="-1"></a>xgb_pred <span class="ot">&lt;-</span> <span class="fu">predict</span>(xgb_model, satimage.te)</span>
<span id="cb529-2"><a href="ensemble-models.html#cb529-2" tabindex="-1"></a></span>
<span id="cb529-3"><a href="ensemble-models.html#cb529-3" tabindex="-1"></a><span class="fu">confusionMatrix</span>(xgb_pred, satimage.te<span class="sc">$</span>target)<span class="sc">$</span>overall[<span class="st">&#39;Accuracy&#39;</span>]</span></code></pre></div>
<pre><code>##  Accuracy 
## 0.9216399</code></pre>
<p>Using the <code>xgboost</code> package directly requires that we do some of the hard-coding
of objects and arguments ourselves. First, the data need to be provided as a
matrix of covariates and a vector of labels, and the labels need to be numeric
valued from <span class="math inline">\(0\)</span> to <span class="math inline">\(K-1\)</span>. We also need to be specific about the loss function,
where either the setting <code>objective = "multi:softmax"</code> or <code>objective = "multi:softprob"</code> can be used for multiclass classification. In addition cross validation is not implemented in the same way as in <code>gbm</code>, but we can use a simple validation approach by providing a so-called “watchlist”, which contains data sets on which evaluation should be performed during training. In this way <code>xgboost</code> is able to stop updating once the validation error stops improving, by providing an argument <code>early_sopping_rounds</code>. Specifically, if the validation error does not improve for <code>early_stopping_rounds</code> iterations then the optimisation terminates.</p>
<div class="sourceCode" id="cb531"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb531-1"><a href="ensemble-models.html#cb531-1" tabindex="-1"></a><span class="do">### Load the library library</span></span>
<span id="cb531-2"><a href="ensemble-models.html#cb531-2" tabindex="-1"></a><span class="fu">library</span>(xgboost)</span>
<span id="cb531-3"><a href="ensemble-models.html#cb531-3" tabindex="-1"></a></span>
<span id="cb531-4"><a href="ensemble-models.html#cb531-4" tabindex="-1"></a><span class="do">### Let&#39;s split the training data into 70% training and 30% validation</span></span>
<span id="cb531-5"><a href="ensemble-models.html#cb531-5" tabindex="-1"></a>valid_ix <span class="ot">&lt;-</span> <span class="fu">createDataPartition</span>(satimage.tr<span class="sc">$</span>target, <span class="at">p =</span> <span class="fl">0.3</span>, <span class="at">list =</span> <span class="cn">FALSE</span>)</span>
<span id="cb531-6"><a href="ensemble-models.html#cb531-6" tabindex="-1"></a></span>
<span id="cb531-7"><a href="ensemble-models.html#cb531-7" tabindex="-1"></a>X_tr <span class="ot">&lt;-</span> <span class="fu">as.matrix</span>(satimage.tr[<span class="sc">-</span>valid_ix,<span class="fu">names</span>(satimage.tr)<span class="sc">!=</span><span class="st">&#39;target&#39;</span>])</span>
<span id="cb531-8"><a href="ensemble-models.html#cb531-8" tabindex="-1"></a>X_val <span class="ot">&lt;-</span> <span class="fu">as.matrix</span>(satimage.tr[valid_ix,<span class="fu">names</span>(satimage.tr)<span class="sc">!=</span><span class="st">&#39;target&#39;</span>])</span>
<span id="cb531-9"><a href="ensemble-models.html#cb531-9" tabindex="-1"></a></span>
<span id="cb531-10"><a href="ensemble-models.html#cb531-10" tabindex="-1"></a><span class="do">### Applying as.numeric(as.factor(vector)) will transform the values</span></span>
<span id="cb531-11"><a href="ensemble-models.html#cb531-11" tabindex="-1"></a><span class="do">### in a vector to the values 1, 2, ..., up to the number of unique</span></span>
<span id="cb531-12"><a href="ensemble-models.html#cb531-12" tabindex="-1"></a><span class="do">### entries. Since satimage.tr$target is already a factor we can</span></span>
<span id="cb531-13"><a href="ensemble-models.html#cb531-13" tabindex="-1"></a><span class="do">### use as.numeric(satimage.tr$target)-1 to convert the labels</span></span>
<span id="cb531-14"><a href="ensemble-models.html#cb531-14" tabindex="-1"></a><span class="do">### to 0, 1, ..., K-1</span></span>
<span id="cb531-15"><a href="ensemble-models.html#cb531-15" tabindex="-1"></a></span>
<span id="cb531-16"><a href="ensemble-models.html#cb531-16" tabindex="-1"></a>y_tr <span class="ot">&lt;-</span> <span class="fu">as.numeric</span>(satimage.tr<span class="sc">$</span>target[<span class="sc">-</span>valid_ix])<span class="sc">-</span><span class="dv">1</span></span>
<span id="cb531-17"><a href="ensemble-models.html#cb531-17" tabindex="-1"></a>y_val <span class="ot">&lt;-</span> <span class="fu">as.numeric</span>(satimage.tr<span class="sc">$</span>target[valid_ix])<span class="sc">-</span><span class="dv">1</span></span>
<span id="cb531-18"><a href="ensemble-models.html#cb531-18" tabindex="-1"></a></span>
<span id="cb531-19"><a href="ensemble-models.html#cb531-19" tabindex="-1"></a><span class="do">### The function xgb.DMatrix allows us to create data objects which xgboost</span></span>
<span id="cb531-20"><a href="ensemble-models.html#cb531-20" tabindex="-1"></a><span class="do">### will be happy with</span></span>
<span id="cb531-21"><a href="ensemble-models.html#cb531-21" tabindex="-1"></a></span>
<span id="cb531-22"><a href="ensemble-models.html#cb531-22" tabindex="-1"></a>train_data <span class="ot">&lt;-</span> <span class="fu">xgb.DMatrix</span>(X_tr, <span class="at">label =</span> y_tr)</span>
<span id="cb531-23"><a href="ensemble-models.html#cb531-23" tabindex="-1"></a>val_data <span class="ot">&lt;-</span> <span class="fu">xgb.DMatrix</span>(X_val, <span class="at">label =</span> y_val)</span>
<span id="cb531-24"><a href="ensemble-models.html#cb531-24" tabindex="-1"></a></span>
<span id="cb531-25"><a href="ensemble-models.html#cb531-25" tabindex="-1"></a>watchlist <span class="ot">&lt;-</span> <span class="fu">list</span>(<span class="at">train =</span> train_data, <span class="at">eval =</span> val_data)</span>
<span id="cb531-26"><a href="ensemble-models.html#cb531-26" tabindex="-1"></a></span>
<span id="cb531-27"><a href="ensemble-models.html#cb531-27" tabindex="-1"></a>xgb_validation <span class="ot">&lt;-</span> <span class="fu">xgb.train</span>(<span class="at">data =</span> train_data,</span>
<span id="cb531-28"><a href="ensemble-models.html#cb531-28" tabindex="-1"></a>                      <span class="at">watchlist =</span> watchlist,</span>
<span id="cb531-29"><a href="ensemble-models.html#cb531-29" tabindex="-1"></a>                      <span class="at">nrounds =</span> <span class="dv">200</span>,</span>
<span id="cb531-30"><a href="ensemble-models.html#cb531-30" tabindex="-1"></a>                      <span class="at">early_stopping_rounds =</span> <span class="dv">10</span>,</span>
<span id="cb531-31"><a href="ensemble-models.html#cb531-31" tabindex="-1"></a>                      <span class="at">num_class =</span> <span class="dv">6</span>, <span class="at">verbose =</span> <span class="dv">0</span>)</span>
<span id="cb531-32"><a href="ensemble-models.html#cb531-32" tabindex="-1"></a></span>
<span id="cb531-33"><a href="ensemble-models.html#cb531-33" tabindex="-1"></a><span class="do">### We now need to train the final model using the number of rounds</span></span>
<span id="cb531-34"><a href="ensemble-models.html#cb531-34" tabindex="-1"></a><span class="do">### after which this model terminated, since this was based on the</span></span>
<span id="cb531-35"><a href="ensemble-models.html#cb531-35" tabindex="-1"></a><span class="do">### validation error, but we only used some of the training data to</span></span>
<span id="cb531-36"><a href="ensemble-models.html#cb531-36" tabindex="-1"></a><span class="do">### actually fit the model</span></span>
<span id="cb531-37"><a href="ensemble-models.html#cb531-37" tabindex="-1"></a></span>
<span id="cb531-38"><a href="ensemble-models.html#cb531-38" tabindex="-1"></a>Bopt <span class="ot">&lt;-</span> xgb_validation<span class="sc">$</span>best_iteration</span>
<span id="cb531-39"><a href="ensemble-models.html#cb531-39" tabindex="-1"></a></span>
<span id="cb531-40"><a href="ensemble-models.html#cb531-40" tabindex="-1"></a>xgb_model2 <span class="ot">&lt;-</span> <span class="fu">xgb.train</span>(<span class="at">data =</span> <span class="fu">xgb.DMatrix</span>(<span class="fu">rbind</span>(X_tr, X_val), <span class="at">label =</span> <span class="fu">c</span>(y_tr, y_val)),</span>
<span id="cb531-41"><a href="ensemble-models.html#cb531-41" tabindex="-1"></a>                        <span class="at">nrounds =</span> Bopt, <span class="at">num_class =</span> <span class="dv">6</span>, <span class="at">verbose =</span> <span class="dv">0</span>)</span>
<span id="cb531-42"><a href="ensemble-models.html#cb531-42" tabindex="-1"></a></span>
<span id="cb531-43"><a href="ensemble-models.html#cb531-43" tabindex="-1"></a><span class="do">### Finally we can predict on the test set, and convert to an appropriate</span></span>
<span id="cb531-44"><a href="ensemble-models.html#cb531-44" tabindex="-1"></a><span class="do">### factor variable to evaluate test performance</span></span>
<span id="cb531-45"><a href="ensemble-models.html#cb531-45" tabindex="-1"></a>xgb_pred2 <span class="ot">&lt;-</span> <span class="fu">levels</span>(satimage.te<span class="sc">$</span>target)[<span class="fu">predict</span>(xgb_model2,</span>
<span id="cb531-46"><a href="ensemble-models.html#cb531-46" tabindex="-1"></a>                       <span class="fu">as.matrix</span>(satimage.te[,<span class="fu">names</span>(satimage.te)<span class="sc">!=</span><span class="st">&#39;target&#39;</span>]))<span class="sc">+</span><span class="dv">1</span>]</span>
<span id="cb531-47"><a href="ensemble-models.html#cb531-47" tabindex="-1"></a></span>
<span id="cb531-48"><a href="ensemble-models.html#cb531-48" tabindex="-1"></a><span class="fu">confusionMatrix</span>(<span class="fu">as.factor</span>(xgb_pred2), satimage.te<span class="sc">$</span>target)<span class="sc">$</span>overall[<span class="st">&#39;Accuracy&#39;</span>]</span></code></pre></div>
<pre><code>##  Accuracy 
## 0.9107421</code></pre>
</div>
</div>
</div>
<div id="variable-importance" class="section level2 hasAnchor" number="10.3">
<h2><span class="header-section-number">10.3</span> Variable Importance<a href="ensemble-models.html#variable-importance" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>The ensemble nature of these models makes their predictions difficult to interpret. Model free
diagnostics like Shapley Values and partial dependence plots are always an option. We will
not go into these model free approaches, but rather briefly discuss the concept of
<em>variable importance</em>. During the fitting of each decision tree in an ensemble models, each
time a variable is used for splitting statistics on the improvement in the impurity due to the
split can be tallied. Aggregating this information over the entire ensemble gives an indication
of which are seen to be the more/less important variables for determining the predictions
from the model. Although these do not give any indication of how modifying the value of one of the
covariates will impact on its predicted value, they nonetheless allow us to gain a high level picture
of the most important variables as they are captured by the models.</p>
</div>
<div id="summary-9" class="section level2 hasAnchor" number="10.4">
<h2><span class="header-section-number">10.4</span> Summary<a href="ensemble-models.html#summary-9" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<ul>
<li><p>Ensemble models combine the predictions from multiple (often very many) <em>base learners</em> in
order to make a final prediction</p></li>
<li><p>Ensemble models can be extremely accurate if well tuned, but are hard to interpret</p></li>
<li><p>Bagging is a general purpose framework which averages/aggregates the predictions from base learners
fit independently on bootstrap samples</p>
<ul>
<li>Bagging can have a remarkable effect in reducing the variance of highly flexible models</li>
<li>Bagging inflexible models is typically not advised, and the highly flexible decision trees
are by far the most popular</li>
<li>Random Forests are a simple modification of bagged decision trees which add randomisation
to the fitting procedure which decorrelates the predictions from different trees and typically leads to
overall better prediction</li>
<li>Out-of-bag statistics can significantly reduce the time needed to tune hyperparameters</li>
</ul></li>
<li><p>Boosting is a sequential procedure where base learners focus on improving specifically where the ensemble
“fit so far” is less accurate</p>
<ul>
<li>Boosting is also a general purpose framework, but decision trees are the most popular base learners</li>
<li>Each boosting iteration reduces the bias of the overall model since it improves the fit to the training data</li>
<li>To combat the increase in variance, “slow learning” by using a small learning rate (<span class="math inline">\(\eta\)</span>) and adding randomness
to the fitting of each tree can be beneficial</li>
</ul></li>
</ul>

</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="nonlinearity2.html" class="navigation navigation-prev navigation-unique" aria-label="Previous page"><i class="fa fa-angle-left"></i></a>

    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
  "sharing": {
    "github": false,
    "facebook": true,
    "twitter": true,
    "linkedin": false,
    "weibo": false,
    "instapaper": false,
    "vk": false,
    "whatsapp": false,
    "all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
  },
  "fontsettings": {
    "theme": "white",
    "family": "sans",
    "size": 2
  },
  "edit": {
    "link": null,
    "text": null
  },
  "history": {
    "link": null,
    "text": null
  },
  "view": {
    "link": null,
    "text": null
  },
  "download": null,
  "search": {
    "engine": "fuse",
    "options": null
  },
  "toc": {
    "collapse": "subsection"
  }
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
