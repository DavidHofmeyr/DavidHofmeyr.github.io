[["introduction-to-r.html", "MATH482: Statistical Learning 1 Introduction to R 1.1 Background on R 1.2 Getting started with RStudio 1.3 Statements, expressions and objects 1.4 Basic plots 1.5 R scripts 1.6 R markdown 1.7 Functions 1.8 Getting Help 1.9 Loops and Flow", " MATH482: Statistical Learning David P. Hofmeyr 1 Introduction to R The purpose of this chapter is to provide you with a gentle introduction to R as an environment for working with and analysing data. 1.1 Background on R R is a free and open-source programming language for statistical computing and graphics. Open-source means that the code that was written to create the R environment is available free of charge for others to use and modify from its original design. Open-source software is typically thought of as a collaborative effort where people improve upon the source code and share the changes within the community so that other members can take advantage of their work, as well as help improve it further. These features mean that R has become the most powerful and flexible domain-specific statistical programming language in the world. However, Rs statistics-specific nature also means that we should regard it as just one of the tools, albeit a powerful tool, in the toolbox available to the modern data scientist. 1.2 Getting started with RStudio 1.2.1 Installing R and RStudio Versions of R are available for Windows, Macs and Linux. To download and install R visit the website http://www.r-project.org. You will also need RStudio (http://www.rstudio.com). Note: If you are using one of the computers in the lab, both of these will already be installed. 1.2.2 R extensions R has a rich ecosystem of extensions (packages, more on that later) that cater for nearly every statistical situation you can think of. The main repository for R packages is CRAN (the Comprehensive R Archive Network). Many people also upload their code as packages to their own repositories, e.g. on github 1.2.3 Saving your work on the Lancaster Windows network If you are using a university Windows computer or are remotely connected to a Windows virtual machine, start by finding your home folder (your H: drive). The H: drive will be made accessible to you no matter which university computer you use. Access it via File Explorer &gt; This PC. This is the best location to save all your files (reports, scripts, plots, data sets, and everything else). do NOT save files to the local (C:) drive as they will probably not be there when you next login. Also, ISS makes regular backups of your H: drive, so if you ever permanently delete a file they should be able to recover a previous version of it. 1.2.4 RStudio and the console Start by opening R Studio. R Studio tries to makes best use of the screen by splitting it up in to three panels: An interactive R console (or REPL)  enter R statements manually at the prompt at the bottom (look for the &gt; symbol). Environment  data structures stored in memory show up here, including things like data sets, plots, and models. Stuff  a miscellaneous panel that has various uses, as you will see later. For now, click on the Plot tab. The panel should become blank (since we have not made any plots yet). 1.3 Statements, expressions and objects We will begin by only using the console panel. In the console, we can type R statements and have them evaluated each time we hit Enter. Try entering a very simple statement: an expression that adds of two numbers. Type the following and press Enter. 4 + 5 ## [1] 9 Pressing Enter instructs the console to evaluate the statement. And since our statement is an expression (i.e. statement that has a result), the result of evaluating the expression is printed back. The [1] refers to the fact that our expression evaluates to a single object, in this case a scalar taking the value 9. Basic maths operations in R. Operation Example Add 2 + 3 Subtract 2 - 3 Multiply 2 * 3 Divide 2 / 3 Modulus 2 %% 3 Square root sqrt(2) Power 2^3 Cosine cos(2 * pi) Sine sin(pi / 2) Tangent tan(pi / 4) Natural log log(2) Expontential exp(2) Using numbers of your own, try some other basic mathematical expressions listed below 100 - 45 65 * 89 45 / -3 100 %% 9 sqrt(810) 2^10 Once youre happy, try combining some of these expressions into more complicated expressions, for example: 3 + 7 * 5 / 3 - 2 (3 + 7) * 5 / (3 - 2) As with mathematics, Rs operator precedence follows the BODMAS rule: Brackets, then Orders (powers, square roots), then Division, then Multiplication, then Addition, then Subtraction. Brackets, then, are key to controlling the order of operations within an expression, for instance \\[ \\frac{\\sqrt{3/4}}{\\frac{1}{3} - \\frac{2}{\\pi^2}}. \\] sqrt(3 / 4) / (1 / 3 - 2 / pi^2) ## [1] 6.626513 Combining expressions, as we did above, may become tedious, especially if we wish to re-use the same expression many times. Better is to store the evaluation of such an expression in memory, so that we can use it later. To store the result of evaluating an expression we assign them to objects using the assignment operator &lt;-. apple &lt;- 20 - 16 banana &lt;- 4 + 5 apple - banana ## [1] -5 Lines 1 and 2 simply evaluate the expressions on the right of &lt;- and store the results in objects named apple and banana respectively. Line 3 is an expression statement, in which R substitutes the values of apple and banana into the expression apple - banana and evaluates it. If you look in the environment panel of RStudio, you should now see two objects listed. These objects are now part of our working environment. We can access their contents by entering their names into the console: apple ## [1] 4 banana ## [1] 9 NB: R will not hesitate in overwriting an object if you try to assign its name to a new object (R does not issue a warning): apple &lt;- -100 apple ## [1] -100 We can also modify the value of an object by referencing itself: apple &lt;- apple + 1 apple ## [1] -99 Note: it is possible to use the equals (=) sign instead of &lt;- in R, but it is preferable to stick with &lt;- for assignment statements. They are functionally different in certain contexts, but work exactly equivalently for assignments In the above we have only encountered numeric objects which by default are of type double (referring to the level of precision with which the computer is storing that number) and of class numeric, but there are very many other object types supported in R. For example, it is sometimes convenient to assign a value to an object which is some description of it. We can do this by writing this description in inverted commas, creating a character object, and assigning this to the object: my_name &lt;- &quot;David&quot; my_name ## [1] &quot;David&quot; class(my_name) ## [1] &quot;character&quot; class(1) ## [1] &quot;numeric&quot; Later on we will encounter logical objects, which form an integral component of computer programming as they allow us to perform operations only under certain conditions, thereby dictating the flow of our programme depending on, e.g. values taken by various objects during the entire process. 1.3.1 Vectors R has been optimised to work with vectors, and any time we can modify a piece of code to exploit this we can likely massively speed up the time it takes to run. A vector is like a list of numbers, and can be created in R in a number of ways, most directly with the function c(), as in the following. x &lt;- c(1, 3, -8, 5) x ## [1] 1 3 -8 5 A lot of mathematical operations we looked at before also work for vectors applying one of the mathematical operators we saw before to a vector and a scalar (a single number) will map the resulting expression to each element in the vector: y &lt;- 2 x+y ## [1] 3 5 -6 7 x*y ## [1] 2 6 -16 10 x/y ## [1] 0.5 1.5 -4.0 2.5 But what happens if y were a vector like x? y &lt;- c(1, 1, 2, 7) x + y ## [1] 2 4 -6 12 x*y ## [1] 1 3 -16 35 You can see that R has applied the operator element-wise, i.e. the first element in x and the first element in y, the second element in x and the second element in y, etc. Note: this works as desired when both x and y have the same length. However, R will still try to make the operation work even if they are not of the same length, by repeating the shorter vector as many times as needed. Still, to ensure things work as we want, it is prudent to try and only use same length vectors unless youre perfectly confident in how R will work otherwise. You can retrieve the length of a vector with the length function: length(x) ## [1] 4 length(y) ## [1] 4 Another way to create vectors with a regular structure is with the seq() function: x &lt;- seq(from = 1, to = 10, by = 1) x ## [1] 1 2 3 4 5 6 7 8 9 10 y &lt;- seq(from = 1, to = 10, by = 2) y ## [1] 1 3 5 7 9 Instead of defining the gap between successive entries (the by argument above), we could equally decide the length of the vector we want out: x &lt;- seq(from = 1, to = 10, length = 10) x ## [1] 1 2 3 4 5 6 7 8 9 10 If we want simply to create a sequence of integers, we can also use the notation from:to: x &lt;- 1:10 x ## [1] 1 2 3 4 5 6 7 8 9 10 y &lt;- 6:-6 y ## [1] 6 5 4 3 2 1 0 -1 -2 -3 -4 -5 -6 Indexing a Vector If we want to extract only some of the elements in a vector, we can use the syntax vector[indices], where the indices inside [] is either an integer, telling R which entry in the vector to extract, or a vector of such integers: y[1] ## [1] 6 y[3:7] ## [1] 4 3 2 1 0 z &lt;- y[c(8, 1)] z ## [1] -1 6 NB: The indices in a vector of length L in R are 1:L, which is different from many other programming languages, which use zero as the first index 1.3.2 Matrices and Arrays A matrix may be seen as a simple extension of a vector, and is a table of numbers; like stacking vectors next to one another in columns (or turning them sideways (transposing them) and stacking them on top of one another in rows). As with vectors, there are multiple ways in which we can create a matrix. Two simple ways to construct a matrix are column-binding (with the function cbind) and row-binding (with the function rbind): m &lt;- cbind(1:5, seq(-6, 7, length = 5)) m ## [,1] [,2] ## [1,] 1 -6.00 ## [2,] 2 -2.75 ## [3,] 3 0.50 ## [4,] 4 3.75 ## [5,] 5 7.00 p &lt;- rbind(1:3, c(-8, 1, 19)) p ## [,1] [,2] [,3] ## [1,] 1 2 3 ## [2,] -8 1 19 Similar to how R will replicate a vector multiple times if we try to apply operators to two vectors of different length, R will try to bind vectors of different length: m &lt;- cbind(1, c(2, 8, -1)) m ## [,1] [,2] ## [1,] 1 2 ## [2,] 1 8 ## [3,] 1 -1 p &lt;- rbind(1:2, c(1, 3, -3, 5, 6, 7)) p ## [,1] [,2] [,3] [,4] [,5] [,6] ## [1,] 1 2 1 2 1 2 ## [2,] 1 3 -3 5 6 7 As an alternative we can create a matrix directly with the function matrix(data, nrow, ncol). By default this function will take the entries in the vector data and fill the matrix column by column: m &lt;- matrix(1:10, 5, 2) m ## [,1] [,2] ## [1,] 1 6 ## [2,] 2 7 ## [3,] 3 8 ## [4,] 4 9 ## [5,] 5 10 If we want R to instead fill the matrix row by row, we set the optional argument byrow = TRUE: m &lt;- matrix(1:10, 5, 2, byrow = TRUE) m ## [,1] [,2] ## [1,] 1 2 ## [2,] 3 4 ## [3,] 5 6 ## [4,] 7 8 ## [5,] 9 10 The transpose of a matrix is simply a rotation (although not in the algebraic sense) of the matrix, which swaps its rows and columns, and is obtained with the function t(): t(m) ## [,1] [,2] [,3] [,4] [,5] ## [1,] 1 3 5 7 9 ## [2,] 2 4 6 8 10 When we start to use the algebra of vectors and matrices the transpose will become increasingly important Those on the MSc Statistics will be very well acquainted with matrix algebra by now, and those on the MSc Data Science, some of whom are not yet, will learn much more in the Foundations of Data Science and AI module The size of a matrix can be obtained using dim() which outputs the number of rows and the number of columns, or nrow() and ncol() which output the number of rows and columns respectively: dim(m) ## [1] 5 2 nrow(m) ## [1] 5 ncol(m) ## [1] 2 Arrays follow the same ideas as vectors and matrices, but are more general in that they have an arbitrary number of dimensions. For example, an array with only one dimension is a vector, and one with two dimensions is a matrix. We will not be working much with arrays having more than two dimensions. Indexing Matrices The same indexing rules as we had for vectors applies for matrices, except now we need to specify which rows and which columns we want to extract. As before we use the square brackets [] to specify the indices we want, and we now separate the row and column indices by a comma: m &lt;- matrix(1:30, 6, 5) m ## [,1] [,2] [,3] [,4] [,5] ## [1,] 1 7 13 19 25 ## [2,] 2 8 14 20 26 ## [3,] 3 9 15 21 27 ## [4,] 4 10 16 22 28 ## [5,] 5 11 17 23 29 ## [6,] 6 12 18 24 30 m[1:3,c(1, 4)] ## [,1] [,2] ## [1,] 1 19 ## [2,] 2 20 ## [3,] 3 21 m[4,3] ## [1] 16 If we want all columns (or all rows) we simply leave the corresponding field empty: m[1:2,] ## [,1] [,2] [,3] [,4] [,5] ## [1,] 1 7 13 19 25 ## [2,] 2 8 14 20 26 m[,c(3, 5)] ## [,1] [,2] ## [1,] 13 25 ## [2,] 14 26 ## [3,] 15 27 ## [4,] 16 28 ## [5,] 17 29 ## [6,] 18 30 m[,] ## [,1] [,2] [,3] [,4] [,5] ## [1,] 1 7 13 19 25 ## [2,] 2 8 14 20 26 ## [3,] 3 9 15 21 27 ## [4,] 4 10 16 22 28 ## [5,] 5 11 17 23 29 ## [6,] 6 12 18 24 30 1.3.3 Lists Vectors/matrices/arrays will only allow one to store contents which have the same object type. In fact, if you try to combine objects of different types R will coerce some of them into different types so that they can be stored together. For example my_name &lt;- &quot;David&quot; my_height &lt;- 190 class(my_name) ## [1] &quot;character&quot; class(my_height) ## [1] &quot;numeric&quot; c(my_name, my_height) ## [1] &quot;David&quot; &quot;190&quot; class(c(my_name, my_height)) ## [1] &quot;character&quot; Since it is not clear how to turn the character object \"David\" into a numeric object, but it is comparatively clear how to convert the numeric 190 into a character, R coerces the object my_height into a character so that it can store them together. Lists are an alternative way of storing multiple objects, but unlike vectors, matrices, arrays, etc. they dont have a rigid structure. For example, we can create a list containing a numeric object, a vector of character objects and an entire other list. my_list &lt;- list(my_height, c(my_name, &quot;boo&quot;), list(1)) my_list ## [[1]] ## [1] 190 ## ## [[2]] ## [1] &quot;David&quot; &quot;boo&quot; ## ## [[3]] ## [[3]][[1]] ## [1] 1 When printing a list R hints at how we can extract individual fields, i.e. with the use of double brackets [[]]: my_list[[2]] ## [1] &quot;David&quot; &quot;boo&quot; Note that there is a clear distinction between list[[index]] and list[index] where the former will extract the indexed object itself from the list, whereas the latter is a sub-list containing only the indexed object. However, lists also provide a more appropriate means of storing and extracting their contents. The object my_list above is an un-named list since we didnt give names to the different fields. We can either name the fields in a list after creating it or we can name them when we initially create it. When we want to isolate a particular field by name we use the dollar sign, i.e. list$fieldname. names(my_list) &lt;- c(&quot;my_height&quot;, &quot;name_and_boo&quot;, &quot;another_list&quot;) names(my_list) ## [1] &quot;my_height&quot; &quot;name_and_boo&quot; &quot;another_list&quot; my_list$another_list ## [[1]] ## [1] 1 my_list_2 &lt;- list( name = &quot;David&quot;, height = 190, birth_place = list(Country = &quot;South Africa&quot;, City = &quot;Johannesburg&quot;) ) my_list_2$name ## [1] &quot;David&quot; my_list_2$birth_place ## $Country ## [1] &quot;South Africa&quot; ## ## $City ## [1] &quot;Johannesburg&quot; We will work a lot in this module with special types of lists called a data frames. 1.3.4 Summary There is a priority of operations in arithmetic statements in R: raising to a power (^), multiplication and division (* and /), addition and subtraction (+ and -). Operations are performed left to right, and priorities are overridden by parentheses. 6 * (5 + 2)^2 / (2 + 6 - 9) / 7 * 2 ## [1] -84 Results can be stored in objects using the assignment statement: &lt;-. Object names are case sensitive. Type the name into the console and R will print the contents. x &lt;- 0:10 y &lt;- x^2 y ## [1] 0 1 4 9 16 25 36 49 64 81 100 Vectors (and matrices) are lists of numbers with special mapping properties. To create a vector of numbers in the console, we use, c(), the combine function. Arithmetic operations work on vectors with the same priorities as single numbers. Use seq() to create a vector containing a specific number sequence, the parameters; by and length can be used to control the sequence. days_long &lt;- c(1, 2, 3, 4, 5, 6, 7) days_quick &lt;- 1:7 half_days &lt;- seq(1, 7, by = 0.5) days_long + 0.5 ## [1] 1.5 2.5 3.5 4.5 5.5 6.5 7.5 2 * half_days ## [1] 2 3 4 5 6 7 8 9 10 11 12 13 14 c(days_long, days_quick) ## [1] 1 2 3 4 5 6 7 1 2 3 4 5 6 7 You can extract selected indices from a vector (or matrix) using the syntax vector[indices] (or matrix[row_indices,column_indices]) days_long[1:3] ## [1] 1 2 3 Lists provide a more dynamic means of storing objects as they do not have a rigid dimension nor are they restricted to containing objects of the same type. We can name the fields in a list and extract them by name using the dollar sign syntax. We can also add fields after creating a list. my_new_list &lt;- list(to_do = list(item1 = &quot;finish this list&quot;)) my_new_list$completed &lt;- list(item1 = &quot;First section of Intro to R&quot;) my_new_list$to_do$item2 &lt;- &quot;Second section of Intro to R&quot; my_new_list ## $to_do ## $to_do$item1 ## [1] &quot;finish this list&quot; ## ## $to_do$item2 ## [1] &quot;Second section of Intro to R&quot; ## ## ## $completed ## $completed$item1 ## [1] &quot;First section of Intro to R&quot; 1.4 Basic plots Visualisation is an extremely powerful and efficient way of communicating information. As statisticians and data-scientists we often rely on visualisations of data, or of the outputs of our statistical models, in order to use these to make effective decisions. The plot() function in R is a versatile tool for producing these visualisations There are a number of other general purpose plotting options, but for this module we will only use the basic plotting functionality For those who are interested the ggplot package and its derivatives have become very popular, and use a modular approach for producing visualisations The way in which plot() produces a graphic depends on the type of arguments given to it. For example, if we try to plot a vector we will see the values in the vector along the vertical axis, and the positions of these values will be equally spaced on the horizontal axis: x &lt;- c(1, 3, 7, -2, 9, 5, 2) plot(x) If we wish to see the relationship between two vectors of equal length, we can provide both as arguments to plot(). Values of the first will be plotted against the horizontal axis and the corresponding values of the second along the vertical axis: y &lt;- x^2 plot(x, y) By default we see this relationship via a set of points in the \\((x,y)\\) plane, but we can also plot the graph of these as we would draw a function by using the argument type = 'l' (l for line). However, note that R will try to connect the points to form this graph in the order they appear. plot(x, y, type = &#39;l&#39;) To correct this behaviour we should provide x in either increasing or decreasing order (and re-order the elements in y appropriately): x_sorted &lt;- sort(x) y_sorted &lt;- x_sorted^2 plot(x_sorted, y_sorted, type = &#39;l&#39;) We could also do this by finding the ordering of the indices in x which leads to them being sorted: ord &lt;- order(x) plot(x[ord], y[ord], type = &#39;l&#39;) 1.4.1 Example: Gravitational Force Newtons law of gravitation tells us that the acceleration due to the gravitational force exerted by an object is determined by the equation \\[ a = G \\frac{M}{r^2} \\ m.s^{-2}, \\] where \\(G \\approx 6.6728 \\times 10^{-11}\\) is the gravitational constant, \\(M\\) is the mass of the object and \\(r\\) is the distance from its centre of gravity. This means that gravity at the surface of Earth (which weighs approximately \\(5.9736 \\times 10^{24} \\ kg\\) and has an average radius of approximately \\(6.37 \\times 10^6 \\ m\\)) is approximately equal to \\(\\frac{6.6728 \\times 5.9736}{6.37^2} \\times 10^{-11+24-12} \\approx 9.82 \\ m.s^{-2}\\). But what if we were to shift higher and higher above the surface of the earth, say by a distance of \\(d\\) metres? First lets set up what is fixed (our constants): G &lt;- 6.6728 * 10^(-11) M_earth &lt;- 5.9736 * 10^24 r_earth &lt;- 6.37 * 10^6 Then lets consider distances up to the distance from Earth to the moon (approximately \\(3.844\\times 10^8 \\ m\\)) d &lt;- seq(0, 3.844*10^8, length = 1000) plot(d, G*M_earth/(r_earth+d)^2, ylab = &#39;Acceleration&#39;, xlab = &#39;Distance above surface of Earth&#39;, type = &#39;l&#39;) We can see that as \\(d\\) increases the force quickly decreases towards zero (according to an inverse square law). Without a sense of the scale of \\(d\\), however, it may be unclear how to interpret this in a realistic sense. We can add vertical or horizontal lines to a plot using the function abline(v = ) or abline(h = ). For example, lets add a vertical line indicating the height at which GPS satellites orbit, approximately \\(20 200 \\ km = 2.02 \\times 10^7 \\ m\\): plot(d, G*M_earth/(r_earth+d)^2, ylab = &#39;Acceleration&#39;, xlab = &#39;Distance above surface of Earth&#39;, type = &#39;l&#39;) abline(v = 2.02*10^7, lty = 2) We can add more interesting lines to a plot using the function lines(), which behaves similar to plot(, type = 'l') but adds the line to an existing plot rather than creating a new one. The function points() can also be used, and behaves almost exactly as plot() only adding its data to an existing plot rather than creating a new one. Specifically, points(, type = 'l') and lines() are functionally equivalent. Lets add a line showing acceleration due to the moons gravity, if we consider that the vertical trajectory as we move above the surface of Earth is directly in the direction of the moon. At a distance \\(d\\) metres above the surface of Earth they will be \\(3.844\\times 10^8 - d\\) metres above the surface of the moon. Incorporating the mass and radius of the moon, we have: M_moon &lt;- 7.346 * 10^22 r_moon &lt;- 1.737 * 10^6 d_moon_to_earth &lt;- 3.844*10^8 plot(d, G*M_earth/(r_earth+d)^2, ylab = &#39;Acceleration&#39;, xlab = &#39;Distance above surface of Earth&#39;, type = &#39;l&#39;) lines(d, G*M_moon/(r_moon + (d_moon_to_earth-d))^2, col = 2) Try modifying the above by replacing the call to the function lines() with one using the function points(). For most of this trajectory the force from both bodies is very small relative to the gravitational force experienced at the surface of either of them (of course these forces also operate in opposite directions). Gravity on the surface of the moon is much less than on Earth, by roughly 6 times: a_earth &lt;- G*M_earth/r_earth^2 a_moon &lt;- G*M_moon/r_moon^2 a_earth/a_moon ## [1] 6.046527 1.4.2 Summary The plot() function produces a plot of \\((x, y)\\) points using two vectors. Whether points are used or a line is draw is determined by the type parameter (type = \"l\" for lines, or type = \"p\" for points). The points() function has the same parameters as plot() but will add its data to an existing plot, rather than create a new one. r &lt;- 1:1000 g &lt;- 1 / r^2 plot(x = r, y = g, type = &quot;l&quot;, col = &quot;red&quot;) h &lt;- g * 2 points(x = r, y = h, type = &quot;p&quot;, col = &quot;blue&quot;) The function abline can be used to add a straight line to a plot, either providing arguments a and b for the intercept and slop, or one of v for a vertical line or h for a horizontal line x &lt;- 1:100 y &lt;- x + sin(x) plot(x, y) abline(a = 0, b = 1, col = 2) 1.4.3 Exercises The exercises that follow can be carried out using the techniques and functions you have learned so far. Expect to make some mistakes. But if you start to get annoyed either take a break or ask for help. Evaluate the following expressions: \\(\\frac{49^2 - 542}{42 + 63^2}\\) \\(a^2\\) for \\(a = 1, 2, \\ldots, 10\\) Create plots for different powers of \\(x\\): Create a vector, \\(x\\), which goes from -100 up to 100. Create a new plot showing \\(y = x^2\\). Add a red line to the existing plot showing \\(y = x^4\\). 1.5 R scripts Some of you may start to be feeling at least slightly frustrated having to type commands line by line in to the console panel. Particularly, if you are like me and you often make mistakes, meaning that you have to re-enter the whole line. So far we have only written fairly short statements, and typically these have not depended on one another to a large extent. However, as our code begins to become more and more complex, potentially including intricate conditional components, it is very easy for small mistakes (bugs) to compound and cause our entire program to fail, or to produce erroneous outputs/results. It should be clear that having to re-type all of the statements coming after the mistake (assuming there is only one, and that we are able to locate it) is absurd, and that keeping a record of the statements comprising a piece of work in an appropriate file is an obvious answer. Fortunately, like all programming languages, R allows us to maintain a list of commands in a file we refer to as an R script. This script can then be edited (correcting our mistakes) as well as processed, either as a whole or line-by-line. The main advantage of this is that we can save and close a script, switch off our computers and go home, with the intention of carrying on at a later time. 1.5.1 Creating and saving an R script Let us put the commands we used for calculating acceleration due to Earths gravity into an R script. To do this in RStudio, go to File &gt; New &gt; Script (or pressing Ctrl + Shift + N). This will have created a Script panel, resizing the Console window to be smaller. The Script panel has some useful features that will be discussed in more detail, but the nicest one is that as we type out commands in the script, they will be colour-coded, making it easier to read. One-by-one, type the commands used for the gravity calculations into the script panel. Your script should look like this: G &lt;- 6.6728 * 10^(-11) M_earth &lt;- 5.9736 * 10^24 r_earth &lt;- 6.37 * 10^6 d &lt;- seq(0, 3.844*10^8, length = 1000) a &lt;- G*M_earth/(r_earth+d)^2 plot(d, a, ylab = &#39;Acceleration&#39;, xlab = &#39;Distance above surface of Earth&#39;, type = &#39;l&#39;) In the script panel, all of the usual text editing tools are available; cut, copy, paste, highlight and delete. If you want to split a command over more than one line, then just break it after a comma, or where it is obviously not complete. For example, as far as R is concerned, these two ways of creating the d object are exactly the same: d &lt;- seq(from = 0, to = 20350 * 1000, by = 1000) d &lt;- seq(from = 0, to = 20350 * 1000, by = 1000) We can also break calculations over multiple lines, as long as it is obvious that we plan on entering more code, such as ending a line with a plus: 1 + 2 + 3 + 4 + 5 + 6 + 7 + 8 ## [1] 36 1 + 2 + 3 + 4 + 5 + 6 + 7 + 8 ## [1] 36 When we enter commands into an R script, these are not being processed by R. It is good to think of writing in a script as a way of preparing commands for R. Note that, even with a script open, you are still free to type directly into the console. To do so, simply click into the console panel and start typing. Click on the script to carry on writing there. A good habit when writing scripts is to save frequently. Go to File &gt; Save and save the script as Lab0-Gravity.R, somewhere on your H drive where you will be able to find it again. You may want to create a folder called R-Programming and save the script in there. If you double click a file with the .R extension, RStudio will automatically try to open it, just like how Word is used to open .doc and .docx files. 1.5.2 Running an R script Given that you have typed out and saved the gravity commands, we are now ready to let RStudio run it! To do this highlight all the lines in your script and then click on the run button: By using the highlight-and-run technique we can highlight just small parts of our script that we want to re-run. Also, using the keyboard shortcut Ctrl + Enter (or Ctrl + R) means you do not have to click the run button. And if no code is highlighted, then it will simply send the current line (wherever the cursor is located) to the console. If you want to run the script line-by-line, you can navigate to the first line you want to run and then repeatedly press Ctrl + Enter (or Ctrl + R). Each time a new line will be run. This can be especially useful when debugging. Another way to run scripts is by sourcing them. To do this you need to know the full path of where the R script is stored, as well as the name of the script. For example, say, I store my gravity script in the following location: H:\\My Awesome R Scripts\\Lab0-Gravity.R. Then in order to source it I would enter into the console the following command: source(&quot;H:/My Awesome R Scripts/Lab0-Gravity.R&quot;) There are two things you must remember; (i) on Windows the backslashes in filepaths must be entered in R as either double backslashes or as forward slashes, and (ii) put the complete file name in quotes. If you have got the path or file name wrong, R will tell you that it cannot find the file. If there is something wrong with your code when sourcing, R will give you a more or less useful error message, at least making it obvious where the error occurred. 1.5.3 Adding comments to your script The best programming habit you could ever develop is putting informative comments in your code. Any line in your R script that begins with # will be ignored by R. Similarly if you use a # in the middle of a line, the remainder of that line will ignored. By using # we are able to add comments to our commands, and even comment-out our code. Type out the following script for calculating gravity, include the comments. Once you have typed out the script be sure to save it, as we will be modifying it soon. You can also use Ctrl + C in RStudio. Everyone prefers a different amount of commenting. So I strongly recommend you to develop your own style (this no excuse for having no comments). For more complex code, a general recommendation is to comment less on what the code is doing but rather on why the code is doing what it is doing (the example below is not following this principle too much). # Look at the relationship between acceleration due to # gravity and distance from the Earth&#39;s surface. # ref: http://en.wikipedia.org/wiki/Gravity_of_Earth # Constants ---- # Gravitational constant (m^3 kg^-1 s^-2) G &lt;- 6.6728 * 10^-11 # Approximate average radius of the Earth (m) r_earth &lt;- 6.37 * 10^6 # Approximate mass of the Earth (kg) M_earth &lt;- 5.9736 * 10^24 # Generate distance vector from Earth&#39;s surface (0m) # up to the distance between Earth and the moon (384,400 km) d &lt;- seq(from = 0, to = 3.844*10^8, length = 1000) # We measure from the centre of the Earth to the centre of # the other object: r &lt;- r_earth + d # Earth&#39;s gravitational acceleration based on distance a &lt;- G * M_earth / r^2 # Produce the plot ---- # Set distance to km instead of m for x-axis labels d_km &lt;- d / 1000 plot(x = d_km, y = a, type = &quot;l&quot;, xlab = &quot;Distance above surface of Earth (km)&quot;, ylab = &quot;Acceleration&quot;) 1.5.4 Summary An R script is a prepared list of R commands that are processed sequentially by R from top to bottom. Using the script editor in RStudio, scripts can be written, edited, saved, and run. To create a new script go to File &gt; New &gt; R Script. To open an existing scrip, File &gt; Open. If you have more than one script open, RStudio will create tabs across the top of the script panel. R scripts have the file extension .R, make sure you include this when saving your script for the first time. Running an R script means sending the contents of the script to the console. Portions of the script can be sent by highlighting the relevant code and click the Run button or using the shortcut Ctrl + Enter. Scripts will often contain bugs. Bugs are usually caused by a missing common or bracket, or from an incorrect object name. In order to track down bugs, try running your script one section at a time. Followed by one line at a time, when you have found the offending section. By starting a line with # we are able to add comments to our code, as R will ignore any text to the right of this symbol. Adding comments, to break-up your code into sections as well as explain the purpose of each section, is crucial for future you to understand the purpose of your script, what it is trying to achieve, and what features are still yet to be added. 1.6 R markdown R Markdown is a file format for making dynamic reports that combine code, text, and formatting. An R Markdown document is written in markdown, an easy-to-write plain text format, and contains chunks of embedded R code. Differently from an R script, that is a simple text file, an R Markdown script is a text document that combines code with text and formatting to create a dynamic report. The code is organized into chunks that can be executed to produce output, such as tables and plots. You can interactively access and edit the .Rmd file in Rstudio. In addition, the .Rmd document can be rendered (exported) into various output formats, such as HTML, PDF, or Word, making it easy to share and present your results and ensure that they can reproduce your results. You will be required to submit an .Rmd for your coursework, as well as the rendered output document in html. In fact the very notes you are reading now were written entirely in .Rmd, and compiled to be an interactive webpage! 1.6.1 R markdown basics To write paragraphs, such as this one you are reading now, simply separate text in a .Rmd file with a blank line. To write titles and subtitles, you can use the # symbol followed by a space and the text of the title. The number of # symbols determines the level of the heading, with one # being the highest level and six # being the lowest. For example: # Title ## Subtitle R chunks are used to embed R code in an R Markdown document. They are enclosed in triple backticks and start with {r}: This will generate an R chunk, as the one below, that will get evaluated, including its output. head(cars) ## speed dist ## 1 4 2 ## 2 4 10 ## 3 7 4 ## 4 7 22 ## 5 8 16 ## 6 9 10 Try to create a new .Rmd file from R studio and give it a try yourself! Go on file &gt; new file &gt; R markdown. There will be a window popping up, fill in the details and see for yourself the new .Rmd document! A video illustrating R markdown, produced by one of my colleagues, can be found below: Video These whole set of lecture notes was generated in Rmarkdown! If you want to learn more about .Rmd, see the official webpage Using R Markdown. 1.6.2 Exercise Based on your gravity script, edit your code so that the gravity plot includes a horizontal line illustrating the magnitude of gravitation acceleration at the surface of the moon. Work in an R markdown to make sure your analysis, from the start to the final plot, is reproducible. Add some textual notes in the .Rmd file which describe the contents of the plot. 1.7 Functions Functions are a really important concept both in mathematics and computing. They are typically used to abstract away detailed computations that perform an operation (or set of operations) on some input, and return an output. In mathematics, we are well used to defining functions, such as \\[ f(x) = x^2 \\] or \\[ f(x; \\mu, \\sigma) = \\frac{1}{\\sqrt{2\\pi}\\sigma} \\exp\\left(-{\\frac{(x - \\mu)^2}{2\\sigma^2}}\\right). \\] Mathematical functions have (input) arguments (\\(x\\), and \\(x, \\mu, \\sigma\\) respectively above), and typically return the result of evaluating the expression on the right hand side of the equality sign. Functions in computer languages are similar to mathematical functions, except they can be regarded as entire computations in themselves, perhaps performing an entire set of steps such as training a neural net by gradient descent methods. Indeed, in our daily programming, it is a very good idea to use functions liberally to avoid un-necessary or repetitive code that can be prone to errors. We have already used several functions: c(...) Combines objects often into a vector, where the comma-separated argument list represented by ... takes objects or values to be combined. seq(from, to, by) Generates a vector containing a seq of numbers specified by the arguments from, to and by. plot(x, y) Creates a scatter plot of points using arguments x and y as co-ordinates for the points. length(x) Returns the number of elements in the vector x Here are some other functions that can be useful when working with vectors: sum(x) Returns the total from adding all the elements in x together prod(x) Returns the total from multiplying all the elements in x. Note that prod is short for product which is the mathematical name given to this process. sqrt(x) Returns a vector of the same length of , but each element is the square-root of the corresponding element in x. max(x), min(x) Returns the maximum and minimum element from a vector, respectively. Try them out: x &lt;- -1:10 sum(x) ## [1] 54 prod(x) ## [1] 0 min(x) ## [1] -1 max(x) ## [1] 10 sqrt(x) ## [1] NaN 0.000000 1.000000 1.414214 1.732051 2.000000 2.236068 2.449490 2.645751 ## [10] 2.828427 3.000000 3.162278 For the sqrt(x) command, we were giving a warning saying NaNs had been produced. NaN is short for Not a Number and in this instance they have occurred because we tried to calculate the square-root of a negative number (which do not exist as real numbers). Once an element becomes Not a Number, that status will persist throughout the rest of our calculations. For example: x &lt;- -2:2 y &lt;- c(10, 5, 0, -5, -10) sqrt(x) + sqrt(y) ## [1] NaN NaN 0 NaN NaN Because sqrt(x) and sqrt(y) produced NaNs that were in different places, this resulted in us producing four NaNs when adding the two square-rooted vectors together. 1.7.1 Creating New Functions R, like all modern programming languages, allows you to create your own functions. This is a great time saver when you are repeatedly performing the same computation, albeit on different data sets. Writing your own functions is definitely a habit you should develop. Our previous code could not be easily re-used without copy and pasting, and it should be pretty obvious that copying blocks of code all over the place greatly increases the chance of errors  maybe you dropped your coffee on your keyboard, or your cat inadvertently hit the delete key somewhere in a long script! Instead, we can abstract our gravity computation into a function called calc_gravity, which should be added to a new R script: calc_gravity &lt;- function(distance) { # constants G &lt;- 6.6728 * 10^(-11) M_earth &lt;- 5.9736 * 10^24 r_earth &lt;- 6.37 * 10^6 # calculation r &lt;- r_earth + distance a &lt;- G * M_earth / r^2 return(a) } Now run the code. You will see nothing really happened, and by now you shouldnt expect it to: we assigned the result of evaluating the expression function(distance) { ... } to an object calc_gravity. We can think of this as the keyword function telling R to build a function which encapsulated the gravity computation taking a single argument distance, which we assign to the name calc_gravity. This gives us three major wins: We can test calc_gravity to ensure it returns correct results over a range of values for distance; We can now use the function in our own code, leading to shorter scripts with fewer opportunities for errors; If we choose the function name to be descriptive of the computation it represents, we will have more readable code. If you check the Environment panel, or use ls() in the console to list the contents of the environment, you should see that calc_gravity is present. To check that it works try: calc_gravity(distance=0) ## [1] 9.82348 This matches our earlier calculations (9.82), so we have good hope that we have implemented it correctly. The single argument (distance) could simply be a number, like zero in the above test, but R (unlike many other languages) does not force us to commit to the types of function arguments, and as long as it is able to perform all the operations/calculations needed to apply the function it will work. For example, we could produce the same plot we had before by passing a vector of distances as an argument: d &lt;- seq(0, 3.844*10^8, length = 1000) plot(d, calc_gravity(d), ylab = &#39;Acceleration&#39;, xlab = &#39;Distance above surface of Earth&#39;, type = &#39;l&#39;) Naming functions There are only two hard things in Computer Science: cache invalidation and naming things.  Phil Carlton The name of the function calc_gravity and its argument distance are just names that we chose, and you may wish to consider others that might be more meaningful or easy to remember. There are a few guidelines when naming functions: Names should be lowercase. Use an underscore, _, to separate words within a name. Function names should use verbs, as functions do things. Argument names should use nouns, as arguments are things. Strive for names that are concise and meaningful (this is not easy!). Avoid existing function names in R, such as length(). Note that how we name functions will not affect how (or whether) they operate, but if you stick to these guidelines, then your coding-life just got a lot easier. This is mainly because it is easier for you to remember and guess what you have called your functions. One exception where R will not allow you to name an object as you wish is if it starts with a number, e.g. 1_function_name is not allowed, but function_name_1 is. The same rules above apply for objects. Except you should try to use nouns rather than verbs. Return values: The code used to calculate gravity sits between two curly-brackets. The return(a) statement then defines the output of the function, and should also be the last command inside your function. In this case, we simply return a single value we calculated. But for more complex functions, it could be a vector, a plot or a model. Scope: When creating your own functions there are two important things to remember regarding how R stores and interprets them: Any objects created inside a function are said to be in the local scope of that function. This means that they exist only inside your function, so you do not need to worry about over-writing objects with the same name outside the function. Local objects are generally not available outside of the function, i.e. from the global scope. The way these values (or computations involving those objects) can be communicated from the local to global scope is via the return() statement at the end. It is possible to create/modify objects in the global scope within a function using &lt;&lt;- instead of &lt;-, however this is considered poor coding practice and is strongly discouraged. Although we can see our function in the Environment browser, the new function is not permanent. It will disappear when we exit RStudio. This is why we store our functions in scripts (or even packages/libraries), so we can source them at the start of session to get back what we need. 1.7.2 Default values for function arguments Now suppose we want to calculate the gravity for the other planets. To do this we would modify our calc_gravity() function, so that it includes arguments for the mass and radius of a planet. Right now, it assumes we are only interested in Earth. Make the following modifications in your script: calc_gravity &lt;- function(distance, mass, radius) { # constant G &lt;- 6.6728 * 10^-11 # calculation r &lt;- radius + distance a &lt;- G * mass / r^2 return(a) } In order to update calc_gravity() so that it includes these additional arguments you will need to run the above code. This means that simply passing the distance will no longer work: calc_gravity(distance = 0) ## Error in calc_gravity(distance = 0): argument &quot;radius&quot; is missing, with no default We now have to include mass and radius: calc_gravity(distance = 0, mass = 5.9736 * 10^24, radius = 6.37 * 10^6) ## [1] 9.82348 But, say, that 90% of the time when we are doing these calculations they are in the context of Earth. Then to be more efficient, rather than continuously having to give the mass and radius of Earth, we could set the arguments to have these as default values: calc_gravity &lt;- function(distance = 0, mass = 5.9736 * 10^24, radius = 6.37 * 10^6) { # constant G &lt;- 6.6728 * 10^-11 # calculation r &lt;- radius + distance a &lt;- G * mass / r^2 return(a) } This means that if one simply calls calc_gravity() without specifying any arguments, the function will assume they are interested in in the gravity at Earths surface. calc_gravity() ## [1] 9.82348 But we are free to override these defaults: # On the surface of Saturn calc_gravity(mass = 5.683 * 10^26, radius = 5.8 * 10^7) ## [1] 11.27275 1.7.3 Summary Functions exist to reduce the amount of repetitive work we do. By creating our own functions, we write out a task or calculation once and then can use it many times. Useful functions for working with vectors include: length(), sum(), prod(), cumsum(), sqrt(), min(), and max(). When creating our own functions we also define the names of the functions arguments, their default values (if any), and what the function returns. In your script: say_hello &lt;- function(person = &quot;World&quot;) { msg &lt;- paste(&quot;Hello &quot;, person) return(msg) } In the console: say_hello() ## [1] &quot;Hello World&quot; say_hello(&quot;Bob&quot;) ## [1] &quot;Hello Bob&quot; Guidelines you should try to stick to when naming functions: Names should be lowercase. Use an underscore, _, to separate words within a name. Generally, function names should use verbs, as functions do things. Strive for names that are concise and meaningful (this is not easy!). Avoid existing function names in R, such as length(). When naming objects, follow the same guidelines but instead use nouns instead of verbs. 1.7.4 Exercises Rewrite your gravity script, implementing the latest function for calc_gravity. Perform the following calculations. You will need to do a bit of research to find the necessary constants. What is the gravity at the surface of each of the planets in our Solar System (Mercury, Venus, Earth, Mars, Jupiter, Saturn, Uranus, Neptune)? Produce a plot of the gravity on Mars, from the surface (0m) up to the orbit height of Phobos (Mars first moon). Hint: Use the &quot;semi-major axis&quot; of Phobos&#39; orbit as the orbit height. This is because Phobos has an elliptical orbit. Figure 1.1: The Sun and planets of the Solar System. Sizes but not distances are to scale. (Image from Wikimedia Commons) A good recipe book will state all its baking temperatures in terms of Gas Mark, degrees Celsius, and degrees Fahrenheit. A lazy book will pick one, and simply provide a conversion table. The rubbish ones will offer no such table. In those instances, I often have to manually convert the numbers myself. Or rather I write a function that I can repeatedly use (which is also what you are about to do). Create and save a new script for this exercise. Look up the calculation for converting Celsius to Fahrenheit. Implement a function which takes a temperature in Celsius and returns it in Fahrenheit, call it degC_to_degF. Look up the conversion for Fahrenheit to Gas Mark, and implement this as degF_to_gas(). Hint: You may need to make use of the `ceiling()`, `floor()` and `round()` functions: x &lt;- 5.4567218 ceiling(x) ## [1] 6 floor(x) ## [1] 5 round(x) ## [1] 5 round(x, 3) ## [1] 5.457 Implement the conversion of Celsius to Gas Mark, degC_to_gas(), using the previous two functions. Confirm the following approximate conversions: Celsius Gas Mark 140 1 150 2 160 3 180 4 190 5 200 6 220 7 230 8 240 9 1.8 Getting Help For help concerning a known R function, you can use the help function. For instance, to get a listing of information about the seq() function, type the following into the console: help(seq) To get the top-level help page for a package, for example the base package, issue: help(package=base) Help pages are all formatted to have the same sections (additional optional section are possible as well), the most important ones are the first three; Description, Usage and Arguments. These will help remind you how a function works, and what its arguments are. There is a shortcut to loading a help page, simply prefix the name of the function with a question mark: ?seq For broader help regarding statistical programming in R look in the library for books such as: Dalgaard, P. (2000). Introductory Statistics with R. Venables W.N. and Ripley B. D. (1999). Modern Applied Statistics with S-Plus. Compared to these notes, these books offer an alternative explanation regarding how to use R. 1.8.1 Ask questions Beyond searching the Internet, you can ask specific questions at www.stackoverflow.com, a searchable forum of questions and answers about all aspects of computer programming. StackOverflow has answered (and archived) thousands of questions related to R programming. So, it is likely that someone else has already asked your question and also got an answer. Use this to your advantage. You can see the latest questions tagged for R at https://stackoverflow.com/questions/tagged/r . Additionally, you can restrict a Google search to StackOverflow by appending site:stackoverflow.com to the end of your Google search. However, if you a have question that is more about statistical methodology than programming, there are also R users who are active on the the Cross Validated Q&amp;A website, a sister site to StackOverflow. 1.8.2 Keep up-to-date with the R community Read R-bloggers (www.r-bloggers.com), a blog aggregator that reposts R related articles from across the web. R bloggers is a good place to stumble across R tutorials, announcements, and example data analyses. Though other peoples code may not always be the best. 1.9 Loops and Flow So far the scripts (and functions) we have created have operated serially (or linearly) in that we have only been able to have R perform the various operations weve instructed it to in a pre-specified order. The only exception is when we manually click on or highlight specific parts of a script in order to execute them As the tasks we wish to perform become more and more complex, however, it becomes increasingly necessary to be able to dictate how R should move through a script (or the contents of a function) without having to manually click or highlight what we want it to do next. Perhaps the two most important examples of this are 1. when we want R to perform the same task or operation multiple times 2. when we want R to only perform some of the operations if certain conditions are met 1.9.1 Conditional Statements and Logical Comparisons Like all other programming languages, R can execute a block of code conditional on whether or not a statement is true. The truth of a statement needs somehow, however, to be communicated to R. This is done with the logical (or Boolean) objects TRUE and FALSE. R uses the common if / else if / else syntax in order to dictate the flow of conditional statements and operations. Using the command if(TRUE){&lt;actions&gt;} will tell R to look inside the if() function and as long as its contents are true it will then perform the actions listed inside the {} brackets. For example if(TRUE){ print(&quot;Hello&quot;) } ## [1] &quot;Hello&quot; Combining an if statement with else will tell R that if the contents of if were not true then it still needs to do something, and that something is contained similarly in {&lt;actions&gt;} notation. For example if(FALSE){ print(&quot;Hello&quot;) } else{ print(&quot;Goodbye&quot;) } ## [1] &quot;Goodbye&quot; So far this is not very useful since we had to explicitly write if(TRUE) or if(FALSE). But we can assign the values TRUE and FALSE to objects just as we have with other object types. Arriving &lt;- TRUE if(Arriving){ print(&quot;Hello&quot;) } else{ print(&quot;Goodbye&quot;) } ## [1] &quot;Hello&quot; But this is still not very useful since we had to explicitly assign the value TRUE to an object before passing that object to if(). 1.9.1.1 Logical Comparisons Logical comparisons in their simplest form compare the values of two objects and are assigned a value of TRUE or FALSE, depending on the result of this comparison. Logical comparisons are therefore operators, much in the same way +, -, etc. are operators, except the evaluation of the logical comparison is a logical object. A list of the logical comparison operations available in R. Symbol Comparison &lt; Less than &gt; Greater than &lt;= Less than or equal to &gt;= Greater than or equal to == Equal to != Not equal to For example, x &lt;- 2 y &lt;- 5 x == y ## [1] FALSE x &lt;= y ## [1] TRUE x &lt; y ## [1] TRUE x &gt;= y ## [1] FALSE x &gt; y ## [1] FALSE x != y ## [1] TRUE Combining logical comparisons with the if/else commands allows us to create much more dynamic code. The function runif(n, min, max) will produce \\(n\\) random numbers between min and max. Lets use this function to generate a random time in the day between 06:00 and 18:00. Depending on whether it is morning or afternoon we can print an appropriate statement time &lt;- runif(1, 6, 18) if(time &lt; 12){ print(&quot;Good Morning&quot;) } else{ print(&quot;Good Afternoon&quot;) } ## [1] &quot;Good Afternoon&quot; If you run this code multiple times you will see that sometimes it prints Good Morning and sometimes Good Afternoon. What do you think the following code will do? time &lt;- runif(1, 6, 18) if(time &lt; 12){ print(&quot;Good Morning&quot;) } else if(time &lt; 13){ print(&quot;Can&#39;t talk, eating!&quot;) } else{ print(&quot;Good Afternoon&quot;) } Try it out many times and see. If you want you can also print the value of time each time to validate your understanding. Hopefully it is clear that the else if command will create a new option, which splits the Good Mornings from the Good Afternoons if it happens to be lunch time. You can add as many else ifs as you want. As an alternative we can apply Boolean operations which provide an algebra for working with logical objects. A list of the Boolean operators available in R. Symbol Phrase &amp; And (ampersand) | Or (vertical bar) ! Not or negation (exclamation) The operators &amp; and | connect two logical comparisons and return TRUE or FALSE depending on the joint truth or falsehood of the two logical comparisons. The &amp; returns TRUE only when both of the comparisons are true. While | returns TRUE if at least one of the comparisons is true. For example the following two conditionals are equivalent if(time &lt; 12 | time &gt;= 13){ print(&quot;Hello!&quot;) } else{ print(&quot;Can&#39;t talk, eating!&quot;) } ## [1] &quot;Hello!&quot; if(time &gt;= 12 &amp; time &lt; 13){ print(&quot;Can&#39;t talk, eating!&quot;) } else{ print(&quot;Hello!&quot;) } ## [1] &quot;Hello!&quot; Conditional Statements inside Functions Lets look at using conditional statements within a function. The following function will simulate the rolling of a six-sided die. Although this is a very inefficient way of achieving it, see if you can understand how it is working. roll_die &lt;- function(){ r &lt;- runif(1, 0, 1) if(r &lt; 1/6){ face &lt;- 1 } else if(r &lt; 2/6){ face &lt;- 2 } else if(r &lt; 3/6){ face &lt;- 3 } else if(r &lt; 4/6){ face &lt;- 4 } else if(r &lt; 5/6){ face &lt;- 5 } else{ face &lt;- 6 } return(face) } Note that the function takes no arguments since nothing which is needed to perform the operations inside the function is variable. 1.9.2 Loops Wrapping a block of code (like the &lt;actions&gt; in the conditional context aove) inside a for loop, means that the code can be repeatedly executed. Because of this, a for loop is classified as an iteration statement; we repeatedly iterate over the same code, with, typically, only one or two parameters (iterators) changing each time. When writing a for loop we need to provide a name for the iterator variable. This is a variable whose value will change with each application of the code inside the loop. We also need to specify a vector containing the values we wish this iterator to assume with each iteration. By far the most common situation is when we iterate over sequences of natural numbers. Lets consider an example. You may all be familiar with the Fibonacci sequence. This sequence is a simplistic description of population growth, and works as follows. Starting with 1 and 1, each subsequent number is the sum of the previous two. The following code can be used to produce the first n Fibonacci numbers ### Let&#39;s first choose a number for n, say 30 n &lt;- 30 ### Now we can initiate a vector to store the first n ### numbers in the sequence fib &lt;- numeric(n) fib[1:2] &lt;- 1 ### Now we can begin our loop. Since we already know the ### first two values we need to start adding from the ### third. We can use the iterator &quot;i&quot; for(i in 3:n){ ### Now we define the operation(s) which we want R ### to perform iteratively as i changes, starting ### from 3, then 4, then 5, etc. ### We know that we want the i-th value in the sequence ### to be the sum of the previous two. These will ### be in the indices i-1 and i-2 fib[i] &lt;- fib[i-1] + fib[i-2] } fib ## [1] 1 1 2 3 5 8 13 21 34 55 89 ## [12] 144 233 377 610 987 1597 2584 4181 6765 10946 17711 ## [23] 28657 46368 75025 121393 196418 317811 514229 832040 In the above our iterator moved through the vector 3:n and R ran the chunk of code inside the {} brackets once for each such value of the iterator. We can equally loop over the contents of any other vector. For example, we can create a vector containing the names of the planets in the solar system, and then iterate over these. planet_names = c(&quot;Mercury&quot;, &quot;Venus&quot;, &quot;Earth&quot;, &quot;Mars&quot;, &quot;Jupiter&quot;, &quot;Saturn&quot;, &quot;Uranus&quot;, &quot;Neptune&quot;) for(name in planet_names){ print(name) } ## [1] &quot;Mercury&quot; ## [1] &quot;Venus&quot; ## [1] &quot;Earth&quot; ## [1] &quot;Mars&quot; ## [1] &quot;Jupiter&quot; ## [1] &quot;Saturn&quot; ## [1] &quot;Uranus&quot; ## [1] &quot;Neptune&quot; NB: It is important to choose the names of your iterators carefully. If you use the same name as an object you have already defined then R will overwrite your other object with the value of the iterator. i &lt;- 10 for(i in 1:3){ print(i) } ## [1] 1 ## [1] 2 ## [1] 3 i ## [1] 3 Sometimes we need to repeatedly perform a set of commands but we dont know initially exactly how many times. This can be achieved with while loops, which will iteratively apply a chunk of code as long as a certain condition remains true, i.e. while it is true. That is, a while loop will continue to iterate as long as the argument to while evaluates to TRUE. Because while loops dont require an iterator, if we want the value of something to change each time then we need to do this explicitly. i &lt;- 1 while(i &lt; 10){ print(i) i &lt;- i+1 } ## [1] 1 ## [1] 2 ## [1] 3 ## [1] 4 ## [1] 5 ## [1] 6 ## [1] 7 ## [1] 8 ## [1] 9 Loops using while are particularly useful in algorithms which often rely on iterating over a set of operations until some criterion (such as convergence) is met. For example, the ratio of consecutive entries in the Fibonacci sequence converges to the so-called golden ratio of (approximately) 1.61803398875. The following code will generate the sequence until it has converged. When it comes to convergence we typically only asymptote at the limit point meaning the ratio will not exactly equal the golden ratio but will get closer and closer to it. We can set a tolerance for how close we wish to get for it to be sufficiently close to convergence. ### Set a very small tolerance tol &lt;- 10^-7 ### Set the constant to which the sequence converges GR &lt;- 1.61803398875 ### Now we can initiate the sequence and then iterate ### until convergence. We can use a variable l to ### represent the length of the sequence and tidy ### up our code a little fib &lt;- c(1, 1) l &lt;- 2 while(abs(fib[l]/fib[l-1] - GR) &gt; tol){ ### increase l as we will be adding another element ### to fib l &lt;- l + 1 fib[l] &lt;- fib[l-1] + fib[l-2] } l ## [1] 19 We see that it only took until the 19-th element in the sequence to come extremely close to convergence. 1.9.3 Exercises Modify the function which simulates the rolling of a die to instead take an argument n and return a vector with the rolls from n dice. Write a piece of code which can evaluate the proportion of times each of the values 1:6 comes up when rolling n dice. See if these proportions match what you would expect. "],["working-with-data-in-r.html", "2 Working with Data in R 2.1 Data Frames 2.2 Importing and Exporting Data 2.3 Data Plotting 2.4 Summary 2.5 Exercises", " 2 Working with Data in R In this chapter we will start to work with data objects within R, and begin to understand better how the various concepts we learned in the introductory chapter are relevant to working with data. R developers have created functions and packages for almost every laborious task that stands between us and good quantitative scientific analysis. Data input, manipulation, and output is easy in R. It is just a matter of breaking down what needs to be done into small achievable tasks, and knowing the right functions. 2.1 Data Frames We saw a number of object types in the introductory chapter, from single numbers to vectors and matrices; character strings; logical objects etc. We also saw dynamic objects like functions. When it comes to working with data, we typically have a collection of variables measured on a set of entities These could be, for example, health related variables measured on a group of individuals (the entities). A convenient way of storing these data is in a tabular format, where each row in the table represents one of the individuals/entities and each column represents one of the variables. Table-like objects for storing multiple individual objects of the same type can be achieved with the use of matrices, as we saw briefly with matrices of numbers in the previous chapter. However, when some of the variables are of a different type it becomes difficult to store them all in a matrix. Data frames are sort of like a mixture of a matrix and a list. Like a matrix, they have a tabular format, but like a list we are not restricted to having the same object types throughout. The elements in a data frame can also be accessed using the same approaches as for a matrix (indexing) or a list (by name or index). When indexing a data frame as one would a matrix, we use exactly the same syntax, i.e. dataframe[row_indices,column_indices]. The result is itself a data frame containing only the subset of the full data frame which is indexed. On the other hand, when treating a data frame as a list, the elements of the data frame correspond with entire columns of its tabular format. For example, dataframe[,1] and dataframe[[1]] operate very similarly. Lets create a data frame containing the data on the sizes and masses of the planets in the solar system. planets &lt;- data.frame( name = c(&quot;Mercury&quot;, &quot;Venus&quot;, &quot;Earth&quot;, &quot;Mars&quot;, &quot;Jupiter&quot;, &quot;Saturn&quot;, &quot;Uranus&quot;, &quot;Neptune&quot;), mass = c(3.30 * 10^23, 4.87 * 10^24, 5.97 * 10^24, 6.42 * 10^23, 1.90 * 10^27, 5.68 * 10^26, 8.68 * 10^25, 1.02 * 10^26), radius = c(2.440 * 10^6, 6.052 * 10^6, 6.371 * 10^6, 3.390 * 10^6, 6.991 * 10^7, 5.823 * 10^7, 2.536 * 10^7, 2.462 * 10^7) ) Note that the type of a data frame is a list, but the class of an object typically dictates how it is actually processed. typeof(planets) ## [1] &quot;list&quot; class(planets) ## [1] &quot;data.frame&quot; is.list(planets) ## [1] TRUE is.data.frame(planets) ## [1] TRUE To access specific contents, we can, as mentioned above, do this by indexing or by name: planets$mass ## [1] 3.30e+23 4.87e+24 5.97e+24 6.42e+23 1.90e+27 5.68e+26 8.68e+25 1.02e+26 planets[1,] ## name mass radius ## 1 Mercury 3.3e+23 2440000 planets[1,&#39;mass&#39;] ## [1] 3.3e+23 In R the class of objects is very important. The same function, for example summary() and plot(), will perform differently depending upon the class of the object. Since we created this data frame, we already know the names of the variables that are contained within it. But say we load a foreign data set, how do we know what the variables are called? Outside R, we hope that this data set also has good documentation, such as a PDF or website telling you all the details you could ever want know. But within R, we can get a list of the attributes of an object by using the names() function: names(planets) ## [1] &quot;name&quot; &quot;mass&quot; &quot;radius&quot; As well as using names(), it is also good practice to check the size of the data frame. As with matrices, the functions nrow(), ncol() and dim() can be applied to data frames. nrow(planets) ## [1] 8 ncol(planets) ## [1] 3 dim(planets) ## [1] 8 3 Ultimately, there is a great function that contains and prints all this information for your in a convenient format, str. This will print also the class of the columns and the first few observations, and can be a pretty useful function to have a first look at a new data frame: str(planets) ## &#39;data.frame&#39;: 8 obs. of 3 variables: ## $ name : chr &quot;Mercury&quot; &quot;Venus&quot; &quot;Earth&quot; &quot;Mars&quot; ... ## $ mass : num 3.30e+23 4.87e+24 5.97e+24 6.42e+23 1.90e+27 ... ## $ radius: num 2440000 6052000 6371000 3390000 69910000 ... As with lists, we can add to a data frame by simplying creating newly named variables. For example, we could estimate the average density of each planet and add this as a new variable/field ### First we can calculate the volumes of each, assuming they ### are approximately spherical volumes &lt;- 4/3*pi*planets$radius^3 ### Now we can add the densities to our data frame planets$density &lt;- planets$mass/volumes planets$density ## [1] 5423.2029 5244.9771 5511.4124 3934.1175 1327.5384 686.7827 1270.5254 1631.7267 Factor Variables It is common in statistical data to have categorical variables, indicating some subdivision of the data, such as social class, primary diagnosis, tumour stage, gender, species, etc. Such variables are stored in data files often as strings indicating their actual value, though abbreviations may be used; e.g. m for male, and f for female. But, due to the actual number of observations, it may be more sensible to store the categorical data using numerical codes; 1 for male, 2 for female. With their meaningful names then being store in the documentation for the data. Regardless of how they are originally stored, in R they should be converted to factors. A factor is a data structure which stores the categorical data as numerical codes, but has labels which make the codes meaningful. Mercury, Venus, Earth and Mars are called the terrestrial planets, as they are primarily composed of rock and metal. Jupiter and Saturn, are composed mainly of hydrogen and helium, thus are referred to as gas giants. Finally, Uranus and Neptune, are composed largely of substances with relatively high melting points, thus are often referred to separately as ice giants. We can add a variable which categorises the planets in this way planets$type &lt;- rep(c(&quot;terrestrial&quot;, &quot;gas_giant&quot;, &quot;ice_giant&quot;), c(4, 2, 2)) planets$type ## [1] &quot;terrestrial&quot; &quot;terrestrial&quot; &quot;terrestrial&quot; &quot;terrestrial&quot; &quot;gas_giant&quot; ## [6] &quot;gas_giant&quot; &quot;ice_giant&quot; &quot;ice_giant&quot; Now, as it is currently planets$type is a vector of character objects, and although it clearly delineates which items are in each of the categories we can explicitly convert this to a factor using as.factor() planets$type &lt;- as.factor(planets$type) planets$type ## [1] terrestrial terrestrial terrestrial terrestrial gas_giant gas_giant ## [7] ice_giant ice_giant ## Levels: gas_giant ice_giant terrestrial Now when we print the type variable we see that it now has what are called Levels, which are the labels which give meaning to the category codes. In this example we named the codes with meaningful labels already, however as mentioned previously it may be more convenient, when there is a large number of categories to use numeric codes but use the Levels to encode meaning pain &lt;- c(0, 1, 3, 2, 2, 1, 1, 3) pain_f &lt;- factor(pain, levels = 0:3, labels = c(&quot;none&quot;, &quot;mild&quot;, &quot;medium&quot;, &quot;severe&quot;)) pain_f ## [1] none mild severe medium medium mild mild severe ## Levels: none mild medium severe ### if you need to work with the actual string labels pain_c &lt;- as.character(pain_f) pain_c ## [1] &quot;none&quot; &quot;mild&quot; &quot;severe&quot; &quot;medium&quot; &quot;medium&quot; &quot;mild&quot; &quot;mild&quot; &quot;severe&quot; 2.1.1 Combining Data Frames Lets suppose we want to combine the contents of multiple data frames. We may, for example, be collating the data from multiple sources. When everything is nice, in that each data frame has the same collection of variables (with the same names!), then the function rbind() can be used. However, when some of the data frames are missing certain of the variables then this will give us an error. Lets create a small data frame containing newly discovered exoplanets. exoplanets &lt;- data.frame( name = c(&quot;Kepler-186f&quot;, &quot;Kepler-62f&quot;, &quot;Kepler-438b&quot;), mass = c(1.9 * 10^23, 3.32 * 10^23, 2.64 * 10^23), radius = c(1.17 * 10^6, 1.41 * 10^6, 1.12 * 10^6) ) Since this data frame does not have the fields $density and $type, which we created, trying to row-bind with planets will fail: rbind(planets, exoplanets) ## Error in rbind(deparse.level, ...): numbers of columns of arguments do not match If we create these fields for exoplanets as well then all should be fine. exoplanets$density &lt;- exoplanets$mass / (4/3*pi*exoplanets$radius^3) exoplanets$type &lt;- &quot;exo&quot; rbind(planets, exoplanets) ## name mass radius density type ## 1 Mercury 3.30e+23 2440000 5423.2029 terrestrial ## 2 Venus 4.87e+24 6052000 5244.9771 terrestrial ## 3 Earth 5.97e+24 6371000 5511.4124 terrestrial ## 4 Mars 6.42e+23 3390000 3934.1175 terrestrial ## 5 Jupiter 1.90e+27 69910000 1327.5384 gas_giant ## 6 Saturn 5.68e+26 58230000 686.7827 gas_giant ## 7 Uranus 8.68e+25 25360000 1270.5254 ice_giant ## 8 Neptune 1.02e+26 24620000 1631.7267 ice_giant ## 9 Kepler-186f 1.90e+23 1170000 28320.9232 exo ## 10 Kepler-62f 3.32e+23 1410000 28274.3179 exo ## 11 Kepler-438b 2.64e+23 1120000 44860.2046 exo Note that when binding the data frames together in this way R will have added a new level to the $type field levels(rbind(planets, exoplanets)$type) ## [1] &quot;gas_giant&quot; &quot;ice_giant&quot; &quot;terrestrial&quot; &quot;exo&quot; However, if we had incorrectly named one of the variables/fields in either of the data frames R would still not have been happy. The dplyr package contains a number of useful functions for handling data frames more effectively. For example, the function bind_rows() is functionally very similar to rbind() but if some of the data frames have different variables then the collection of all variables across all data frames being combined will be included. ### First load the dplyr library library(dplyr) ### Let&#39;s add to the planets data frame their distances from the sun planets$distance &lt;- c(58*10^6, 108*10^6, 150*10^6, 228*10^6, 779*10^6, 1400*10^6, 2900*10^6, 4500*10^6) bind_rows(planets, exoplanets) ## name mass radius density type distance ## 1 Mercury 3.30e+23 2440000 5423.2029 terrestrial 5.80e+07 ## 2 Venus 4.87e+24 6052000 5244.9771 terrestrial 1.08e+08 ## 3 Earth 5.97e+24 6371000 5511.4124 terrestrial 1.50e+08 ## 4 Mars 6.42e+23 3390000 3934.1175 terrestrial 2.28e+08 ## 5 Jupiter 1.90e+27 69910000 1327.5384 gas_giant 7.79e+08 ## 6 Saturn 5.68e+26 58230000 686.7827 gas_giant 1.40e+09 ## 7 Uranus 8.68e+25 25360000 1270.5254 ice_giant 2.90e+09 ## 8 Neptune 1.02e+26 24620000 1631.7267 ice_giant 4.50e+09 ## 9 Kepler-186f 1.90e+23 1170000 28320.9232 exo NA ## 10 Kepler-62f 3.32e+23 1410000 28274.3179 exo NA ## 11 Kepler-438b 2.64e+23 1120000 44860.2046 exo NA Notice that, rather than producing an error, bind_rows() has created NA entries in the $distance field for exoplanets. In R, NA is a special value that represents missing or undefined data. 2.1.2 Filtering We saw how we could isolate different parts of a data frame either by explicitly specifying row and column indices, or by accessing the fields in data frame as a list using dataframe$fieldname. Indexing by explicit lists of row/column indices can be risky if we arent fully on top of where different items and variables are stored. We can also isolate parts of a vector/matrix/array/data frame by specifying conditions for the indices or field names, just as we used conditional statements to direct the flow of our code. Specifically, if we have a data frame with dimensions n (rows) and p (variables/columns), then we can pass a vector of n logical objects (i.e. TRUE and FALSE) instead of specifying row indices (or p such logicals instead of column indices). R will then extract only the rows/columns associated with the TRUE indices. For example, suppose we wanted to extract the planets whose masses are less than the average mass: planets[planets$mass &lt; mean(planets$mass),] ## name mass radius density type distance ## 1 Mercury 3.30e+23 2440000 5423.203 terrestrial 5.80e+07 ## 2 Venus 4.87e+24 6052000 5244.977 terrestrial 1.08e+08 ## 3 Earth 5.97e+24 6371000 5511.412 terrestrial 1.50e+08 ## 4 Mars 6.42e+23 3390000 3934.117 terrestrial 2.28e+08 ## 7 Uranus 8.68e+25 25360000 1270.525 ice_giant 2.90e+09 ## 8 Neptune 1.02e+26 24620000 1631.727 ice_giant 4.50e+09 Lets examine what has happened here. In place of an explicit list of row indices we provided a logical comparison between the vector of entries in planets$mass and the single numeric quantity mean(planets$mass). planets$mass &lt; mean(planets$mass) ## [1] TRUE TRUE TRUE TRUE FALSE FALSE TRUE TRUE We can see that this logical comparison has compared each entry in the vector of masses with the single quantity mean(planets$mass) and returned a vector with the corresponding logical evaluations of the comparisons. When passed as indices R has extracted only those rows where the logical comparison evaluated to TRUE. We can use the Boolean operators to create more complex indexing. planets[planets$mass&lt;mean(planets$mass) &amp; planets$density&gt;mean(planets$density),] ## name mass radius density type distance ## 1 Mercury 3.30e+23 2440000 5423.203 terrestrial 5.80e+07 ## 2 Venus 4.87e+24 6052000 5244.977 terrestrial 1.08e+08 ## 3 Earth 5.97e+24 6371000 5511.412 terrestrial 1.50e+08 ## 4 Mars 6.42e+23 3390000 3934.117 terrestrial 2.28e+08 The filter funcion in the dplyr package provides a perhaps more accessible syntax for performing this sort of filtering. filter(planets, mass &lt; mean(mass)) ## name mass radius density type distance ## 1 Mercury 3.30e+23 2440000 5423.203 terrestrial 5.80e+07 ## 2 Venus 4.87e+24 6052000 5244.977 terrestrial 1.08e+08 ## 3 Earth 5.97e+24 6371000 5511.412 terrestrial 1.50e+08 ## 4 Mars 6.42e+23 3390000 3934.117 terrestrial 2.28e+08 ## 5 Uranus 8.68e+25 25360000 1270.525 ice_giant 2.90e+09 ## 6 Neptune 1.02e+26 24620000 1631.727 ice_giant 4.50e+09 filter(planets, density &gt; mean(density)) ## name mass radius density type distance ## 1 Mercury 3.30e+23 2440000 5423.203 terrestrial 5.80e+07 ## 2 Venus 4.87e+24 6052000 5244.977 terrestrial 1.08e+08 ## 3 Earth 5.97e+24 6371000 5511.412 terrestrial 1.50e+08 ## 4 Mars 6.42e+23 3390000 3934.117 terrestrial 2.28e+08 filter(planets, mass &lt; mean(mass), density &gt; mean(density)) ## name mass radius density type distance ## 1 Mercury 3.30e+23 2440000 5423.203 terrestrial 5.80e+07 ## 2 Venus 4.87e+24 6052000 5244.977 terrestrial 1.08e+08 ## 3 Earth 5.97e+24 6371000 5511.412 terrestrial 1.50e+08 ## 4 Mars 6.42e+23 3390000 3934.117 terrestrial 2.28e+08 We saw earlier that R will use the value NA when data entries are missing. Trying to filter when there are NA values will typically produce errors or result in strange behaviour, since applying logical comparisons with NA will always produce NA, even if we try to check if NA is equal to itself! NA==NA ## [1] NA However R has a special function is.na() which can allow us to handle this. For example planets_all &lt;- bind_rows(planets, exoplanets) planets_all[planets_all$distance &gt; 10^9,] ## name mass radius density type distance ## 6 Saturn 5.68e+26 58230000 686.7827 gas_giant 1.4e+09 ## 7 Uranus 8.68e+25 25360000 1270.5254 ice_giant 2.9e+09 ## 8 Neptune 1.02e+26 24620000 1631.7267 ice_giant 4.5e+09 ## NA &lt;NA&gt; NA NA NA &lt;NA&gt; NA ## NA.1 &lt;NA&gt; NA NA NA &lt;NA&gt; NA ## NA.2 &lt;NA&gt; NA NA NA &lt;NA&gt; NA planets_all[planets_all$distance &gt; 10^9 | is.na(planets_all$distance),] ## name mass radius density type distance ## 6 Saturn 5.68e+26 58230000 686.7827 gas_giant 1.4e+09 ## 7 Uranus 8.68e+25 25360000 1270.5254 ice_giant 2.9e+09 ## 8 Neptune 1.02e+26 24620000 1631.7267 ice_giant 4.5e+09 ## 9 Kepler-186f 1.90e+23 1170000 28320.9232 exo NA ## 10 Kepler-62f 3.32e+23 1410000 28274.3179 exo NA ## 11 Kepler-438b 2.64e+23 1120000 44860.2046 exo NA 2.2 Importing and Exporting Data 2.2.1 Saving and Exporting The are multiple ways in which we can export or save our data, and other R objects. The functions write.table() and write.csv() will write the contents of a data frame (or matrix) to a file, where by default write.table() separates the columns by a Tab and write.csv() by a comma. In addition to the name of the data frame/matrix which is to be written, both functions require an argument file, the filename to which the contents should be written. R will not hesitate to overwrite the contents of a file with the same name, so always be careful to choose sensible and unique names for your files so as not to accidentally lose anything. For example, suppose we have created a folder in our H drive called My Awesome Data. We can then write the contents of our planets data frame to a new file in that folder using either of write.csv(planets, file = &quot;H:/My Awesome Data/planets.csv&quot;, row.names = FALSE) write.table(planets, file = &quot;H:/My Awesome Data/planets.txt&quot;, row.names = FALSE) Note that setting row.names = FALSE prevents R from adding an additional column with either the rownames (not the same as the field planets$name) of the data being written or, if there are no such rownames, then naming the rows from 1 to nrow(planets). The Working Directory If the argument file does not specify an entire path then R will by default write to the working directory. You can check the working directory with the command getwd(), and set it with setwd(directory). For example, we could navigate to the directory H:/My Awesome Data and then simply write the content of planets using. setwd(&quot;H:/My Awesome Data/&quot;) write.csv(planets, file = &quot;planets.csv&quot;, row.names = FALSE) Saving R objects of any type can be achieved with the function save(). Similar to the write.*() functions one needs to specify the filename where the objects being saved will be stored. However save() operates by saving an R environment, containing all the other arguments passed to save(). Although save() will allow you to use any filename, typically we use the file extension .RData. save(planets, exoplanets, file = &quot;planets.RData&quot;) 2.2.2 Importing data Just as we have the files write.csv() and write.table() so too are there related functions read.csv() and read.table(). When using either of these we need to allocate the contents of the file being read to a data frame in R. In addition we need to provide the full path if reading from outside the working directory, but if reading from the working directory we can simply use read.csv(\"~/&lt;filename&gt;\"). For example, if we have navigated (i.e. set the working directory) to the location where planets.csv is saved then we can read it using planets_read &lt;- read.csv(&quot;~/planets.csv&quot;) planets_read ## name mass radius density type ## 1 Mercury 3.30e+23 2440000 5423.2029 terrestrial ## 2 Venus 4.87e+24 6052000 5244.9771 terrestrial ## 3 Earth 5.97e+24 6371000 5511.4124 terrestrial ## 4 Mars 6.42e+23 3390000 3934.1175 terrestrial ## 5 Jupiter 1.90e+27 69910000 1327.5384 gas_giant ## 6 Saturn 5.68e+26 58230000 686.7827 gas_giant ## 7 Uranus 8.68e+25 25360000 1270.5254 ice_giant ## 8 Neptune 1.02e+26 24620000 1631.7267 ice_giant File Format In general, before you import a data set, try to inspect the file using a text editor to confirm the structure. Once you have identified the structure, and if it happens to be something standard, like a CSV, or Tab-delimited file, then go ahead and use those respective functions to read it in. But in the event of it being something a bit more special, like colon separated values, you can setup read.table(, sep = \":\") to import the data correctly. So what would happen if we use the wrong format? Lets read in the Tab separated planets.txt as though it is a .csv and inspect it planets_tab &lt;- read.csv(&quot;~/planets.txt&quot;) str(planets_tab) ## &#39;data.frame&#39;: 8 obs. of 1 variable: ## $ name.mass.radius.density.type: chr &quot;Mercury 3.3e+23 2440000 5423.20288031323 terrestrial&quot; &quot;Venus 4.87e+24 6052000 5244.97706969092 terrestrial&quot; &quot;Earth 5.97e+24 6371000 5511.41236928615 terrestrial&quot; &quot;Mars 6.42e+23 3390000 3934.11747589116 terrestrial&quot; ... The read.csv() function expects the columns/variables to be separated by commas, and so continues to read each line until it finds a comma. Since there are none, it thinks there is only a single item to read in each line and concatenates the contents of each line. planets_tab[1,1] ## [1] &quot;Mercury 3.3e+23 2440000 5423.20288031323 terrestrial&quot; Loading saved R objects can be achieved with the function load(). Unlike the read.*() functions we do not need to allocate the contents being loaded since the saved objects (with their names) will be imported into the global environment. Lets remove the planets data frame and then reload it. rm(planets) planets ## Error: object &#39;planets&#39; not found load(&quot;~/planets.RData&quot;) planets ## name mass radius density type ## 1 Mercury 3.30e+23 2440000 5423.2029 terrestrial ## 2 Venus 4.87e+24 6052000 5244.9771 terrestrial ## 3 Earth 5.97e+24 6371000 5511.4124 terrestrial ## 4 Mars 6.42e+23 3390000 3934.1175 terrestrial ## 5 Jupiter 1.90e+27 69910000 1327.5384 gas_giant ## 6 Saturn 5.68e+26 58230000 686.7827 gas_giant ## 7 Uranus 8.68e+25 25360000 1270.5254 ice_giant ## 8 Neptune 1.02e+26 24620000 1631.7267 ice_giant Be careful when using the load() function since if the specified file contains objects with the same names as those in the current environment these will be overwritten. 2.2.2.1 Loading Data from Packages For the most part we will be working with data which are contained in existing R libraries. Depending on the package, when we load its library using the function library() either R will also import all of the packages data sets to the global environment or it will allow you to access these data sets with the function data(). We can also use the function data() to inspect the entire list of data sets included in a package. Lets look at the ISLR2 package to start. If you havent already done so, install the package using install.packages(\"ISLR2\"). library(ISLR2) data(package = &quot;ISLR2&quot;) This should bring up a new tab called R data sets which includes the list of data sets included in the ISLR2 package. Since ISLR2 automatically loads its data sets to the environment (so-called lazy loading) along with the library, you should be able to access these. For example str(Auto) ## &#39;data.frame&#39;: 392 obs. of 9 variables: ## $ mpg : num 18 15 18 16 17 15 14 14 14 15 ... ## $ cylinders : int 8 8 8 8 8 8 8 8 8 8 ... ## $ displacement: num 307 350 318 304 302 429 454 440 455 390 ... ## $ horsepower : int 130 165 150 150 140 198 220 215 225 190 ... ## $ weight : int 3504 3693 3436 3433 3449 4341 4354 4312 4425 3850 ... ## $ acceleration: num 12 11.5 11 12 10.5 10 9 8.5 10 8.5 ... ## $ year : int 70 70 70 70 70 70 70 70 70 70 ... ## $ origin : Factor w/ 3 levels &quot;American&quot;,&quot;European&quot;,..: 1 1 1 1 1 1 1 1 1 1 ... ## $ name : Factor w/ 304 levels &quot;amc ambassador brougham&quot;,..: 49 36 231 14 161 141 54 223 241 2 ... ## - attr(*, &quot;na.action&quot;)= &#39;omit&#39; Named int [1:5] 33 127 331 337 355 ## ..- attr(*, &quot;names&quot;)= chr [1:5] &quot;33&quot; &quot;127&quot; &quot;331&quot; &quot;337&quot; ... 2.3 Data Plotting It might seem to you that scientists have an obsession with quantification. Your impressions would be correct. To build reliable, reproducible results, scientists naturally seek to gather and record counts, measurements, and attributes of the phenomenon under observation. Data, the collective name for such counts, measurements, and attributes, are the lifeblood of science. Studying data with quantitative tools allows scientists to detect patterns, make predictions, and assess the reliability of current theory. Being able to visualise data is crucial to this endeavour. Even a small data set is practically incomprehensible all by itself. Plotting data allows us to visualise the relationships we are interested in. Visualisation can be used for two things; (a) for the computer to show the scientist what is happening in the data, and (b) for you to show other people. Graphs that fall into (a) tend to be produced quicker and dirtier than the graphs we produce for (b). Finally, different types of graphs emphasise different aspects of the data and variables under study. Building a good graph therefore takes time and is often an iterative process. We saw in the previous chapter how the function plot() could be used to graphically display the relationships between two vector arguments. In this section we will briefly look at some useful was of visualising data, either single variables or the relationships between two variables. The airquality Data Set For illustrative purposes we will be working with one of the data sets which is loaded automatically when we start R. The airquality data set includes multiple air quality measurements taken in the Summer (May - September) of 1973 in New York. As always, if you want details on anything you can use the function help(airquality). Lets begin by inspecting the contents of that data set. str(airquality) ## &#39;data.frame&#39;: 153 obs. of 6 variables: ## $ Ozone : int 41 36 12 18 NA 28 23 19 8 NA ... ## $ Solar.R: int 190 118 149 313 NA NA 299 99 19 194 ... ## $ Wind : num 7.4 8 12.6 11.5 14.3 14.9 8.6 13.8 20.1 8.6 ... ## $ Temp : int 67 72 74 62 56 66 65 59 61 69 ... ## $ Month : int 5 5 5 5 5 5 5 5 5 5 ... ## $ Day : int 1 2 3 4 5 6 7 8 9 10 ... We can see that the data contain 6 variables including Ozone, Solar.R, Wind, Temp, Month and Day. Most of these are self explanatory, where checking the documentation tells us that Solar.R is solar radiation and Day is the day of the month. 2.3.1 Scatter Plots A scatter plot is a plot of the values of two variables against one another. When we used the function plot() in the previous chapter, even though we primarily used this to visualise the graphs of functions, what was actually happening is that R was plotting pairs of values of the vector of arguments to the function and the function evaluations against one another (either as points, type = \"p\" or as a line, type = \"l\"). Since in general we dont expect two measured (rather than derived) variables in a data set to fit precisely to some functional form using a scatter plot of two measured variables as points is useful for getting a sense of the general trend in the relationship between the variables, or perhaps to see whether or not there is a relationship at all. For example, we may expect that there will be a negative relationship between temperature and wind speed since (i) high wind speeds may have a direct cooling effect; and (ii) high pressure systems (low temperatures) produce high wind speeds plot(airquality$Wind, airquality$Temp, xlab = &quot;Wind Speed&quot;, ylab = &quot;Temperature&quot;) Indeed we can see that the trend is that high temperatures typically coincide with lower wind speeds, but that there is a lot of scatter of the points around this general relationship. Although in this case there is a physical law which suggests we may see such a relationship, scatter plots can also simply be used as exploratory tools to investigate whether there may be a relationship between variables or not. If we call plot() on an entire data frame R will produce a matrix of plots showing the scatter plots of all pairs of variables against one another. When the number of variables is only up to about six or seven this can be instructive, but if there is a large number of variables then this will not be very easy to digest. plot(airquality) The scatter plots of the continuous variables Ozone, Solar.R, Wind and Temp are quite instructive, and to a lesser extent their plots against Month as well. However, although we should expect some relationship between these measured variables and time, since the day of the month resets somewhat arbitrarily at the turn of each month plotting against Day may not have been very useful. We can plot a sub-data frame in the obvious way, by selecting some of its columns plot(airquality[,!(names(airquality)%in%c(&quot;Day&quot;, &quot;Month&quot;))]) 2.3.2 Box-plots Box-plots (or box and whisker plots) provide a very simply representation of the distribution of values of a single variable. They are based on what is known as the five number summary which includes the minimum and maximum values, and the three intermediate quartiles. The first quartile is the value which separates the bottom \\(25\\%\\) of values from the upper \\(75\\%\\), the second quartile (or median) separates the bottom and top halves, and the third quartile separates the top \\(25\\%\\) from the rest. A box-plot shows the distribution of values with a box illustrating the range between the first and third quartiles along with a line for the median, and whiskers reaching to the maximum and minimum values. - An exception to this is that if there are values above the value \\(Q_3 + 1.5(Q_3-Q_1)\\), where \\(Q_3-Q_1\\) is called the interquartile range (IQR) then the top whisker only extends to the largest value below this threshold and all points above it are shown individually. The same applies to the values below \\(Q_1 - 1.5(Q_3-Q_1)\\). For example the boxplot of Ozone shows that there are two values above the threshold \\(Q_3 + 1.5 IQR\\), but no such outliers in the lower values. boxplot(airquality$Ozone) Boxplots are extremely useful for getting a sense of the symmetry/asymmetry of the distribution. If the distribution is roughly symmetric we should see the median roughly in the middle of the box and the lengths of the whiskers being more or less the same length. The distribution of Ozone is right skewed since the length of the upper half of the box is longer than that of the lower half and the upper whisker is longer than the lower whisker (plus there are some fairly extreme values in the upper tail). We saw when using scatter plots that the discrete variables Month and Day were less instructive. We can investigate the relationships numeric and categorical variables (where we might reasonably think of Month as a category for this purpose) by comparing the distributions of the numeric variable within different levels/values of the category. We can achieve this using box-plots by calling boxplot(Ozone~Month, data = airquality) The argument Ozone~Month is a formula which tells R we want to know how does Ozone relate to Month?. We will use formulas a lot in this module when we start to fit predictive models. We can clearly see above that the distribution of Ozone varies a lot with the changing months. Also, since the months are ordered this gives us a sense of change over time, with Ozone peaking in July and Aurgust (Months 7 and 8). 2.3.3 Histograms Histograms provide a far more detailed representation of the distribution of a numeric variable. How they work is by breaking up the interval/range of values of the variable into equal width bins and then showing the proportions of the observations which fall into each of these bins. hist(airquality$Ozone, freq = FALSE) Setting freq = FALSE tells R we want to see the proportion of observations in each bin and not the total, however in some cases we may wish to see the total and can simply omit this argument since its default is to show the totals. It should be clear that the shape of the histogram will depend on the number of bins being used. Although the general shape will be similar for any appropriate number of bins, the more bins we include the more potential for the histogram for capturing detail in the distribution. However, using a large number of bins increases the variability in the proportions. By default hist() will use what is known as Sturges rule to compute an appropriate number of bins, but we can also vary this manually by providing breaks = number_of_bins par(mfrow = c(2, 2)) hist(airquality$Ozone, breaks = 4, freq = FALSE) hist(airquality$Ozone, breaks = 7, freq = FALSE) hist(airquality$Ozone, breaks = 12, freq = FALSE) hist(airquality$Ozone, breaks = 25, freq = FALSE) 2.4 Summary Data frames are the standard object for storing data in R. Each column in a data frame is considered to be a variable, such that the type of data it contains (strings vs numbers) can differ from the other columns. Accessing data can be done using both the dollar syntax and the square-bracket syntax: planets$name ## [1] &quot;Mercury&quot; &quot;Venus&quot; &quot;Earth&quot; &quot;Mars&quot; &quot;Jupiter&quot; &quot;Saturn&quot; &quot;Uranus&quot; &quot;Neptune&quot; planets$name[1:2] ## [1] &quot;Mercury&quot; &quot;Venus&quot; planets[, 1] ## [1] &quot;Mercury&quot; &quot;Venus&quot; &quot;Earth&quot; &quot;Mars&quot; &quot;Jupiter&quot; &quot;Saturn&quot; &quot;Uranus&quot; &quot;Neptune&quot; planets[1:2, 1] ## [1] &quot;Mercury&quot; &quot;Venus&quot; To export data use either the write.table() or write.csv() functions. Make sure to set the parameter row.names=FALSE. Check the help pages for each for more details. Both have these have sister functions; read.table() and read.csv(). With read.table() having lots of options that can be modified to read in any structured data file. R has a working directory, use getwd() to find out what it currently is. And use setwd() to change it. Equally, there is a menu in RStudio that allows you to do the same thing. Working directories are good, as they allow you to use relative paths, rather than absolute ones. # relative file names setwd(&quot;H:/Awesome R Data/&quot;) write.table(planets, file = &quot;planet.csv&quot;, row.names = FALSE) # absolute file names write.table(planets, file = &quot;H:/Awesome R Data/planet.csv&quot;, row.names = FALSE) It can be a good idea to set your working directory at the top of your script. Rs base plotting functions allow us to conduct cursory investigations of our data Scatterplots can show give a sense of relationships between pairs of numerical variables Box-plots and histograms can show the distributions of numeric variables. In addition inspecting box-plots of a variable across different levels of a categorical variable can give us a sense of the relationship between them. Although we didnt cover them here, bar-plots (with the function barplot()) can be used to illustrate the distribution of a categorical variable by showing the proportions of observations in each category. Also we can inspect the relationship between two categorical variables much as we did with histograms using barplot(variable1~variable2, data). We have only used Rs base plotting functionality, where more advanced visualisations can be achieved using ggplot and its derivatives. You can see the documentation of the more recent ggplot2 at http://docs.ggplot2.org and there have been many ggplot2 questions answered on StackOverflow. 2.5 Exercises Working on your planets data frame: Add the volumes for each planet as new variable using the dollar syntax. Now add variables with the following definitions: * `mass_earths` The mass of a planet as a proportion of the mass of Earth. * `volume_earths` Express `volume` as a proportion of Earth&#39;s volume. * `gravity_earths` Express `gravity` as a proportion of Earth&#39;s gravity. Create a new data frame which only contains the names of the planets, and each of the measurements expressed as a proportion of Earth. Inspect this data frame for information on the other planets relative to Earth. Make sure you are familiar with writing and reading from data files that are Tab-delimitted or CSV. For each file format: Look-up the help page, see the arguments needed to use the particular write.*() function. Store the planet data frame with the appropriate file extension. Can you open it in Excel? Can you read it back into R? Explore the Auto data set in the package ISLR2. Use the help() function to get additional information on its variables if needed. The plan is to explore this data set using graphs/plots. Separate out numerical variables between which you think there might be a relationship. Use scatter plots to identify any strong relationships. Investigate the presence of relationships between categorical variables. See if any of the interesting relationships you found differ by origin. For example, you can include different colours for each origin in a scatter plot of two variables using the argument col in a call to plot. The field $name is the vehicle model, which is almost unique for each entry. Create a new field $make which is a factor variable specifying the make of car. You are certainly not expected to do this manually. Start by writing a function which uses the base function substr() to look for the first space in a character object (use help(substr) to get you started. You may also wish to consult the function nchar()), and so return only the first word/acronym/initialism in each of the car models. Then apply this function to Auto$name to obtain $make. "],["background.html", "3 Statistical Background 3.1 Probability Basics 3.2 Random Variables 3.3 Samples and Statistical Modelling 3.4 Statistical Estimation 3.5 Multivariate Random Variables and Dependence 3.6 Summary 3.7 Exercises", " 3 Statistical Background \\[ \\def\\x{\\mathbf{x}} \\def\\Rr{\\mathbb{R}} \\newcommand{\\argmin}{\\mathop{\\rm argmin}} \\newcommand{\\argmax}{\\mathop{\\rm argmax}} \\def\\F{\\mathcal{F}} \\def\\hbbeta{\\hat{\\boldsymbol{\\beta}}} \\def\\bbeta{\\boldsymbol{\\beta}} \\def\\X{\\mathbf{X}} \\def\\y{\\mathbf{y}} \\def\\hg{\\hat g} \\] The purpose of this chapter is to introduce you to some basic and fundamental concepts in statistics and probability, as these will be important in understanding the main topics to come. We will cover the material only at a high level, so that we can familiarise ourselves with the notation and some of the fundamental ideas. Many of the topics we will be touching on go far deeper than we can in so short a space of time, and it will also be the case that from time to time things will be described in ways which are not quite precise at the deepest level but this is done only to convey the ideas at the level they are required for the content of this module. 3.1 Probability Basics We all have an innate understanding of what probability means. Indeed very little in life is certain, but we all need to make decisions about how to conduct our lives based on our perceived beliefs about how likely different possible truths are, or futures are to be. In studying probability, however, we need to break things down to their essence. To do so we think of a single chance event, which we refer to as a random experiment. A random experiment is simply an experiment (somewhat loosely defined) whose outcome is not predetermined; multiple possible outcomes could take place, and we cannot predict exactly which. Simple examples of random experiments include rolling a die, flipping a coin, choosing an individual at random from the class, etc. Importantly it is not necessarily the case that all outcomes are equally: When Steph Curry takes a free throw there are two outcomes, either he scores or he misses, and the probability he scores is somewhere around 0.9 We express probabilities as values between 0 and 1, with a probability 0 essentially being impossible and a probability of 1 essentially being certain Some people are more comfortable with speaking in percentages, and a probability of 0.9 may be thought of as 90% Sample Space We refer to the set of all possible outcomes of a random experiment as the sample space, and it is often denoted by \\(\\Omega\\) (the Greek O, pronounced omega). For example, in our coin flip experiment we have \\(\\Omega = \\{heads, tails\\}\\) or \\(\\{H, T\\}\\) for short. When rolling a (regular) die we have \\(\\Omega = \\{1, 2, ..., 6\\}\\). Event An event may simply be seen as a subset of the sample space, i.e., a collection of potential outcomes to the experiment. For example, if we are choosing someone randomly from the class we could have events selecting a female, selecting someone who is older than 30, etc. When rolling a die we could have as events rolling an even number, rolling a number below three, etc. A Simple Definition of Probability For any event \\(E\\), we could imagine conducting the random experiment \\(n = 1, 2, 3, ...\\) times, and counting in how many of these experiments the event occurs. The probability of \\(E\\), denoted \\(P(E)\\), may then be seen as \\[ P(E) = \\lim_{n \\to \\infty} \\frac{\\mbox{Number of times E occurred in first $n$ trials}}{n} \\] It should be intuitively the case that the probability of an event is closely linked to the proportion of times the event occurs if we conduct the experiment lots of times. But obviously we cant actually perform the experiment infinitely many times, and sometimes we dont need to conduct the experiment at all and can reasonably assume what the propobabilities of different events are, or come up with analytical expressions for these probabilities. Example: When flipping a coin we can usually assume that \\(P(H) = P(T) = 0.5\\) Example: When choosing an individual at random from the class, the probability of selecting someone older than 30 is simply the proportion of people in the class who are older than 30 (which could be zero, Im not sure). If the sample space is countable (either finite or countably infinite, like the natural numbers \\(\\{1, 2, 3, ...\\}\\)), and we know the probabilities of each of the individual outcomes, then we can easily determine the probability of an event \\(E\\) as \\[ P(E) = \\sum_{o \\in E} P(\\{o\\}) \\] In general if \\(E\\) is the union of mutually exclusive events \\(E_1\\) and \\(E_2\\), i.e. \\(E_1\\) and \\(E_2\\) cannot occur together, then \\[ P(E) = P(E_1 \\cup E_2) = P(E_1) + P(E_2) \\] The union notation \\(E_1 \\cup E_2\\) means \\(E_1\\) or \\(E_2\\), and the event \\(E_1 \\cup E_2\\) occurs if at least one of \\(E_1\\) and \\(E_2\\) occurs A more general form for the above is \\[ P(E) = P(E_1) + P(E_2) - P(E_1 \\cap E_2), \\] where \\(E_1 \\cap E_2\\) means \\(E_1\\) and \\(E_2\\) and the event \\(E_1 \\cap E_2\\) occurs if and only if both \\(E_1\\) and \\(E_2\\) occur An important consequence is that for any event \\(E\\) we have \\[ P(E) = 1-P(\\overline E), \\] where \\(\\overline E\\) is the complement of \\(E\\), and is every outcome not in \\(E\\). 3.1.1 Conditional Probability and Independence For two events \\(E_1\\) and \\(E_2\\) we may talk about \\(E_1\\) occurring given that \\(E_2\\) occurs, and we have the conditional probability of \\(E_1\\) given \\(E_2\\) defined as \\[ P(E_1|E_2) = \\frac{P(E_1 \\mbox{ and } E_2)}{P(E_2)} = \\frac{P(E_1 \\cap E_2)}{P(E_2)} \\] For the above to be defined, we have to assume that \\(P(E_2) &gt; 0\\), however we would not be (practically) interested in conditional probabilities where the event on which we are conditioning cannot happen. Independence Events \\(E_1\\) and \\(E_2\\) are said to be independent if \\[ P(E_1 \\cap E_2) = P(E_1)P(E_2). \\] This implies (as long as \\(P(E_2)&gt;0\\)) that \\(P(E_1|E_2) = P(E_1)\\), i.e. if we know that \\(E_2\\) happens it doesnt influence the probability that \\(E_1\\) also happens. 3.2 Random Variables The main reason for studying the basics of probability, within the context of statistical learning, is for its importance for understanding Random Variables (RVs). A random variable \\(X\\) is simply a real-valued function on the sample space of a random experiment. Every time we conduct the random experiment, it has an outcome \\(o\\), and the random variable assumes its corresponding value \\(X(o)\\). In fact the importance of random variables so supersedes the underlying random experiment that, for our purposes, we will typically not even mention the experiment, and simply think of a random variable as being a quantity whose value is determined randomly. Some simple examples of random variables (and their underlying random experiment) include For the experiment of flipping two coins, with sample space \\(\\{HH, HT , TH, TT\\}\\), we could define a number of different random variables \\(X = 1\\) if first flip is heads, and 0 otherwise \\(X = 1\\) if both flips are the same, and 0 otherwise \\(X\\) = total number of heads For the experiment of choosing an individual randomly from the class, we could define \\(X\\) = their height (in cm) \\(X\\) = their weight (in kg) \\(X\\) = 1 if they have brown eyes, and 0 otherwise etc. Random variables like the number of heads out of two coin flips are discrete, as they can only take separate distinct values (0, 1 and 2 in this example). Technically a random variable is discrete if it can only take on countably many values (again either a finite number, or something like the integers or whole numbers). Continuous random variables, on the other hand, can take any value over a range/interval, or a collection (union) of ranges/intervals. Examples include the weights/heights of people, or the time at which the first bus departs from the underpass on a given day. The first bus is scheduled for 06h20, so it will most likely depart around that time, perhaps in the interval 06h15 to 06h30. It may be reasonable to assume the most likely departure time is the scheduled time, and that this likelihood decreases as we move away either forwards or backwards in time. 3.2.1 Probability Distributions (Discrete) Consider a discrete random variable \\(X\\) which can take values in a set \\(S\\) (called the support of \\(X\\)). A realisation of the random variable is the actual value it assumes as a result of a particular instance of the random experiment. Typically we use lower case \\(x\\) to denote the (variable) value of the realisation of \\(X\\). Note that for a given \\(x\\), \\(X = x\\) is an event, containing all outcomes of the experiment which lead \\(X\\) to assume the value \\(x\\). As an event, we must be able to quantify its probability. The probability mass function (pmf) of \\(X\\) is the function \\[ p_X(x) = P(X=x); x \\in S \\]and satisfies \\(0 \\leq p_X(x) \\leq 1; x \\in S\\) (a probability of 1 is certain, and we cant have negative probabilities) \\(\\sum_{x \\in S} p_X(x) = 1\\) (the random variable has to take on some value, and cannot take on multiple different values at the same time). The cumulative distribution function (cdf) of a random variable \\(X\\), often denoted \\(F_X\\) is defined as \\[ F_X(x) = P(X \\leq x) = \\sum_{t \\in S, t \\leq x} p_X(t) \\] Example: Counting Heads We flip three unbiased coins, and let \\(X\\) be the total number of heads we observe. Write out the probability mass function of \\(X\\). The simplest (at least conceptually) approach is to enumerate the entire set of possible outcomes, giving a sample space: \\(\\{HHH, HHT, HTH, THH, HTT, THT, TTH, TTT\\}\\) Since each of these eight is equally likely (we have the same probability of seeing a heads/tails in each of the three flips) we can simply count how many of the eight outcomes aligns with the events \\(X = 0, X = 1, X = 2\\), and \\(X = 3\\) But what if \\(n\\), the number of coins flipped, had been much larger? Actually enumerating all possibilities would have been tedious (or practically impossible if \\(n\\) is very large). There is a much more efficient way, which uses combinatorics. Consider the example above, and the event \\(X = 1\\), which is equivalent to \\(\\{HTT, THT, TTH\\}\\) The only thing differentiating them is where we place the single \\(H\\) The number of outcomes associated with \\(X=1\\) is therefore just how many ways we could choose where to place the one \\(H\\), out of a potential three places More generally the number of outcomes in the event \\(X=x\\), when \\(X\\) is the number of heads in \\(n\\) flips, is simply the number of ways we can choose \\(x\\) locations for these heads, out of the total potential \\(n\\) Mathematically we denote this by \\({n \\choose x}\\) and it is equal to \\(\\frac{n!}{x!(n-x)!}\\), where the \\(n!\\) means \\(n\\)factorial and is equal to \\(n\\times (n-1)\\times (n-2)\\times ... \\times 2 \\times 1\\) We will come back to this idea when we introduce the Binomial distribution. Dont stress, you will not need to derive any formulations which rely on combinatorics yourselves. 3.2.1.1 Expected Value The mean (or expected value) of a (discrete) random variable is defined as \\[ E[X] = \\sum_{x \\in S} x \\ p_X(x). \\]Intuitively the expected value can be thought of as the average value wed see from infinitely many realisations of \\(X\\). We often denote the mean by \\(\\mu\\), or \\(\\mu_X\\) if we want to be explicit about which RVs mean is being referred to. The expected value is whats called a linear operator, meaning if \\(a\\) and \\(b\\) are real numbers (or more generally scalars) then \\[ E[a + bX] = a + bE[X] \\] Also, if we have two random variables, say \\(X\\) and \\(Y\\), then \\[ E[X + Y] = E[X] + E[Y]. \\] 3.2.1.2 Variance and Standard Deviation Sometimes it is useful to transform a random variable \\(X\\) into a new one, say by applying a function \\(g\\). We then have \\[ E[g(X)] = \\sum_{x \\in S} g(x) p_X(x). \\] An important example gives rise to the variance, defined as \\[ Var(X) = E[(X-\\mu_X)^2] = \\sum_{x \\in S} (x-\\mu_X)^2 p_X(x). \\] The variance captures how spread out realisations of \\(X\\) tend to be around their mean, and is often denoted by \\(\\sigma^2\\) (or sometimes \\(\\sigma^2_X\\)) The square root of the variance is referred to as the standard deviation, \\(\\sigma\\) (or \\(\\sigma_X\\)). 3.2.1.3 The Binomial Distribution When counting the number of successes from \\(n\\) independent trials, when each trial is a success with probability \\(p\\): We write \\(X \\sim Binom(n, p)\\), and \\[ p_X(x) = {n \\choose x} p^x(1-p)^{n-x}; \\ x = 0, 1, 2, ..., n \\] Note that when \\(p = 0.5\\) we have \\(p^x(1  p)^{nx} = 0.5^n\\), as in the unbiased coin example For \\(n = 1\\) we have the Bernoulli distribution; \\(X \\sim Bern(p)\\) We have \\(E[X] = np\\) and \\(Var(X) = np(1  p)\\). The binomial distribution is useful in modelling for classification (which we will see a lot of later on in the module) The Binomial Distribution in R The pmfs of random variables in R use the prefix d, so that if we want to evaluate \\(P(X = x)\\), when \\(X \\sim Binom(n, p)\\), we use dbinom(x, n, p). Similarly the cdf uses the prefix p, i.e. pbinom(x, n, p). Example: Daffodil bulbs Twenty daffodil bulbs are planted in a tub. The probability that a given bulb germinates is 0.32. Calculate the probability that exactly 7 bulbs germinate dbinom(7, 20, 0.32) ## [1] 0.1770433 the probability that at most 5 bulbs germinate ### Fill in the gaps to complete pbinom(, , ) ### How could you get the same answer by combining ### the dbinom() and sum() functions? 3.2.1.4 The Poisson Distribution The Poisson distribution is also often used to represent counts, however in this context we are not conducting a fixed number of trials but rather counting the number of occurrences of interest; like the number of cars passing a point in a road over a chosen time interval, or the number of molecules of a gas in a chosen region. We write \\(X\\sim Pois(\\lambda)\\), where \\(\\lambda\\) is the only parameter of the distrbiution, and \\[ p_X(x) = e^{-\\lambda}\\frac{\\lambda^x}{x!}; x = 0, 1, 2, ... \\] Both \\(E[X]\\) and \\(Var(X)\\) are equal to \\(\\lambda\\) In the above examples \\(\\lambda\\) would be equal to the average rate cars pass that point times the length of time its being observed, and the average density of the molecules times the volume of the region being studied. Sometimes the observations we make are not consistent with \\(E[X] = Var(X)\\), meaning that using a Poisson distribution to model our problem is inappropriate A popular alternative counting distribution is the negative binomial, however that is beyond the scope of this module The Poisson Distribution in R Example: Coffee Customers Customers arrive to a coffee shop at a constant rate of fifteen per hour between 08h30 and 11h00 so that the number of customers in any time interval may be treated as a Poisson random variable. Calculate the probability that exactly five customers arrive between 08h30 and 09h00: The \\(\\lambda\\) parameter is 15 × 0.5 = 7.5, and again we use the prefix d for the pmf, i.e. the function dpois(x, lambda) dpois(5, 7.5) ## [1] 0.1093746 the probability that at least forty customers arrive between 08h30 and 11h00: Note that \\(P(X \\geq x) = 1-P(X &lt; x)\\) (recall the rules for probabability) ### Fill in to complete 1-ppois(, ) 3.2.2 Probability Distributions (Continuous) Recall that a continuous random variable \\(X\\) is one which can take any value in an interval, or union of intervals. Although \\(X\\) can take any value in its support, when it comes to assigning a probability \\(P(X = x)\\), for some specific value of \\(x\\), we have to conclude that it is zero, since the = means to infinite precision We therefore describe how we expect realisations of \\(X\\) to arise through what is called a probability density function (pdf), \\(f_X\\), which satisfies \\[ P(X \\leq t) = \\int_{-\\infty}^t f_X(x)dx \\] The pdf, similar to the pmf, satisfies \\(f_X(x) \\geq 0\\) (otherwise we could have negative probabilities) \\(\\int_{-\\infty}^\\infty f_X(x)dx = 1\\) (the random variable must take on some value) Generally speaking we take sums involving the pmf of a discrete random variable, and integrals involving the pdf of a continuous one. For example, if \\(X\\) is a continuous random variable we have \\[ \\mu_X = E[X] = \\int_{-\\infty}^\\infty x \\ f_X(x)dx \\] \\[ \\sigma^2_X = Var(X) = \\int_{-\\infty}^\\infty (x-\\mu_X)^2 \\ f_X(x)dx \\] but the intuitive interpretations of these moments are the same as before. 3.2.2.1 The Normal Distribution By far the most important continuous distribution is the Normal (or Gaussian) distribution. It is the well-known bell-shaped distribution, and is parameterised by its mean \\(\\mu\\) and variance \\(\\sigma^2\\). We write \\(X\\sim N(\\mu, \\sigma^2)\\) and \\[ f_X(x) = \\frac{1}{\\sqrt{2\\pi}\\sigma}\\exp\\left(-\\frac{1}{2\\sigma^2}(x-\\mu)^2\\right); \\ -\\infty &lt; x &lt; \\infty \\] The cdf does not have an explicit form The density is symmetric, with its maximum at \\(\\mu\\), and although it is strictly positive for all \\(x\\) it decreases very quickly to zero as we move away from \\(\\mu\\) in either direction It has short tails If we add normal random variables together, we get another normal random variable Remarkably even if the variables being added arent themselves normal, the sum typically looks more normally distributed than the individual variables themselves (well revisit this a bit later) The Standard Normal Distribution If \\(X \\sim N(\\mu, \\sigma^2)\\) then \\(Z = \\frac{X-\\mu}{\\sigma}\\) has whats called a standard normal distribution Sometimes informally called a \\(Z\\) distribution Arises in \\(Z\\) tests, but we will not go into hypothesis testing in any great depth in this module The density of a standard normal random variable is often denoted \\(\\phi\\), and its cdf \\(\\Phi\\) (the Greek f and F) The above figure shows the pdf of the standard normal distribution, while the following figures show the probabilities that \\(Z\\) lies within the intervals \\((-1, 1)\\), \\((-2, 2)\\) and \\((-3, 3)\\) Note that the cdf \\(\\Phi\\) does not have a nice form, but numerical integration techniques have been used to obtain extremely close approximations. Moreover, these approximations only need to be known for the standard normal, since for \\(X \\sim N(\\mu, \\sigma^2)\\) we have \\[\\begin{align} P(X \\leq x) &amp;= P((X-\\mu)/\\sigma \\leq (x-\\mu)/\\sigma)\\\\ &amp;= P(Z \\leq (x-\\mu)/\\sigma)\\\\ &amp;= \\Phi((x-\\mu)/\\sigma). \\end{align}\\] This also means that in general when \\(X\\) is normally distributed we have \\[\\begin{align} P(\\mu - \\sigma \\leq X \\leq \\mu + \\sigma) &amp;= 0.6827\\\\ P(\\mu - 2\\sigma \\leq X \\leq \\mu + 2\\sigma) &amp;= 0.9545\\\\ P(\\mu - 3\\sigma \\leq X \\leq \\mu + 3\\sigma) &amp;= 0.9973. \\end{align}\\] The Normal Distribution in R Example: Midday Temperatures The temperature at midday in Lancaster has mean \\(14^\\circ\\)C and standard deviation \\(10^\\circ\\) C. Assuming that daily temperatures follow a normal distribution, what is the probability that the temperature on a randomly chosen day is lower than \\(0^\\circ\\)C? This can be obtained directly from the cdf, pnorm(0, mean = 14, sd = 10) ## [1] 0.08075666 higher than \\(20^\\circ\\)C? For this we are looking for the complement of \\(X \\leq 20\\) 1-pnorm(20, 14, 10) ## [1] 0.2742531 Note that when dealing with discrete random variables which take on only integer values we have for integer \\(x\\) that \\(P(X \\leq x) = 1-P(X&gt;x) = 1 - P(X \\geq x+1)\\) since the event \\(X &gt; x\\) includes only values for \\(X\\) which are at least \\(x+1\\) On the other hand for continuous random variables the event \\(X &gt; x\\) includes all values in the interval \\((x, \\infty)\\). Also, since \\(P(X=x) = 0\\) and hence \\(P(X \\geq x) = P(X &gt; x)\\), we therefore have \\(P(X\\leq x) = 1- P(X&gt;x) = 1-P(X\\geq x)\\). 3.3 Samples and Statistical Modelling So where does this study of random variables get us? In statistics we refer to our sample of data or sample of observations, and generally this is drawn from some larger population This could be a physical population of objects (often people or animals), but could also only exist in principle: Example: Suppose we repeated a chemistry experiment multiple times to understand properties of the reagents. The outcomes of the experiments I didnt conduct, but could have if I had continued or if I had conducted my experiments at different times, dont actually exist, but they represent other potential members of the population of outcomes I could have seen and included in my sample. By treating the elements of our sample as realisations of random variables, we can use the theory of probability in order to make appropriate conclusions, which account for the inherent randomness in the sampling process. 3.3.1 Models and Assumptions We often talk about modelling assumptions, which are assumptions about the population from which our sample came, and also about the sampling process (i.e. the way in which elements were taken from the population and added to our sample) Examples include: A form for the population distribution, e.g. Poisson, Normal, etc. Independence, i.e., that knowing the value of one (or some) of the values in the sample should not give you any additional information about the other values Later on we will talk about assumptions on the relationships between multiple variables (not the same as relationships/dependence between different elements in the sample) But what exactly is a model? Just as in the case of a model airplane, or car, a model is a simplified representation of something. In the context of statistics we may not be able to solve problems directly for the true population/process/system we are studying, but if a model of this phenomenon is a reasonable enough reflection of reality and we are able to solve the problem for the model version then it is useful without necessarily being precise or optimal This idea is captured well by the famous quote: All models are wrong, but some are useful ~ George Box With reference to our modelling assumptions, if these assumptions are all reasonable AND they are sufficient to allow us to solve our problem then our model may be useful If the assumptions are not sufficient, we cannot solve the problem anyway If our assumptions are not reasonable, then even if we can get a solution, it is questionable whether we can trust it Going forward we will typically refer to our sample of observations, \\(x_1, x_2, , x_n\\), as assumed to have been drawn independently from a common probability (population) distribution, the latter condition essentially meaning identically distributed. What this assumption means is that our sample may be seen as a realisation of a random sample, \\(X_1, X_2, , X_n\\), of independent and identically distributed (i.i.d.) random variables. It is by understanding the statistical properties of such random samples that we can appropriately use our observed sample to make decisions and infer properties of the overall population. 3.4 Statistical Estimation Lets begin our study of statistical estimation with an example scenario: Coffee Customers (again): Consider again the coffee shop example we described in relation to the Poisson distribution. Imagine that we now start to see that actually the probabilities we calculated did not seem to match very well with our observations, and that actually there were often considerably more customers than we anticipated and staff are barely coping with the demand. If we took note of the numbers of customers arriving during the busy morning session, over multiple days, we would have a sample of realisations from the population of potential numbers we might see in the near future Although the demand for coffee at our shop may change again going forward, meaning that the observations we make now may not be from the same distribution as those in a few years, it might be reasonable to assume that over the period of a year the demand will not change very dramatically. Using our sample we could obtain an estimate of the actual arrival rate, and then use this to answer questions about what we might expect on future days, like how busy the shop is likely to be at its busiest, or the number of days on which the shop makes a loss. It is important to note, however, that even if our assumptions are reasonable that does not necessarily make our estimate suddenly equal to the true value. Had we chosen a different set of days on which to record the arrivals, we would have arrived at a different value for our estimate. Both of these would be valid and useful, despite being different. This is a fundamental consideration in statistical estimation; that the statistics we compute from our sample (like the average arrival rate from our coffee shop over a sample of days) are themselves realisations of random variables. 3.4.1 Estimates and Estimators This notion, that our entire sample, and therefore any statistics taken from it, could have been different, leads us to question to what extent we can trust the conclusions/inferences we make from them. It is sensible to use our observed arrival rate as a proxy for the actual arrival rate, but we are still uncertain about what the true value is, and how far we expect this estimate might be from the actual rate may influence any decisions we make as a result. As statisticians we may therefore like to ask questions like if I had observed infinitely many samples, what proportion of them would have led to an estimate which is within a certain (chosen) distance from the true value being estimated? To answer this question we are ultimately interested in the probability distribution of an estimator. Any way of combining the elements of a random sample to produce an estimate for something about the underlying population, is itself a random variable, and called an estimator. For example, The sample mean and variance \\[\\begin{align} \\bar X &amp;= \\frac{1}{n}\\sum_{i=1}^n X_i,\\\\ S_X^2 &amp;= \\frac{1}{n-1}\\sum_{i=1}^n \\left(X_i - \\bar X\\right)^2 \\end{align}\\] are estimators for the population mean and variance, \\(\\mu_X\\) and \\(\\sigma^2_X\\) Both \\(\\bar X\\) and \\(S_X^2\\) are indeed random variables, and so have probability distributions in their own right, called sampling distributions. Our particular sample \\(x_1, ..., x_n\\) (as opposed to the random sample \\(X_1, ..., X_n\\) of which our sample is a realisation) gives rise to our corresponding estimates or sample statistics \\[\\begin{align} \\bar x &amp;= \\frac{1}{n} \\sum_{i=1}^n x_i\\\\ s_X^2 &amp;= \\frac{1}{n-1}\\sum_{i=1}^n \\left(x_i - \\bar x\\right)^2 \\end{align}\\] and these can be seen simply as realisations of the random variables \\(\\bar X\\) and \\(S_X^2\\) If youre confused about the denominator \\(n-1\\) for \\(S_X^2\\) and \\(s_X^2\\) it is primarily in place so that \\(S_X^2\\) is an unbiased estimator for \\(\\sigma_X^2\\). We will discuss the topic of bias shortly. To summarise: \\(\\bar X\\) and \\(S_X^2\\) are estimATORS; rules for combining the elements of a (random) sample in order to obtain an estimate for a characteristic of the underlying population distribution. The underlying population meaning the population which gave rise to each element in our sample Applying the rule/procedure associated with an estimator to an observed sample gives us our estimATE, and the thing we are trying to estimate is called the estimAND. \\(\\bar X\\) is an estimator for the population mean \\(\\mu_X\\) (our estimand) and \\(\\bar x\\) is our estimate for it \\(S_X^2\\) is an estimator for the population variance \\(\\sigma_X^2\\) and \\(s_X^2\\) is our estimate for it 3.4.1.1 Properties of Estimators The above examples were estimators specifically for the mean and variance of the underlying population. More generally, we use the notation \\(\\theta\\) to denote an arbitrary quantity related to the underlying population. It is common to use the hat notation, \\(\\hat \\theta\\), theta hat, to denote an estimator for \\(\\theta\\). If it is not ambiguous, we may also use \\(\\hat \\theta\\) to be a particular estimate from an observed sample, and when it is needed we can differentiate the two by using \\(\\hat \\theta_{obs}\\) to be the observed estimate. For example \\(\\bar X = \\hat \\mu\\) and \\(\\bar x = \\hat \\mu_{obs}\\), etc. If we can understand the probability distribution of an estimator, \\(\\hat \\theta\\), we can quantify our uncertainty about the actual \\(\\theta\\) when using \\(\\hat \\theta_{obs}\\) as a proxy for it. Standard Error A direct way of quantifying this uncertainty is through the variance or standard deviation of the estimator If an estimator has high variance, then realisations of it tend to be quite spread out Believing that any single realisation is close to the true value is therefore prone to risk It is very important to note, however, that having high variance is not a limitation of an estimator, per se, but knowing that it has high variance allows us to appropriately incorporate this information in any decision making. The standard error of an estimate is simply the standard deviation of the estimator of which it is a realisation Bias The variance of a random variable quantifies how spread out it tends to be about its mean. In the context of estimation, therefore, knowing only the variance of an estimator is not always useful on its own In an absurd example we could design an estimator which always takes the value zero, regardless of the sample values. Such an estimator has zero variance, but it is clearly not of much use since it doesnt actually use any information from the sample The bias of an estimator is a measure of how far off the mean of the estimator is from the estimand. Specifically \\[ Bias(\\hat \\theta) = E[\\hat \\theta] - \\theta. \\] Bias is therefore signed, i.e. Positive bias means the estimator tends to overestimate the target Negative bias means the estimator tends to underestimate the target 3.4.1.2 Confidence Intervals Although the variance and expected value of an estimator can give us a good sense of how accurate our estimate is likely to be, while also giving a sense of uncertainty, confidence intervals communicate this information extremely effectively. At a high level a confidence interval may be seen as a set of plausible values for the true parameter. Whereas a point estimate, say \\(\\hat \\theta_{obs}\\) may be thought of as in a sense the most plausible, we never really believe that \\(\\hat \\theta_{obs} = \\theta\\), only that it is likely that \\(\\hat \\theta_{obs} \\approx \\theta\\). But how rough or precise this approximation is may have far reaching consequences, and though the standard error of \\(\\hat \\theta_{obs}\\) gives a sense of this roughness a confidence interval is far more explicit. Imagine we could access values \\(l\\) and \\(u\\) having the property that \\[ P(l &lt; \\theta - \\hat \\theta &lt; u) = 0.95. \\] Since \\(\\hat \\theta_{obs}\\) is just a realisation of the random variable \\(\\hat \\theta\\), this would mean that of all potential samples I could see, \\(95\\%\\) of them would lead to an estimate \\(\\hat \\theta_{obs}\\) which satisfies \\(\\hat \\theta_{obs} + l &lt; \\theta &lt; \\hat \\theta_{obs} + u\\). If I know that of all potential samples I could see, \\(95\\%\\) of them would give me an interval of the form \\((\\hat \\theta_{obs} + l, \\hat \\theta_{obs} + u)\\) which contains the true parameter \\(\\theta\\), then surely I can be pretty confident that the sample I actually have is one of these nice ones. After all, only \\(5\\%\\) of all possible samples would let me down in this regard. Think of the analogy of pulling a ball from a bag. If I know that \\(95\\%\\) of the balls are blue, and I take one ball out of the bag with my eyes closed, even before I look at the ball I can be very confident that I have a blue ball. Why are intervals like this useful, you may ask? To answer, lets consider the basic reproduction number of a disease (you probably all remember the famous \\(R_0\\) from the COVID pandemic). Suppose a group of statisticians estimates that \\(R_0\\) is \\(0.93\\). If you remember the pandemic well, youll remember that the critical value is \\(R_0 = 1\\), with a smaller value meaning the infection is dying out and a greater value meaning it will continue to spread. So, great, right? Were told that someone thinks \\(R_0\\) is \\(0.93 &lt; 1\\) and everyone is happy and society can begin to re-start. But wait, \\(R_0\\) is a tricky thing to estimate and there is a lot of uncertainty. Actually the statisticians communicate in addition that their \\(95\\%\\) confidence interval for \\(R_0\\) is \\((0.82, 1.09)\\). Because of the potentially disastrous outcomes if actually \\(R_0\\) is above one, it may be prudent to behave as though \\(R_0\\) is actually \\(1.09\\) since the statisticians think this is also a plausible value. Statistical Errors Lets quickly take an aside. It may be that some of you are uncomfortable about the fact that there is still a \\(5\\%\\) of being wrong. Unfortunately we can never completely eliminate uncertainty without losing practicality. We could always say we are \\(100\\%\\) confident that \\(R_0\\) is greater than zero. But a statement like this is completely useless since \\(R_0\\) is greater than zero by its definition, not because of some fancy statistical procedure which produced a \\(100\\%\\) confidence interval. Because we are dealing in the realms of randomness, it is always possible to get unlucky and be led to a conclusion which is misleading. This doesnt mean that a mistake was made. The best statisticians in the world, asked to produce a whole lot of \\(95\\%\\) confidence intervals, will get unlucky \\(5\\%\\) of the time. Why \\(95\\%\\)? The choice of going with \\(95\\%\\) confidence intervals in the above description was completely arbitrary. In fact we can choose to have any level of confidence between \\(0\\) and \\(100\\%\\). However, there is a cost. If we want to have a high degree of confidence then we cannot be very precise. What this means is that we could increase our level of confidence, say to \\(99\\%\\), but this would mean that the confidence interval would have to be wider than the \\(95\\%\\) confidence interval. There is no correct answer for the level of confidence and it will often depend on the context. In the high risk pandemic context we may want to have a high degree of confidence, even if it means we are led to be overly risk averse. 3.4.1.3 The Central Limit Theorem and the Bootstrap Now to the hard part. Constructing confidence intervals is contingent on being able to access the values \\(l\\) and \\(u\\) which allowed us to actually obtain our interval \\((\\hat\\theta_{obs} + l, \\hat \\theta_{obs} + u)\\). It should be clear that knowing the sampling distribution of an estimator can be crucial for any decision making on the basis of an observed estimate, and would allow us to at obtain the values \\(l\\) and \\(u\\). However, it is rare that an exact sampling distribution is known and frequently we must rely on (at best) approximations. One of the most fundamental results in statistics is called the central limit theorem (CLT), and in its simplest form states that the sampling distribution of the sample mean, \\(\\bar X\\), is approximately normal, and formally we have that \\[ \\sqrt{n}\\frac{\\bar X - \\mu_X}{\\sigma_X} \\] has a distribution well approximated by the standard normal distribution provided only The sample is reasonably large. The actual theory is asymptotitic, meaning that the distribution tends to a normal distribution as the sample size tends towards infinity. A working rule of thumb people use is \\(n \\geq 30\\) is sufficient provided the underlying distribution is at least very roughly symmetric The population variance, \\(\\sigma_X^2\\), is finite. This is absolutely vital for the central limit theorem to apply, but for the vast majority of applications is not something we really need to concern ourselves with. More important in most cases is whether an appropriate estimate for \\(\\sigma_X^2\\) is available When the population itself is normally distributed, we always have that \\(\\sqrt{n}(\\bar X - \\mu_X)/S_X\\) has what is known as a t-distribution with \\(n-1\\) degrees of freedom. The t-distributions are similar to the normal distribution, in that they are symmetric, but the density tends to zero much more slowly as one moves away from the mean in either direction. As the number of degrees of freedom increases, however, the t-distribution gets closer and closer to a standard normal If the sample is particularly large (some suggest \\(n \\geq 100\\) as a rule of thumb) then regardless of the underlying population we can assume that \\(\\sqrt{n}(\\bar X - \\mu_X)/S_X\\) has an approximate (standard) normal distribution. There are many extensions and generalisations of the CLT, with one particularly important one being that (under some conditions beyond the scope of this module) estimators based on the principle of maximum likelihood, which we will se shortly, are approximately normally distributed provided the sample is reasonably large. But what about when we cannot use this theory? The bootstrap is a remarkably simple but beautiful idea. Suppose we have an estimator \\(\\hat \\theta\\) whose sampling distribution we would like to understand. If we were able to obtain a large number of samples from the underlying population, then we could compute all of the resulting estimates, and this would give us a sample of realisations of \\(\\hat \\theta\\). We could then use this sample to estimate features of the sampling distribution of \\(\\hat \\theta\\) We of course cannot just obtain more samples from the underlying population; we only get one. The bootstrap simply says lets pretend like the distribution of values in our single sample is a good representation of the population distribution. If this is the case then we can re-sample from these values to obtain pseudo-samples, or bootstrap samples. Bootstrap theory says that the distribution of the difference between the estimates we obtain on these bootstrap samples and the estimate we obtain on our actual sample approximates the distribution of \\(\\hat \\theta - \\theta\\). Practically then, the booststrap works as follows. Suppose our sample is \\(x_1, ..., x_n\\), and we choose some large \\(B\\) (the number of bootstrap samples we plan to use). We then do, for each \\(b = 1, 2, ..., B\\): Re-sample from \\(x_1, ..., x_n\\) with replacement to obtain \\(x_{1,b}^*, ..., x_{n, b}^*\\) Compute \\(\\hat \\theta^*_b\\), the estimate arising from the bootstrap sample \\(x_{1,b}^*, ..., x_{n, b}^*\\) Then take the collection of values \\(\\hat \\theta^*_1-\\hat \\theta_{obs}, ..., \\hat \\theta^*_B-\\hat \\theta_{obs}\\) to approximate the distribution of \\(\\hat \\theta - \\theta\\). Recall that \\(\\hat \\theta_{obs}\\) is the particular estimate we obtain from our original sample An approximate confidence interval for \\(\\theta\\) can then simply be obtained by taking the quantiles of the distribution of \\(2\\hat\\theta_{obs} - \\hat \\theta^*_b; b = 1, ..., B\\). Examples in R Lets begin by seeing the central limit theorem in action. We will simulate multiple samples from a standard exponential distribution (with density function \\(\\exp(-x)\\)) and investigate the distribution of the sample means, after appropriate standardisation. Just as the prefices d and p were used to evaluate the pmf/pdf and cdf respectively, the prefix r is used to denote functions used for simulating/generating random realisations from a distribution. For example rexp(n) will generate a sample of size \\(n\\) from a standard exponential distribution. Now, the mean and variance of the standard exponential are both one, and so we should see the distribution of all the \\(\\sqrt{n}(\\bar x - 1)\\) values looking close to a standard normal, provided the sample size is large enough. ### We will use three sample sizes, 10 (too small for the CLT), 30 (the apparent border of what is ### large enough although the exponential distribution is quite heavily skewed) and 100 ### (should be large enough even for a very skew distribution). n1 &lt;- 10 n2 &lt;- 30 n3 &lt;- 100 ### To compute the sample means efficiently we can store the different samples in the rows of a matrix ### and then use the function rowMeans. Let&#39;s generate nsamp = 10000 samples of each of the different sample ### sizes and calculate the means nsamp &lt;- 10000 xbar_n1 &lt;- rowMeans(matrix(rexp(nsamp*n1), nsamp, n1)) xbar_n2 &lt;- rowMeans(matrix(rexp(nsamp*n2), nsamp, n2)) xbar_n3 &lt;- rowMeans(matrix(rexp(nsamp*n3), nsamp, n3)) ### We can use a histogram to estimate the densities of the sample means (after standardisation). ### Histograms chop up the interval of the sample into equal width &quot;bins&quot; and then present the ### density as proportional to the number of points falling in each bin ### We can also overlay the standard normal density to see how good the approximation is par(mfrow = c(1, 3)) hist(sqrt(n1)*(xbar_n1-1), main = &quot;Sample size = 10&quot;, freq = FALSE) lines(seq(-5, 5, length = 1000), dnorm(seq(-5, 5, length = 1000)), col = 2) hist(sqrt(n2)*(xbar_n2-1), main = &quot;Sample size = 30&quot;, freq = FALSE) lines(seq(-5, 5, length = 1000), dnorm(seq(-5, 5, length = 1000)), col = 2) hist(sqrt(n3)*(xbar_n3-1), main = &quot;Sample size = 100&quot;, freq = FALSE) lines(seq(-5, 5, length = 1000), dnorm(seq(-5, 5, length = 1000)), col = 2) The histogram for \\(n = 10\\) shows very clear skewness, and the approximation of the normal density is not good. For \\(n=30\\) the approximation is better, but the skewness is still evident especially in the tails. When \\(n = 100\\) the approximation becomes much better. The skewness of a random variable \\(X\\) is equal to \\(E[(X - \\mu_X)^3]/\\sigma_X^3\\) and a large positive value indicates the tail of the distribution on the right is longer than that on the left, and a large negative value indicates the reverse. If \\(X\\) has what is known as a Gamma distribution, with shape parameter \\(\\alpha\\) and scale parameter \\(\\sigma\\) then its skewness is \\(2/\\sqrt{\\alpha}\\). The distribution of sample skewness is close to normal for large \\(n\\), but nonetheless we can use the bootstrap in order to obtain an estimate for a \\(95\\%\\) confidence interval. Below we simulate a sample of size \\(n=50\\) from a Gamma\\((3, 1)\\) distribution and then estimate a confidence interval from \\(B = 1000\\) bootstrap samples. ### Start with settings of the sample size, and shape and scale parameters n &lt;- 50 shape &lt;- 3 scale &lt;- 1 ### Generate a sample from the Gamma(shape, scale) distribution x &lt;- rgamma(n, shape = shape, scale = scale) ### Define a function to calculate the sample skewness skew &lt;- function(z) mean((z-mean(z))^3)/mean((z-mean(z))^2)^1.5 ### Now we can conduct the bootstrap procedure ### Set up a vector in which to store the skewness values from the ### bootstrap samples B &lt;- 1000 skews_B &lt;- numeric(B) ### Now loop over b in 1:B for(b in 1:B){ # Resample from x with replacement x_b &lt;- sample(x, replace = TRUE) # Calculate the sample skewness for the bootstrap sample skews_B[b] &lt;- skew(x_b) } ### Compute the 95% confidence interval using the quantile function ### on the distribution of 2*skew(x) - skews_B quantile(2*skew(x)-skews_B, c(0.025, 0.975)) ## 2.5% 97.5% ## 0.2422004 1.1211485 ### We can also visualise the estimated distribution whose quantiles define ### confidence interval hist(2*skew(x)-skews_B, main = &quot;Bootstrap Distribution&quot;, freq = FALSE, xlab = expression(2~hat(theta)[obs]~-~hat(theta)^{&quot;*&quot;})) ### We can also add vertical lines to show the estimated value and the ### boundaries of the confidence interval abline(v = skew(x), col = 2) # estimated value abline(v = quantile(2*skew(x)-skews_B, c(0.025, 0.975)), col = 2, lty = 2) # confidence interval ### We can also add the true skewness abline(v = 2/sqrt(shape), lty = 2) 3.4.2 Maximum Likelihood Estimation In the previous section we spoke a lot about estimators in general, and a few specific examples being the sample mean and sample variance, where we simply described their expressions explicitly. However it is not always obvious how to actually go about finding these expressions which allow us to estimate properties of the distribution, and sometimes there may not be explicit expressions like these at all. The theory associated with maximum likelihood estimation and maximum likelihood estimators is extremely deep, but for the purpose of this course we will only introduce maximum likelihood estimation as a concept as it becomes relevant to some of the predictive modelling we cover later on. Suppose as before that our sample is denoted \\(x_1, ..., x_n\\), and suppose that we are modelling the population distribution with a particular form (e.g. binomial, Poisson, etc.) which has parameter \\(\\theta\\), and for simplicity to start lets assume the distribution is discrete. To begin we may not, as alluded to above, know how to go about obtaining an estimate for \\(\\theta\\) from our sample. The principle of maximum likelihood simply says that the most appropriate estimate is that which, if it were the true value of \\(\\theta\\), would maximise the probability of seeing our particular sample. In other words one should choose the estimate which is most consistent with the sample observations. If we write \\(P(X=x|\\theta) = p_X(x|\\theta)\\) to be the probability mass function for a particular setting of \\(\\theta\\) then the probability of observing our sample is \\[ P(X_1 = x_1, ..., X_n = x_n|\\theta) = \\prod_{i=1}^n p_X(x_i|\\theta), \\] where \\(\\prod\\) is the notation for the product of all the terms, i.e. of multiplying them all together. The reason we can turn the probability \\(P(X_1 = x_1, ..., X_n = x_n|\\theta)\\) into this form is (i) we assume the variables \\(X_1, ..., X_n\\) are independent, and so we can turn \\(P(X_1 = x_1, ..., X_n = x_n|\\theta)\\) into \\(P(X_1=x_1|\\theta)P(X_2=x_2|\\theta)...P(X_n=x_n|\\theta)\\) and (ii) we assume the observations came from the same population, i.e. \\(P(X_i=x_i|\\theta) = p_X(x_i|\\theta)\\) for each \\(i\\). We refer to this term above as the likelihood of the data, and is a function of \\(\\theta\\). Now, it is typically hard to maximise products directly, but a useful trick allows us to circumvent this. Specifically for two real numbers \\(z_1, z_2\\) we have \\(z_1 &gt; z_2 \\iff \\log(z_1) &gt; \\log(z_2)\\) and as a result finding the maximum value of \\(\\prod_{i=1}^n p_X(x_i|\\theta)\\) is the same as finding the maximum value of \\(\\log\\left(\\prod_{i=1}^n p_X(x_i|\\theta)\\right) = \\sum_{i=1}^n \\log(p_X(x_i|\\theta))\\), and maximising sums is far more straightforward. This trick is essentially universal and so you will often see reference not to the likelihood but to the log-likelihood. The convenience of looking first at discrete distributions is ultimately because it is sensible to talk about the probability of our sample, whereas we know that the probability of any specific value of a continuous random variable is zero. However, we can refer to the likelihood as the probability of seeing a sample like our sample, i.e., one with observations very close to our own. This allows us to simply use the density function in place of the mass function, and so our likelihood becomes \\(\\prod_{i=1}^n f_X(x_i|\\theta)\\). For the purpose of this course we essentially just need the concept of maximum likelihood, and will not be deriving any solutions to maximum likelihood problems. However you will encounter optimisation as a topic in your Foundations of Data Science and AI module (and those on the MSc stats will be very familiar with the concept already). 3.5 Multivariate Random Variables and Dependence Although much of what we have already seen translates directly to the multivariate context, it has certainly been from the perspective that realisations of random variables are numbers, and not vectors (or even more elaborately structured objects). As we now get closer to studying our main topic for this module, that of predictive modelling, it becomes important to think more in depth about how multiple (random) variables may be dependent on one another. Although we have at least considered whether or not there is a relationship at all, i.e. are random variables independent or not, we have not seen how we may describe or quantify the relationships between them when they are present. When speaking about only a few random variables we may name them specifically, e.g. \\(X\\) and \\(Y\\) in the case of two, but when we want to be more general we may talk about multivariate random variables as random vectors of the form \\(X = (X_1, X_2, ..., X_p)^\\top\\). That is \\(X\\) is just a list of \\(p\\) separate random variables. Unlike when we spoke about a random sample \\(X_1, X_2, ..., X_n\\) however, we make no assumptions that the different entries in \\(X\\) have the same distribution, and certainly not that they are independent. 3.5.1 Joint and Conditional Distributions Just as we had probability mass and density functions for single random variables, we have analogous functions in the context of multivariate random variables. For example, if we are in the general setting of a random vector \\(X\\), if all of the elements in \\(X\\) are discrete then the joint probability mass function is given by \\[ p_X(\\x) = P(X_1 = x_1, ..., X_p = x_p) = P(X_1 = x_1 \\cap \\ ... \\ \\cap X_p = x_p), \\] where we distinguish numbers \\(x\\) from vectors \\(\\x = (x_1, ..., x_p)^\\top\\) by faint vs bold font. The same rules (summing to one and being non-negative) apply here as they did for single (univariate) random variables. When we wish to speak about the conditional distribution of one entry (or even multiple entries) in \\(X\\), given values for the others, the same rules as we had for conditional probability apply, i.e. \\[\\begin{align*} p_{X_i|X_{-i}}(x_i|\\x_{-i}) &amp;= P(X_i = x_i|X_1=x_1, ..., X_{i-1}=x_{i-1}, X_{i+1}=x_{i+1}, ..., X_p=x_p)\\\\ &amp;= \\frac{P(X_1=x_1, ..., X_p=x_p)}{P(X_1=x_1, ..., X_{i-1}=x_{i-1}, X_{i+1}=x_{i+1}, ..., X_p=x_p)}\\\\ &amp;= \\frac{P(X=\\x)}{P(X_{-i}=\\x_{-i})}\\\\ &amp;= \\frac{p_X(\\x)}{p_{X_{-i}}(\\x_{-i})} \\end{align*}\\] In the above weve used the subscript \\(_{-i}\\) to mean all except index \\(i\\). Similarly if all entries in \\(X\\) are continuous then we have joint and conditional density functions, which have analogous interpretations. That is, although we dont think about probabilities specifically when thinking about density functions we can intuit the density \\(f_X(\\x)\\) as capturing the relative likelihood/probability of seeing \\(X\\) close to \\(\\x\\). In more complex situations we have have the situation where some of the entries in \\(X\\) are discrete and others continuous, and all the intuitive interpretations carry over. 3.5.2 Moments of Multivariate Random Variables The mean of a vector random variable is also a vector, and is simply equal to the vector of means of each of the entries. That is \\[ E[X] = (E[X_1], E[X_2], ..., E[X_p])^\\top = (\\mu_{X_1}, ..., \\mu_{X_p})^\\top = \\mu_X. \\] When talking about the variance of a multivariate random variable then we need to be precise about what we mean. Certainly the vector \\((\\sigma^2_{X_1}, ..., \\sigma^2_{X_p})\\) has meaning and is relevant. However when we talk the square of a vector \\(\\x\\) we typically mean either the inner or outer product of \\(\\x\\) with itself. However since some of you will not be familiar with these terms, we will simply describe the meaning of that with which we typically describe the second moment of a vector random variable, called the variance-covariance matrix (or simply covariance matrix): \\[ \\Sigma_X = Cov(X) = E\\left[(X-\\mu_X)(X-\\mu_X)^\\top\\right]. \\] This is the \\(p\\times p\\) matrix (like a square table of numbers) and in the \\(i\\)-th row and \\(j\\)-th column we have the quantity \\(E\\left[(X_i-\\mu_{X_i})(X_j-\\mu_{X_j})\\right]\\), which is the covariance between \\(X_i\\) and \\(X_j\\). Note that the covariance of a random variable with itself is just its variance A positive covariance suggests that \\(X_i\\) tends to be large/small when \\(X_j\\) is large/small By large/small we mean well above/below their respective means Why is this the case? If \\(X_i &gt; \\mu_{X_i}\\) tends to happen along with \\(X_j &gt; \\mu_j\\) and \\(X_i &lt; \\mu_{X_i}\\) tends to happen along with \\(X_j &lt; \\mu_{X_j}\\) then the terms \\((X_i-\\mu_{X_i})(X_j-\\mu_{X_j})\\) are usually either positive times positive and negative times negative, i.e., are positive. On the other hand if \\(X_i\\) tends to be large/small when \\(X_j\\) is small/large, then terms in the product are usually one positive and one negative, leading to negative covariance. 3.5.3 Measures of Dependence Although the sign of the covariance gives an indication that there may be a positive/negative relationship between two random variables, it does not on its own give a sense of how strong the relationship is. The reason for this is that it depends on the scale of the random variables. In the following x and y are very weakly related to and have a large covariance, whereas w and z are very strongly related to one another but have a comparatively minuscule covariance. x &lt;- rnorm(1000, sd = 10) y &lt;- x + rnorm(1000, sd = 100) w &lt;- rnorm(1000, sd = 0.1) z &lt;- w + rnorm(1000, sd = 0.05) par(mfrow = c(1, 2)) plot(x, y, main = paste0(&quot;Covariance = &quot;, round(cov(x, y), 4))) plot(w, z, main = paste0(&quot;Covariance = &quot;, round(cov(w, z), 4))) Correlation The correlation between two random variables is a direct standardisation of the covariance to account for their scale. Specifically the correlation between random variables \\(X\\) and \\(Y\\) is given by \\(\\rho_{X, Y} = \\frac{Cov(X, Y)}{\\sigma_X\\sigma_Y}\\). Correlation lies between -1 and 1 with values below -0.9 or above 0.9 indicating a very strong linear relationships between the two variables, and anything between about -0.2 and 0.2 suggesting no substantial (linear) relationship is present. Intermediate values indicate some relationship and the relationship gets stronger the closer to -1 or 1 the correlation gets. par(mfrow = c(1, 2)) plot(x, y, main = paste0(&quot;Correlation = &quot;, round(cor(x, y), 4))) plot(w, z, main = paste0(&quot;Correlation = &quot;, round(cor(w, z), 4))) (Normalised) Mutual Information Correlation captures linear dependence between random variables extremely well. However, it is possible for two variables to be extremely highly dependent on one another and yet have a correlation of zero. u &lt;- rnorm(1000, sd = 1) v &lt;- u^2 + rnorm(1000, sd = .1) plot(u, v, main = paste0(&quot;Correlation = &quot;, round(cor(u, v), 4))) The correlation between \\(X\\) and \\(Y\\) in the above is zero at a population level, but because of the randomness in the samples x and y the sample correlation is slightly off zero. The mutual information (MI) quantifies the amount of shared information is in two random variables. We will not go into information theory in any explicit form, but rather introduce the mutual information only for illustrative purposes. Just like covariance mutual information depends on the scale of the random variables, and various normalised versions of mutual information have been proposed. We will simply use a normalisation which first standardises each of the variables by its standard deviation, i.e. \\(MI(X/\\sigma_X, Y/\\sigma_Y)\\). Lets use the mutinfo function in the package FNN to compute mutual information: ### Start by loading the library library(FNN) ### Now let&#39;s look at all three pairs we generated before par(mfrow = c(1, 3)) plot(x, y, main = paste0(&quot;Normalised MI = &quot;, round(mutinfo(x/sd(x), y/sd(y)), 4))) plot(w, z, main = paste0(&quot;Normalised MI = &quot;, round(mutinfo(w/sd(w), z/sd(z)), 4))) plot(u, v, main = paste0(&quot;Normalised MI = &quot;, round(mutinfo(u/sd(u), v/sd(v)), 4))) Although the (normalised) mutual information is able to capture dependence which is not linear, it cannot describe the nature of the relationship (like positive/negative correlation). We can see that the mutual information estimated from x and y is very close to zero and the linearly dependent z and w have a normalised mutual information close to one. The estimated normalised mutual information from u and v is much higher still, even though their correlation is very close to zero. 3.6 Summary Probability provides a mathematical framework for understanding the properties of chance events, and random variables allow us to transform the outcomes of these chance events into numbers so that we can use the well explored algebra of the real numbers to even better understand them By treating a sample of data as realisations of a random variable, we can then use the theory of probability and random variables in order to (theoretically) quantify things like estimation uncertainty Estimation is the task of using the observations in a sample in order to obtain plausible values of population parameters Confidence intervals provide an explicit communication of estimation uncertainty The central limit theorem and the bootstrap are practical ways of approximating confidence intervals and for quantifying estimation uncertainty We can study the relationships between different variables in our data by treating each observation as a random vector (multivariate random variable). 3.7 Exercises Refer to the airquality data set. Obtain a 95% confidence interval for the mean maximum daily temperature in the month of August. Use the Central Limit Theorem to approximate the confidence interval. Do the same for the month of May. What would you say about the findings? One of the assumptions of the Central Limit Theorem is that the population distribution has finite variance. However, not all random variables have finite variance. The \\(t\\) distributions with degrees of freedom less than two have infinite (or undefined if df is less than or equal to one) variance. Generate a sample of size n = 100 from a \\(t\\) distribution with df = 1.1. (Use help(rt) to find details) and use this to obtain an approximate 95% confidence interval for the mean of the distribution. Repeat a. above 1000 times, and compute the proportion of times the value zero (the mean of the population distribution) lies within the confidence interval. Does this align with what you expect? Repeat the Q2. but using a bootstrap based approach to compute the confidence intervals. Is there a difference? Refer to the bootstrap experiment where we obtained an approximate confidence interval for the skewness, using a sample from a Gamma distribution. a. Re-do this experiment but by using the function `apply` instead of using a `for` loop. (Remember you can always use `help(&lt;function name&gt;)` to find information on how to use a function in R) b. Create a loop which repeats the entire experiment (sampling from the Gamma distribution and then applying the bootstrap to approximate a confidence interval) 1000 times. Calculate the proportion of these in which the true value of the population skewness lies inside the confidence interval. Is this more or less what you expected? Try changing the value of `n` to each of 20 and 200. Did your observations, in terms of the proportion of confidence intervals which contained the true skewness, change? In this chapter we only looked at applying the bootstrap to a univariate sample. What is often of interest, however, is whether or not two variables are dependent or not. a. Write an R function which takes arguments `x`, `y`, `B`and `alpha`; where `x` and `y` should each be samples with the same number of observations (seen as samples of random variables $X$ and $Y$ respectively). The function should return an approximate $(1-2\\alpha)\\times 100\\%$ confidence interval for the correlation between $X$ and $Y$. b. Use this function to obtain an approximate $99\\%$ confidence interval for the correlation between wind speed and temperature, using the `airquality` data set. You should use `B=1000` bootstrap samples. Would you say that there is strong evidence that the true correlation is negative based on this? "],["fundamentals1.html", "4 The Fundamentals of Predictive Modelling I 4.1 Two Archetypal Problems 4.2 Some Preliminaries 4.3 Model Training 4.4 Summary 4.5 Exercises", " 4 The Fundamentals of Predictive Modelling I \\[ \\def\\x{\\mathbf{x}} \\def\\Rr{\\mathbb{R}} \\newcommand{\\argmin}{\\mathop{\\rm argmin}} \\newcommand{\\argmax}{\\mathop{\\rm argmax}} \\def\\F{\\mathcal{F}} \\def\\hbbeta{\\hat{\\boldsymbol{\\beta}}} \\def\\bbeta{\\boldsymbol{\\beta}} \\def\\X{\\mathbf{X}} \\def\\y{\\mathbf{y}} \\] In the last chapter we discussed how multiple random variables may be dependent on one another, as well as some ways to quantify the strength of dependence. So far, however, none of the variables had any special importance over the others. We now turn to our main focus for this module, where we are particularly interested in using the dependence between variables in order to predict likely/reasonable values for a specific one of them, often called the response variable. We will denote the response variable by \\(Y\\), and all \\(p\\) other variables are grouped as \\(X = (X_1, X_2, ..., X_p)\\), and are referred to as a vector of covariates. We would like to use a sample of observations of \\(Y\\) and \\(X\\) in order to estimate a function, into which we can put new values for the covariates and obtain a prediction for what the response might be. The response is sometimes called the dependent variable or target variable, while the covariates may be referred to as predictors. A fundamental point worth making is that we generally cannot expect the covariates to explain absolutely everything about the response Example: We may wish to predict a persons lifespan (the response variable, \\(Y\\)) given their current age, blood pressure, BMI, whether they have diabetes, etc. (our covariates, \\(X\\)) But we do not assume that everyone who is 47 years old; has BP 128/83 and BMI 27.2 and is not diabetic will survive for exactly the same amount of time Rather, there is a distribution of lifespans for people with these characteristics: the conditional distribution of \\(Y\\) given \\(X\\) Predictive modelling is therefore about estimating functions which capture features of the conditional distribution(s) of \\(Y|X\\) E.g. these could be the most likely outcome, or the expected value (i.e., mean) of the outcome In more complex examples we may wish to estimate the entire conditional distribution(s) of \\(Y|X\\), which would allow us to answer very detailed questions. As with everything, however, there is a cost, and the more detail we want to estimate about a distribution the more variance we have to accept in our estimator 4.1 Two Archetypal Problems Just as we distinguished discrete and continuous random variables, so too is it necessary to distinguish between what are the two archetypal predictive modelling problems Regression typically refers to the context where the response variable is numeric Some would argue that all predictive modelling is a form of regression, but we will use this term to refer to the standard regression problem, where we assume the response variable takes on numeric values, usually continuous over some range. Classification refers to the context where the response variable is categorical Categorical variables take on a finite set of values, and typically we do not assume there is any ordering of categories Typical examples include demographic characteristics like gender, race, etc. Categorical variables are discrete, but even if we happen to name the categories as 1, 2 and 3 (for example) we do not necessarily interpret these as numbers in the real sense, since we cant, for example, add two instances of category one and get a category two Although we do not treat categories as numbers, whether we name them with the names of numbers or not, in some contexts it is sensible to invoke an ordering on categories For example we may wish to predict the grades (A, B, C, etc.) of statistics students. It is sensible to say A is better than B is better than C etc., but we still cannot add two Bs to get some other grade We will start by focusing primarily on regression, as some of the fundamental principles associated with predictive modelling are most easily communicated in the regression context. A Simple Example: Car Stopping Distances The cars data set (included in Rs base distribution) contains information documenting the distance needed to bring a car to halt (in feet) for a variety of different initial speeds (in miles per hour, mph). data(cars, package = &quot;datasets&quot;) plot(cars) Unsurprisingly the distance needed to stop a car increases with speed. Unsurprisingly, also, there is variability in the distances even for the same initial speeds. This would be the case even for the same drivers and the same models of car, due to variations in temperature, road surface, etc. In the interest of safety we may wish to model this relationship, so that drivers may be aware of, for example, appropriate following distances depending on speed. The function lm(formula, data) will fit the line of best fit to these data. A formula object in R has the form y~x1 + x2 +  where here y is the name of the response variable within the data frame argument data, and x1 etc. are the names of the covariates we want to include in our model. Here we only have a single covariate speed. model &lt;- lm(dist~speed, data = cars) model ## ## Call: ## lm(formula = dist ~ speed, data = cars) ## ## Coefficients: ## (Intercept) speed ## -17.579 3.932 Printing the model shows that the line of best fit has an intercept at -17.579 and a slope of 3.932. We can add this line to our plot using the function abline plot(cars) abline(model$coefficients) Although the model does a reasonable job capturing the trend of the relationship, it has some obvious limitations. For one, it has a negative intercept and yet we know that distances must be non-negative. There are multiple ways to ensure we do not obtain such a result. Note that the intercept corresponds with the predicted value when all covariates are equal to zero. In the current example it should be clear that if the speed is zero (i.e. the car is not moving) then the distance needed to stop is also zero. We can force R to set the intercept to zero by modifying the formula as y~0 + x1 + x2 + : model0 &lt;- lm(dist~0+speed, data = cars) model0 ## ## Call: ## lm(formula = dist ~ 0 + speed, data = cars) ## ## Coefficients: ## speed ## 2.909 plot(cars) abline(0, model0$coefficients) It may be clear to you that the first model looks better. The line fits closer to the data points, on average. This is because the first model (despite giving some negative predictions) was allowed to choose the best value for the intercept, whereas the second model did not have such a choice. The first model had one extra degree of freedom, and is a more flexible model. It is worth considering what the purpose of this modelling problem is, however. Given that we may be more concerned about stopping distances for high speeds, a model which gives negative predictions for small speeds may not be a problem if it is more accurate over the sorts of values we really care about. Recall that models do not have to be right in order to be useful. 4.2 Some Preliminaries This is an appropriate place to pause briefly, before we continue to learn more about how to fit and assess predictive models, as there are important components of the predictive modelling pipeline which precede these phases. However, as these are not the main focus of the module, and most of these are covered elsewhere in your degrees, we will only cover these superficially here. It is also the case that some of the reasons for why things are done the way they are will not be clear until we cover material which comes later on in the module. 4.2.1 Problem Objectives It is crucial to be aware of what the problem to be addressed actually is, as well as what a success means in this context. This should inform which sort of models we might wish to consider, as well as which criteria we should use to select a final model (or models). In the previous example whether we would have preferred the first or second model would have depended on whether we needed the model to appropriately respect physical laws (like not giving negative predictions for a distance) or whether we were willing to concede some impossible predictions for better accuracy where it matters. It is hopefully clear that we want a predictive model to at least fairly accurately predict the target variable. However in some circumstances we may care more about why a model is giving the predictions it is. Lets return to our diabetes example. It is undeniable that being able to predict whether someone is likely to develop type II diabetes is a useful thing. However, unless we are able to also propose interventions which can reduce this risk for high risk individuals, such information becomes less useful. Some models have a fairly simple structure, in that the way they capture the relationships between the covariates and the response are easy to understand and sometimes quantify. We typically use the word interpretable in this context. Although there are certainly many exceptions to this, it is generally the case that the more flexible/complex models (which are less interpretable) are able to provide more accurate predictions. As with everything, there is a trade-off. 4.2.2 Data Acquisition Historically statisticians would design experiments with a particular goal in mind, and then subsequently have to go out into the field to collect their own data, which would allow them to address the problem they face. In this way the data could be gathered in a way which made them as amenable as possible to analysis; avoiding any deviations from what we previously referred to as our modelling assumptions. This experimental design remains extremely important, and necessary in fields like medicine and pharmaceuticals. However, these days many statisticians and data scientists working in industry will never actually be involved in the data collection process, nor in the design of the procedure by which the data are collected. More often data are just collected because companies know that data are extremely valuable. Very often the problems to be addressed, or experiments to conduct, or analyses to be done, only arise long after the data have been collected. Nonetheless it is very important to be aware (wherever possible) of how data have been obtained as this will allow one to assess whether/which modelling assumptions are reasonable. It is also important to be aware of potential pitfalls associated with such instances. One important consideration is that one should not use the same data in order to decide on your problem objectives AND to perform the subsequent analysis unless one knows and fully understands the potential implications This is a form of what is known as data leakage, and can lead to substantial bias and poor decisions. 4.2.3 Exploratory Data Analysis Once the data have been collected, there is typically an initial inspection, often referred to as Exploratory Data Analaysis (EDA). There are two types of EDA (i) an initial superficial inspection of data types, data integrity issues, and at most checking univariate statistics associated with the variables; and (ii) more in-depth inspection which may include checking relationships between variables, choosing potential transformations, checking for outliers, etc. The more in-depth type of EDA should more appropriately be paired with the actual modelling phase, and should respect the boundaries associated with data splitting, which we come to in the next chapter. For now we only consider the initial EDA, briefly. 4.2.3.1 Initial EDA The initial EDA is in place primarily to get a feel for the data, and to ensure they can actually be processed appropriately by the software we are choosing to use. This may include Checking the types of variables: How many numeric variables do I have, and how many categorical variables? Are categorical variables nominal or ordinal? Are categories within nominal variables all well enough represented, or are there some categories which only arise in one or a few data points? Checking data integrity: Are there any obvious erroneous data points? For example distances/volumes/etc. which have negative values. Are there missing data? It is common that data sets have some entries which are missing, and handling missing data is an entire field of statistics. Missing data can simply be due to human (or digital) error in data capturing, but are also common in survey data where respondents may either not have information relevant to some questions or may choose to withhold that information. Univariate statistics: Simple summary statistics, like five-number-summaries, etc: These may highlight data integrity issues, such as implausible or impossible values. Histograms: All other things being equal, numeric variables whose distributions are at least roughly symmetric and unimodal (have a single maximum or maximal region) are more amenable to analysis and inclusion in predictive models than very skewed ones. Sometimes simple transformations of variables which lead to nicer marginal distributions is beneficial. Initial EDA should be fairly superficial, and in fact if not there is a risk of over analysis which can lead to problematic data leakage. Ultimately the initial EDA should ideally only be for checking data integrity and useability, and should not include steps which investigate the relationships between the covariates and the response. We will cover some of the basic EDA tasks in the context of practical examples later on. 4.2.4 Data Cleaning and Pre-processing Based on the outcomes of the EDA, we then go on to perform any tasks which are deemed important either to be able to process/handle the data, or to appropriately model the relationships therein. Common steps include Imputation of missing entries: As mentioned previously handling missing data is itself a broad field, and we will not go into much depth on the topic. Removal of problematic cases: Some models are very sensitive (i.e. not robust) to observations which are far removed from the rest, or which dont respect the general relationships between the variables among the other observations. Removal of observations is a controversial topic, and ultimately it is down to the person conducting the modelling whether to remove points or to modify the model to make it more robust to these cases. Scaling: Some models are sensitive to the scale on which covariates are measured/captured. What this means is that, unless appropriately scaled to have similar overall variation, the variables which have larger scale will have a greater influence on the model purely by nature of how they were captured and not because they are intrinsically important to the prediction task. Categorical Covariates and Dummy Variables Because of the frequency with which categorical covariates arise in practice is so high, and the potentially very misleading results which could arise if we incorrectly handle them, we will look in a little bit of depth at the pre-processing needed to handle them. Although we spoke about the distinction between regression and classification when the response variable is either numeric or categorical, many models in their standard formulation do not handle categorical covariates directly. It should be pointed out that we can force the issue artificially by naming our categories with numbers, and then treating the categorical variables as though they are numeric. This will not stop R from processing the data as instructed, however it should be clear that this is inappropriate, not least of all because by treating the categories as numbers we are imposing some ordering on them which may be totally inappropriate. For example: If we had data related to difference species of animal and one of our covariates described the family of species as, e.g. feline, canine and ovine, then if we decided to encode these as feline = 1, canine = 2 and ovine = 3, if we treated these as numbers we would be saying that a cat plus a sheep is equal to two dogs! It is also worth pointing out that in some case categorical variables will have been stored as numeric and we have to convert these to categorical in order for R to do the right thing. We will encounter some instances of this as we go forward. The most common approach for handling categorical variables is with the use of dummy variables. Specifically, suppose one of the covariates, say \\(X_j\\), is a categorical variable with \\(K\\) different categories. To capture the information in this variable we can replace \\(X_j\\) with \\(K-1\\) 0/1 (binary) variables so that if an observation of \\(X_j\\) is in the \\(k\\)-th category (for \\(k = 1, ..., K-1\\)) we set all of these 0/1 variables except the \\(k\\)-th equal to zero, and the \\(k\\)-th equal to one. And if the observation is in category \\(K\\) we simply set them all to zero. Hopefully it can be seen that the information in these \\(K-1\\) dummy variables is equivalent to the single categorical variable since we can easily recreate each perfectly from the other. It should also be clear that encoding the data in this way does not impose any ordering on the categories. 4.2.5 Feature Engineering and Transformation In some situations the data are not in an appropriate format for modelling using standard approaches. For example longitudinal data include measurements taken over time, and the timing and number of measurements may differ by individual. Other examples include image and text data, which do not fit the format of a response variable \\(Y\\) and a vector of covariates \\(X\\), a format typically referred to as tabular data. Historically feature engineering, that is the derivation of covariates from either multiple other covariates or these non-standard data formats, was frequently achieved using domain knowledge or common sense approaches. The advent of neural networks, which build the feature engineering into the model training process, however, has utlimately eclipsed these manual approaches in many contexts. 4.3 Model Training From the point of view of statistical learning, the primary topics of interest relate to the tasks of model training and model selection. These are strongly related topics, and we cover the basics of training from the point of view of regression in this chapter. Training is a term which emerged in the machine learning literature, which ultimately refers to the process of estimating, or fitting a predictive model. 4.3.1 Regression For simplicity we focus, for now, on the context where \\(Y\\) is a numeric variable, and the problem of predicting \\(Y\\) from \\(X\\) is called regression. Since we cannot expect \\(X\\) to explain everything about \\(Y\\), we typically describe the relationship between them via the regression equation: \\[ Y = g^*(X) + \\epsilon, \\] where \\(g^*\\) is the true regression function and \\(\\epsilon\\) is called the residual and characterises what \\(X\\) does not explain/capture about \\(Y\\) Statistically we treat \\(\\epsilon\\) as a random variable, and assume that it has mean zero (\\(E[\\epsilon] = 0\\)) and is independent of \\(X\\) When \\(\\epsilon\\) and \\(X\\) are not independent, for example the variance of the residual may be different for different \\(X\\), we can sometimes transform \\(Y\\) (and hence \\(\\epsilon\\)) in such a way that they become closer to independent Recall: all models are wrong, but some are useful Note that \\(E[\\epsilon] = 0 \\Rightarrow E[Y|X] = g^*(X)\\). That is, the function \\(g^*(X)\\) captures the conditional expectation of \\(Y\\) given \\(X\\). In fact very often we make the further assumption that \\(\\epsilon\\) has a normal distribution. Why might a normal distribution be a reasonable assumption? One way to think of the reason why we have a residual term at all, i.e. why the covariates dont explain absolutely everything about \\(Y\\), is simply that there are other factors which contribute to the variation in the values of \\(Y\\) which we have not measured (i.e. factors which are not included in the covariates we actually have). If we think of the residual as being the sum of all the contributions from these unmeasured factors, and we know that summing random variables together often results in a roughly normal distribution, we can see that the treating the overall effect of these unmeasured factors as normally distributed is not unreasonable. There are many other arguments, such as the fact that for fixed variance the normal distribution is the most uncertain in an information theory sense and it may be prudent to presume the least amount of knowledge about the residual; as well as the fact that mathematically the normal distribution is nice to work with. A Simulated Example The following piece of code simulates potential values of the response variable for pairing with values of a single covariate, \\(X\\), which are equally spaced on the interval \\((0, 1)\\), and where the regression equation is given by \\[ Y = 1+4X-3X^2-X^3+2X^4 + \\epsilon, \\] where \\(\\epsilon \\sim N(0, 1/25)\\). That is, the regression function \\(g^*\\) is a degree four polynomial and the residual term is a normal random variable with standard deviation \\(1/5\\). ### The regression function, a degree 4 polynomial g &lt;- function(x) 1 + 4*x - 3*x^2 - x^3 + 2*x^4 ### For now we will fix thiry values of X equally spaced ### in the interval (0, 1) X &lt;- 1:30/31 ### We can simulate potential values for Y associated with ### these, by adding residuals. We will simulate residuals ### from a N(0, 1/25) distribution y &lt;- g(X) + rnorm(30, sd = 1/5) plot(X, y, xlab = &#39;x&#39;, ylim = c(0.75, 3.25)) ### We can also add the line showing the true regression function lines(seq(-.1, 1.1, length = 100), g(seq(-.1, 1.1, length = 100)), lwd = 2) But the realisations of \\(Y\\) could be different even if the values of \\(X\\) didnt change, since for each \\(X\\) there is an entire distribution of \\(Y|X\\). If you re-run the code you will see a different set of potential realisations. 4.3.1.1 Fitting Regression Models In practice we have access to a sample of realisations of \\(Y\\) and \\(X\\), \\(\\{(y_1, \\mathbf{x}_1), (y_n, \\mathbf{x}_n)\\}\\), and from this we want to estimate the regression function \\(g^*\\) In predictive modelling we often talk about fitting models, or in the machine learning context training them The sample of observations used to estimate/fit/train the model is often called the training set, and this terminology will become more important in the following chapter Estimating functions is, however, in general not as straightforward as estimating the parameters of a distribution. We need to first ask ourselves what do we want from our model? Ultimately we want to use our regression model in order to predict likely/appropriate values for the response: We would like for our predictions to be close to the actual values. Although the importance of interpretability of the model must in some contexts not be overlooked, a model which doesnt actually predict the response accurately at all is not really modelling the relationships between the response and covariates appropriately, and hence the interpretations it offers may be meaningless. The first step is therefore to ensure that the predicted values for the responses in our training set are in general similar to their actual values If they arent then we couldnt reasonably expect the predictions for new sets of covariates to be close to their corresponding responses We already know the values for \\(Y\\) in our sample, and our ultimate goal is to use our model to obtain predictions on sets of covariates from other members of the population from which our sample came We thus seek to quantify how similar/dissimilar our predictions are to/from the actual values, by way of a loss function. Some Notation: Returning to the hat notation, we will refer to our fitted regression model/function as \\(\\hat g\\), and for a specific prediction (i.e. for a fixed \\(\\mathbf{x}\\)) we sometimes use \\(\\hat y\\) to be the predicted value, \\(\\hat y = \\hat g(\\mathbf{x})\\) When considering the statistical properties of \\(\\hat g\\), we again (as in Chatper 3) think of a random sample (random training set), \\((Y_i , X_i); i = 1, ..., n\\), and \\(\\hat g\\) becomes a random function We differentiate the notation for a prediction by using \\(\\hat Y\\) to be the predicted value from the random function \\(\\hat g\\) , i.e. the one trained on the random sample We will use \\(L\\) to denote the (generic) loss function, and it takes two arguments: \\(y\\) and \\(\\hat y\\) (or \\(Y\\) and \\(\\hat Y\\), depending on context) Returning to the fact that we want the predictions on our training sample, i.e. \\(\\hat y_i = \\hat g (\\x_i); i = 1, ..., n\\) to be close to the actual \\(y_i\\)s, we can seek to characterise how different they are overall by the average training loss \\[ L_{train}(\\hat g) = \\frac{1}{n}\\sum_{i=1}^n L(y_i, \\hat g(\\x_i)) \\] We will typically refer to this as the training error \\(L_{train}\\) is a function of \\(\\hat g\\) since if we had a different \\(\\hat g\\) wed have a different training error Given our objective, it would be sensible to choose our fitted model as that which gives the smallest training error \\[ \\hat g = \\argmin_{g \\in \\mathcal{F}} \\frac{1}{n} \\sum_{i=1}^n L(y_i, g(\\x_i)). \\] There are crucial subtleties at play, which we get into in Chapter 5, but for now we will work with this objective of minimising training error Explaining the notation: The object \\(\\mathcal{F}\\) is a collection of functions from which we are allowed to select our fitted model, and we will go into this more later on argmin means an element in this collection which gives the lowest possible training error \\(g^*\\) vs \\(g\\) vs \\(\\hat g\\): \\(g^*\\) is the TRUE regression function, \\(g\\) is an arbitrary function in \\(\\mathcal{F}\\) (a potential candidate for \\(\\hat g\\)), and \\(\\hat g\\) is the actual member of \\(\\mathcal{F}\\) which we choose 4.3.2 A Concrete Example: Simple Linear Regression So far we have considered things at a very high level. We now consider a very straightforward example, the simple linear regression model. At this stage the main decision points when facing a regression problem are (i) which loss function should I use?, and (ii) from which collection of functions should I select my estimate?. In the context of regression it is sensible to quantify the dissimilarity between the true and predicted responses with some measure of the distance between them. For example, we may define our loss function to be either the squared error, \\(L(y, \\hat y) = (y- \\hat y)^2\\); or the absolute error, \\(L(y, \\hat y) = |y - \\hat y|\\). As it turns out the function which minimises the squared error loss over the entire distribution of potential values for \\(X\\) and \\(Y\\) is the function \\(g^*(\\x) = E[Y|X=\\x]\\). It is for this reason (as well as some others) that the squared error loss is by far the most commonly used loss function in the standard regression context. The function which minimises the absolute error over the whole distribution is the function whose output is the conditional median of \\(Y|X\\). Now, in the simple regression context we have only a single covariate, \\(X\\), and when we talk about linear regression we mean that the class of functions from which we select our estimate, \\(\\mathcal{F}\\), is specifically all linear (or more correctly affine) functions of the covariate, i.e. each \\(g\\) in \\(\\mathcal{F}\\) may be written as \\[ g(x) = \\beta_0 + \\beta_1 x, \\] for some real numbers \\(\\beta_0\\) and \\(\\beta_1\\). \\(\\beta_0\\) and \\(\\beta_1\\) parameterise the function \\(g\\) and we say that \\(\\F\\) is a parametric class, since all the functions in \\(\\F\\) have the same general form, and as long as we know the values of the parameters we can combine them with this general form in order to express/define the function fully Since each candidate for \\(\\hat g\\) can be written in this form, we must also have \\[ \\hat g(x) = \\hat \\beta_0 + \\hat \\beta_1 x, \\] where \\[ \\hbbeta = \\argmin_{\\bbeta \\in \\Rr^2} \\frac{1}{n}\\sum_{i=1}^n \\left(y_i - \\beta_0 - \\beta_1 x_i\\right)^2. \\] We refer to \\(\\hbbeta\\) as the (estimated) vector of regression coefficients, with \\(\\hat \\beta_0\\) called the intercept term as this is the point where the function \\(\\hat g\\) cuts through the y-axis. We already saw the simple linear regression model informally when we looked at the cars data at the start of this chapter. The formulation above is (hopefully) a lot less scary than the general problem of choosing the best function from a collection of functions, since now we are just looking for the best two numbers (values for the \\(\\hat \\beta\\)s). Generally speaking optimisation of parametric functions; those which only depend on a fixed number of values (the functions parameters) is comparatively more straightforward than general functional optimisation. You will cover the topic of optimisation, how we actually go about finding these optimal parameters, in the Foundations of Data Science and AI module In the case of simple linear regression, there is a closed form solution for the optimal parameters, meaning we dont actually need an optimisation algorithm to find them. In particular we have\\[ \\hbbeta = (\\X^\\top \\X)^{-1}\\X^\\top \\y, \\] where \\(\\X\\) is the \\(n \\times 2\\) matrix with \\(i\\)-th row \\((1, x_i)\\). \\(\\y\\) is the vector \\((y_1, y_2, ..., y_n)^\\top\\). In fact even if we had more than one covariate, and so would have our estimated linear (affine) function expressed as \\(\\hat g(\\x) = \\hat\\beta_0 + \\sum_{j=1}^p \\hat \\beta_j x_j\\), the equation above would still provide the optimal solution except \\(\\X\\) would change so that the rows are of the form \\((1, \\x_i^\\top)\\). We will look briefly at this multiple linear regression in far more depth in Chapter 6. A quick note on notation. As mentioned in Chapter 3 we will use bold font to indicate vectors and faint font for individual values of a variable. For example, when we have a single covariate our observations of that covariate are \\(x_1, ..., x_n\\). On the other hand if we have multiple covariates then our observations are \\(\\x_1, ..., \\x_n\\) and the \\(j\\)-th element of \\(\\x_i\\) will be written as \\(x_{ij}\\). In this context we will also need to express our regression function (either estimated or true) as taking a generic vector argument \\(\\x = (x_1, ..., x_p)^\\top\\), and so there is some overlap between the notation for the \\(i\\)-th element of the generic vector argument \\(\\x\\) and the \\(i\\)-th observation of a single covariate. However, there should be no ambiguity as the contexts in which these will arise are distinct. Simple Linear Regression in R Lets return to the cars data set we saw earlier. We already saw that the function lm produces a line of best fit, and in fact is solving the optimisation problem which minimises the training error, using the squared error loss function. We can validate this by comparing its output with the closed form solution described above ### Our X matrix described above has a column of ones followed ### by a column containing the single covariate. We can use ### the cbind function to produce this X &lt;- cbind(1, cars$speed) ### Next we can find the coefficients for the best linear fit to the data y &lt;- cars$dist beta_hat &lt;- solve(t(X)%*%X)%*%t(X)%*%y beta_hat ## [,1] ## [1,] -17.579095 ## [2,] 3.932409 You may recall these are indeed what we saw from the output of the function lm. For the vast majority of the time we will be using existing functions in Rs numerous libraries. Many of these will be more versatile and efficient than our own code, however being able to validate these existing implementations, when possible, is beneficial. The output of lm contains more than just the estimates for \\(\\hat \\beta_0\\) and \\(\\hat \\beta_1\\). For example, it also contains the field $fitted.values which is the vector of values \\((\\hat y_i, ..., \\hat y_n)^\\top\\) from the model. We can use this to evaluate the training error: model &lt;- lm(dist~speed, data = cars) mean((cars$dist - model$fitted.values)^2) ## [1] 227.0704 A Straightforward Extension: Simple Polynomial Regression An obvious question which might arise is why linear?. We will look to far greater depth at flexible regression models later on, but for now we simply engage in a an illustrative experiment using polynomials: If \\(\\F\\) contains all degree \\(d\\) polynomials of our covariate, then we will have \\[ \\hat g(x) = \\hat \\beta_0 + \\sum_{j=1}^d \\hat \\beta_j x^j, \\] where \\[ \\hbbeta = \\argmin_{\\bbeta \\in \\Rr^{d+1}} \\frac{1}{n}\\sum_{i=1}^n \\left(y_i - \\beta_0 - \\sum_{j=1}^d \\beta_j x_i^j\\right)^2. \\] Within an R formula we can use the function poly(variable, degree, raw = TRUE) to produce a polynomial expression, and so directly fit such a model using the lm function. The argument raw = TRUE tells the function not to transform the variable argument in any way, which it would do by default otherwise. The following code will fit a quadratic (degree 2 polynomial) function to the cars data set, and then plot the result and evaluate the training error. ### First plot the data again plot(cars) ### Now we fit the model model2 &lt;- lm(dist~poly(speed, 2, raw = TRUE), data = cars) ### We now cannot use the abline function to illustrate the fit, but can use ### predict(model, new_data) for a range of speeds (e.g. 0 to 30) and use ### the function lines() to add to the plot. The new data argument must ### have the same covariates as the data used to fit the model: new_data &lt;- data.frame(speed = seq(0, 30, length = 100)) lines(new_data$speed, predict(model2, new_data)) ### We can use the same command again to evaluate the training error mean((cars$dist - model2$fitted.values)^2) ## [1] 216.4943 How does this training error compare with the training error from the linear model? Ask yourself Is this what I expected? Notice that the expression \\(\\hat \\beta_0 + \\sum_{j=1}^d \\hat \\beta_j x^j\\) has remarkable similarity to the equation describing the fitted muliple linear regression model \\(\\hat \\beta_0 + \\sum_{j=1}^p \\hat \\beta_j x_j\\). The only difference is that instead of the single covariate raised to power \\(j\\) we have the \\(j\\)-th from \\(p\\) total covariates, i.e. we see \\(x_j\\) instead of \\(x^j\\). Indeed fitting the degree \\(d\\) polynomial regression model is equivalent to fitting the multiple linear regression model where we use the engineered features \\(X_2 = X^2, X_3=X^3, ..., X_d=X^d\\). We could therefore equally apply the closed form expression for the multiple linear model after creating these features for ourselves. ### Start by creating our new X matrix with a column of ### ones and then the single covariate raised to each of the ### powers 1 to d, where to match the above we have d=2 d &lt;- 2 X &lt;- sapply(0:d, function(j) cars$speed^j) # make sure you understand why this is creating the matrix it should y &lt;- cars$dist beta_hat &lt;- solve(t(X)%*%X)%*%t(X)%*%y ### We can check that the solutions are the same: cbind(beta_hat, model2$coefficients) ## [,1] [,2] ## (Intercept) 2.4701378 2.4701378 ## poly(speed, 2, raw = TRUE)1 0.9132876 0.9132876 ## poly(speed, 2, raw = TRUE)2 0.0999593 0.0999593 Using these nonlinear transformations of the covariate(s) as additional features/covariates is a principled approach, and we will cover this idea in greater detail in Chapter 8. 4.4 Summary One of the major components of statistical learning is how we go about training predictive models Two important decision points are (i) which loss function should we use for training, and (ii) which sort of functions should we choose our estimate from? Even before we get to the training phase, however, we should ideally Be fully aware of what the problem objectives are, as this could inform how we choose our loss function and collection of functions \\(\\F\\) For example, if it is necessary that the model is reasonably interpretable, then we should choose \\(\\F\\) to contain only models which allow for interpretation Be aware of how the data have been obtained/collected, as these will help determine which modelling assumptions are reasonable to make Perform appropriate EDA and cleaning/preprocessing to ensure we can actually apply the methods we want during training (and the steps which come later) We saw that linear regression is a type of predictive modelling where we fit linear/affine funtions (our class \\(\\F\\)) to capture the relationship(s) between \\(X\\) and a numeric response \\(Y\\) We saw a simple extension where we introduce non-linearities using powers of the covariate (polynomials) using the poly() function within a linear regression model We also saw that this is equivalent to adding to our covariate some additional covariates which are engineered from the original covariate, and then applying linear regression 4.5 Exercises Refer to the cars data set. Fit polynomial models for degrees up to eight, to predict dist from speed. Compare the fits visually and numerically. What do you think about which model(s) is/are likely to be the best? Use the bootstrap to estimate the variance of the coefficient estimators for the polynomial models in Q 1. above. Download the nhanes.RData file in the R Scripts folder on Moodle. Load the data using load(&lt;full path to filename&gt;) or set the working directory to the location where the file lies and use load(\"nhanes.RData\"). This is a small subset of the National Health And Nutrition Examination Survey data set. If you want details on the specific variables which have been included, you can install and load the NHANES package and then call help(NHANES). The goal is to predict Mean Arterial Pressure (MAP; defined as Diastolic Blood Pressure + \\(\\frac{1}{3}\\)(Systolic - Diastolic Blood Pressure)) using the non Blood Pressure related variables. Perform some EDA and data pre-processing, before fitting a linear regression model (or multiple models) to predict MAP. Note that some of the factor variables might reasonably be treated as numeric. That is not to say they should, but if you want to you could consider models which treat them as factors and ones which treat them as numeric. "],["fundamentals2.html", "5 The Fundamentals of Predictive Modelling II 5.1 A Quick Recap 5.2 Overfitting 5.3 Prediction Error and Generalisation 5.4 Estimating (Expected) Prediction Error 5.5 Summary 5.6 Exercises", " 5 The Fundamentals of Predictive Modelling II \\[ \\def\\x{\\mathbf{x}} \\def\\Rr{\\mathbb{R}} \\newcommand{\\argmin}{\\mathop{\\rm argmin}} \\newcommand{\\argmax}{\\mathop{\\rm argmax}} \\def\\F{\\mathcal{F}} \\def\\hbbeta{\\hat{\\boldsymbol{\\beta}}} \\def\\bbeta{\\boldsymbol{\\beta}} \\def\\X{\\mathbf{X}} \\def\\y{\\mathbf{y}} \\def\\hg{\\hat g} \\] In the previous chapter we looked mainly on model training, where we focus on estimating predictive models by minimising a loss function. In this chapter we look mainly at evaluating models so that we can use these evaluations in order to select which (from multiple possible models) to take forward for actually predicting on cases which are not in our observations. 5.1 A Quick Recap Recall the regression setting, where a numeric response variable, \\(Y\\), is related to covariates \\(X\\) via the equation \\[ Y = g^*(X) + \\epsilon \\] We can think of \\(g^*(X)\\) as the signal (what is predictable about \\(Y\\) from \\(X\\)) and \\(\\epsilon\\) as the noise (what is not predictable) We are considering fitting a model by minimising training error: \\[ \\hat g = \\argmin_{g \\in \\F} \\frac{1}{n}\\sum_{i=1}^n (y_i - g(\\x_i))^2 \\] We finished by looking at polynomials of a single covariate: \\(\\hat g(x) = \\hat \\beta_0 + \\sum_{j=1}^d \\hat \\beta_j x^j\\), where \\(d\\) is the degree of the polynomial 5.2 Overfitting We saw in the last section that as we increased the degree of the polynomial, we were able to fit better and better to the training data (getting smaller and smaller training error) This is because the higher degree models have more flexibility: Lets use the notation \\(\\hat g_d\\) to be the fitted degree \\(d\\) polynomial model, so that \\[\\begin{align*} \\hat g_d(x) &amp;= \\hat \\beta_0 + \\sum_{j=1}^d \\hat \\beta_j x^j,\\\\ \\mbox{where } \\hbbeta &amp;= \\argmin_{\\bbeta \\in \\Rr^{d+1}} \\frac{1}{n}\\sum_{i=1}^n \\left(y_i - \\beta_0 - \\sum_{j=1}^d \\beta_j x_i^j\\right)^2 \\end{align*}\\] But for any \\(d&#39; &gt; d\\) we could also write \\(\\hat g_d(x) = \\hat \\beta_0 + \\sum_{j=1}^d \\hat \\beta_j x^j + \\sum_{j=d+1}^{d&#39;} 0 x^j\\) In other words, \\(\\hat g_d\\) is also a degree \\(d&#39;\\) polynomial, but we are only allowed to choose the value zero for all of the coefficients \\(\\hat \\beta_{d+1}, ..., \\hat \\beta_{d&#39;}\\) On the other hand the fitted degree \\(d&#39;\\) model, \\(\\hat g_{d&#39;}\\) is allowed to have zero OR non-zero values for these coefficients, and would choose whichever gave the lower training error These extra options available to \\(\\hat g_{d&#39;}\\) means it is a strictly more flexible model than \\(\\hat g_d\\), and so we should expect it to have lower training error (it is impossible for it to have higher training error) If we took this idea to the extreme, we would be able to fit the training data perfectly (i.e. achieve zero training error) if we went as far as using a degree \\(n-1\\) polynomial The fact that we can always fit the training data perfectly, provided we have a flexible enough model, should give us pause when choosing to fit models only based on training error. Indeed we saw previously, when using the cars data set, that although our training error kept decreasing as we increased the degree of the polynomial, when plotting the functions which had been fit they did not seem like realistic representations of the actual relationship between speed and stopping distance. Another way of viewing this is that a model which fits the data perfectly must essentially have memorised every aspect of them, even the random residual terms which are, by definition, unpredictable A model which fits too well to the data (incorporating all or most of the noise) is said to overfit the data When we try to make predictions from such a model it will ultimately be reproducing the noise from the training data. Example: Lets return to the example we saw right at the beginning of this section. We have the setup where \\[\\begin{align*} Y = 1 + 4X - 3X^2 - X^3 + 2X^4 + \\epsilon, \\end{align*}\\] where \\(X\\) is uniformly distributed on \\((0,1)\\) and \\(\\epsilon \\sim N(0, 1/25)\\). In the following code chunk we simulate a set of \\(n = 30\\) realisations of \\(X\\) and \\(Y\\). ### First let&#39;s set up what is fixed #sample size n &lt;- 30 # residual standard deviation sigma_resid &lt;- 1/5 # true regression function g &lt;- function(x) 1 + 4*x - 3*x^2 - x^3 + 2*x^4 ### Now we can sample our realisations of X and Y x &lt;- runif(n, min = 0, max = 1) y &lt;- g(x) + rnorm(n, sd = sigma_resid) ### We then fit a high degree polynomial, say with degree six model &lt;- lm(y~poly(x, 6), data = data.frame(x, y)) ### For plotting reasons we can obtain predictions over a grid of values of x x_grid &lt;- data.frame(x = seq(from = -.1, to = 1.1, by = 0.01)) predicted &lt;- predict(model, x_grid) ### Finally let&#39;s visualise the sample points, the true function and ### the fitted function plot(x, y) curve(g, from = -.1, to = 1.1, add = TRUE) lines(x_grid$x, predicted, col = 2) Try re-running the above code multiple times. You will see how much the fitted function varies from sample to sample. This is because the model is trying to model too much of the data and is capturing a lot of what makes the samples different (the residual noise terms) and not only what makes them similar (the actual functional signal component). Now try changing the degree of the polynomial and run the code multiple times. What do you observe? Although we never know the true function in practice, for a comparison like we have here, using simulations like this in order to see how well different types of models tend to perform can be very instructive. 5.3 Prediction Error and Generalisation What we actually want is not a model which simply reproduces the training data, but one which generalises well, meaning that it predicts well/accurately on new cases (i.e. those not in our sample). We already know the values of \\(Y\\) in our sample, and we want our model to pick up on the trend in the relationships with the covariates so that it will be able to reproduce that trend in its predictions, but without unnecessary incorporation of the noise. The prediction error (or generalisation error) of a model, \\(\\hg\\), is its expected error/loss over the entire distribution of potential pairs of \\(Y\\) and \\(X\\): \\[\\begin{align*} PredictionError(\\hat g) &amp;= E_{X,Y}\\left[L(Y, \\hg(X))\\right]. \\end{align*}\\] In the standard regression setting we have \\(L(y, \\hat y) = (y-\\hat y)^2\\) and so \\[\\begin{align*} PredictionError(\\hat g) &amp;= E_{X,Y}\\left[\\left(Y - \\hg(X)\\right)^2\\right]. \\end{align*}\\] Typically the training error underestimates the prediction error, since the model has been fit/chosen/optimised specifically to minimise the error on the training data, and not the entire distribution of \\(Y\\) and \\(X\\) Note that this is the case even in the absence of overfitting, but overfitting will tend to lead to a larger difference between prediction error and training error Moreover we cannot directly tell whether a model has overfit or not, but models with more flexibility have a greater risk of overfitting. But there is another side of the coin, and if a model is not given enough flexibility, it may underfit, meaning it may not pick up much on the noise in the data but may be so inflexible that it also misses a lot of what is actually the signal. Perhaps the most difficult problem in predictive modelling is identifying the right amount of flexibility: so that the model can capture the nuances of the regression function \\(g^*\\), but is not so flexible that it captures an unnecessarily large amount of the noise as well. Choosing a model, or the right amount of flexibility to give a model when training, is known as model selection. If we somehow had direct access to the prediction error of a model, or of a collection of models given different amounts of flexibility, then selecting the one with the lowest prediction error is an obvious approach Or, if model interpretability is also a key aspect of the analysis, then we may choose a model based on some trade-off of accuracy and interpretability. 5.3.1 Expected Prediction Error It turns out that obtaining a reliable enough estimate for prediction error of a model, \\(\\hat g\\), for the purpose of model selection, is not so easy. We typically instead rely on the expected prediction error of \\(\\hat g\\) as an estimator: \\[\\begin{align*} E\\left[PredictionError(\\hat g)\\right] = E_{\\hat g, X, Y}\\left[L(Y, \\hat g(X))\\right]. \\end{align*}\\] How this differs from prediction error is: Instead of the prediction error of the model I obtained from my specific training data, it quantifies the average prediction error of the models I could have obtained by using the same fitting/training/estimation procedure (the same estimator) on infinitely many training data sets from the same population 5.3.2 The Bias-Variance Tradeoff Expected prediction error has (at least) two advantages over prediction error: We can obtain more stable estimates for it, and we can use the statistical properties of \\(\\hat g\\) (as an estimator) to better understand it. In particular, in the regression setting, and using as before the squared error as the loss function, it can be shown that \\[\\begin{align} e^2(\\hat g) :=&amp; E\\left[PredictionError(\\hat g)\\right]\\\\ =&amp; \\sigma_{\\epsilon}^2 + \\int_{\\Rr^p} f_X(\\x) Bias(\\hat g(\\x))^2 d\\x + \\int_{\\Rr^p}f_X(\\x) Var(\\hat g(\\x))d\\x,\\label{eq:biasvariance} \\end{align}\\] where \\(\\sigma^2_\\epsilon\\) is the variance of the residual \\(\\epsilon\\). \\(\\sigma_\\epsilon^2\\) is called the irreducible error since there is no model which has prediction error below this quantity. \\(\\sigma_\\epsilon^2\\) is the prediction error of \\(g^*\\) The second term corresponds with the average squared bias of \\(\\hg\\), across all possible values of \\(X\\). Recall that the bias of \\(\\hg\\) at a particular \\(\\x\\) is \\(E[\\hg(\\x)]  g^*(\\x)\\), i.e. how far off \\(\\hg\\) is from \\(g^*\\), when evaluated at \\(\\x\\), on average over infinitely many training sets The final term is the average variance of \\(\\hg\\), again across all possible values of \\(X\\). We will refer to these latter two terms simply as the model bias and model variance, and they are statistical properties of the estimator \\(\\hg\\). The sum of the (squared) bias and the variance is also sometimes referred to as the risk, and quantifies the excess error of the model over the best possible model \\(g^*\\) It is worth noting that although this exact decomposition holds only for the squared error loss function, and where \\(Y = g^*(X) + \\epsilon\\), the intuition we can gain from it applies to other contexts as well. Broadly speaking for models to have low bias, they require more flexibility in order to be able to fit to a potentially complex regression function, \\(g^*\\) When \\(g^*\\) is relatively simple, we may not need much flexibility, but typically adding flexibility does not increase bias, but could decrease it But as we have seen, more flexibility allows models to fit closer to the data, thus incorporating more of the noise. This makes them more variable across different samples, hence increasing their variance. This balancing of flexibility against susceptibility to noise is referred to as the Bias-Variance tradeoff and is one of the fundamental principles in statistical learning. 5.3.2.1 The Bias and Variance of a Regression Model Lets return to a simple example we saw right at the beginning of this section. We have the setup where \\[\\begin{align*} Y = 1 + 4X - 3X^2 - X^3 + 2X^4 + \\epsilon, \\end{align*}\\] where \\(X\\) is uniformly distributed on \\((0,1)\\) and \\(\\epsilon \\sim N(0, 1/25)\\). ### First we can set up what is constant: # The regression function, a degree 4 polynomial g &lt;- function(x) 1 + 4*x - 3*x^2 - x^3 + 2*x^4 # The standard deviation of the residuals (later we can try varying this to see what effect it has on estimation) sigma_resid &lt;- 1/5 # The sample size, which has a very substantial effect on the statistical properties of the regression estimator n &lt;- 30 ### We can now simulate some data with this set-up x &lt;- runif(n, min = 0, max = 1) y &lt;- g(x) + rnorm(n, sd = sigma_resid) ### Now let&#39;s plot the data, and add the true regression function plot(x, y, ylim = c(0.75, 3.25), xlim = c(0, 1)) curve(g, add = TRUE, from = -.1, to = 1.1) ### Now let&#39;s add the simple linear regression fit from the data model &lt;- lm(y~x, data = data.frame(x, y)) abline(model$coefficients, col = 2) Run the above code multiple times. You should not be surprised to see a different fitted model each time, since the sample changed. Although no single fitted model can show us what the bias and variance are, repeated experiments can give us some understanding of these. The simple linear model is, well simple, and so it doesnt have very high variance, and you probably noticed that even with a small-ish sample size (30) the models fit on different samples were not that different from one another. You will probably also have been able to see regions where the models tend to over/under estimate the true function, indicating potential bias. The following chunk of code will run this experiment 500 times, and add all of the lines fit to the different data sets generated. ### First we set the number of repeats of the experiment repeats &lt;- 500 ### We will be adding the lines from multiple fitted regression models ### and so first need to set up a plot on which to add these curve(g, from = -.1, to = 1.1, xlim = c(0, 1)) ### Now we repeat the experiment. We will also store the predictions over ### a grid of values for x so that we can calculate estimate the bias and ### variance x_grid &lt;- data.frame(x = seq(-.1, 1.1, by = .01)) predicted &lt;- matrix(, repeats, length(x_grid$x)) for(rep in 1:repeats){ # simulate data x &lt;- runif(n, min = 0, max = 1) y &lt;- g(x) + rnorm(n, sd = sigma_resid) # fit model model &lt;- lm(y~x, data = data.frame(x, y)) # store predictions predicted[rep,] &lt;- predict(model, x_grid) # add line to graph abline(model$coefficients, col = adjustcolor(&quot;red&quot;, alpha.f = .1)) } ### All of the lines will have obscured the original plot. We can add that back ### and also add the averaged model predictions curve(g, from = -.1, to = 1.1, add = TRUE) lines(x_grid$x, colMeans(predicted), lty = 2) Now we are able to see the bias more clearly, where the dashed line is the average fitted value for each value of \\(x\\) over all 500 fitted models. We can also see that that variance increases as we move away from the middle of the range of \\(x\\) values. We know that as we increase the flexibility (degree of the polynomial, where a linear/affine function is just a degree one polynomial) we should see the bias decreasing and the variance increasing. Lets run the same experiment but for degrees two up to five for comparison. par(mfrow = c(2, 2)) ### We can put the above code into a loop over the degree ### of the polynomial. We&#39;ve seen the linear (degree one) ### polynomial so let&#39;s look at degrees 2 - 5 for(d in 2:5){ curve(g, from = -.1, to = 1.1, xlim = c(0, 1), main = paste0(&quot;Degree &quot;, d, &quot; polynomial fits&quot;)) x_grid &lt;- data.frame(x = seq(-.1, 1.1, by = .01)) predicted &lt;- matrix(, repeats, length(x_grid$x)) for(rep in 1:repeats){ # simulate data x &lt;- runif(n, min = 0, max = 1) y &lt;- g(x) + rnorm(n, sd = sigma_resid) # fit model model &lt;- lm(y~poly(x, d), data = data.frame(x, y)) # store predictions predicted[rep,] &lt;- predict(model, x_grid) lines(x_grid$x, predicted[rep,], col = adjustcolor(&quot;red&quot;, alpha.f = .1)) } curve(g, from = -.1, to = 1.1, add = TRUE) lines(x_grid$x, colMeans(predicted), lty = 2) } We can see that the quadratic (degree two) model still has very obvious bias, in that the average fit shown with the dashed line over/underestimates the true function at different values of \\(x\\). In the extreme left and right we can also see slight bias in the degree three polynomial model. Although we can see very slight deviations in the average fit from the true function for degrees four and five, these are down to the fact that we have averaged 500 fitted models and not infinitely many. In fact if the true model is a degree \\(d\\) polynomial (where here we know \\(d = 4\\)) then the bias of all models with degree at least \\(d\\) is theoretically zero. The cost of choosing a higher degree polynomial is, however, that the variance increases, and the higher variation in the fitted models with degrees 4 and 5 should be very apparent in these experiments. 5.4 Estimating (Expected) Prediction Error It is important to note that the experiments we conducted in the previous subsection could only be performed because the true distribution of \\(Y\\) and \\(X\\) was known We knew the true regression function, \\(g^*\\) We knew the distribution of the residuals, and so could generate lots of samples and fit lots of regression models In reality we do not know \\(g^*\\) and only get one sample from which we have to do everything We need to find our estimates for \\(g^*\\) AND try to estimate what their (expected) prediction errors are (so that we can choose the one seen to be the best) By definition (expected) prediction error is based on the loss attained by our estimate(s)/estimator(s) for points separate from the training data 5.4.1 Validation for Estimating (Expected) Prediction Error We saw in our introduction to statistics that the bootstrap was a way of overcoming the fact that we have only a single sample from which to perform estimation and potentially estimate our uncertainty. A similar idea can be applied in the context of predictive modelling. Although it is possible to estimate the variance of a model directly using the bootstrap, estimating bias is typically not possible without making some assumptions. However, we dont necessarily need to estimate the bias and variance of a model if we can more directly estimate its expected prediction error, bypassing the need for estimating these two components of the risk altogether. The most principled approach for estimating the prediction error of a model is by actually testing it on cases (pairs of values for \\(X\\) and \\(Y\\)) which were not given to it for training Part of the sample which is set aside from training, to assess the prediction capabilities of different trained models, and which is used for model selection is called a validation set. For clarity, lets adopt the following notation. Letting \\(\\hg\\) be an estimator for \\(g^*\\), for a specific training set \\(T\\) we will write \\(\\hg^T\\) to be the resulting model fit after training on \\(T\\). Then, if we split our entire sample into training \\(T = \\{(y_1^T, \\x_1^T), ..., (y_{n_T}^T, \\x_{n_T}^T)\\}\\) and validation \\(V = \\{(y_1^V, \\x_1^V), ..., (y_{n_V}^V, \\x_{n_V}^V)\\}\\) sets then the validation error of \\(\\hg^T\\) is \\[ Val(\\hg^T) := \\frac{1}{n_V}\\sum_{i=1}^{n_V}L\\left(y_i^V, \\hg^T(\\x_i^V)\\right) \\] Hopefully it is clear that the training and validation sets must not overlap, and any overlap would be a direct form of data leakage, something which was mentioned in relation to exploratory data analysis. A few other points may also have come to mind. We are aware that, all other things being equal, models trained on larger samples tend to give more accurate predictions. \\(n\\) seldom affects bias substantially and increasing \\(n\\) will decrease variance What this means is that the model trained on the entire sample \\(\\hg^{T\\cup V}\\) is likely to be a better model than \\(\\hg^T\\). Although \\(Val(\\hg^T)\\) is an unbiased estimate for \\(PredictionError(\\hg^T)\\) it will typically overestimate \\(PredictionError(\\hg^{T\\cup V})\\). When it comes to actually producing a model for deployment, we would like to be able to use all of (or as much of) the data for training this final model as possible, to get the best results once it is deployed. A small validation set would mean that \\(Val(\\hg^T)\\) is only slightly biased as an estimate for \\(PredictionError(\\hg^{T\\cup V})\\), and so this particular issue may not be too problematic But a small validation set would mean that the variance of \\(Val(\\hg^T)\\) is large, and we may not want to base such an important decision as model selection on an unreliable estimate for performance. 5.4.1.1 Cross Validation To achieve both relatively low variance and relatively low bias (overestimation) when estimating prediction error, we could repeatedly split the data into training and validation sets, with each validation set being relatively small, and then average the resulting validation errors Since each validation set is small, we should not have too much bias The variance of the averaged validation errors is lower than that of a single validation error The issue with this is that the averaged validation error is not an estimate for the prediction error of any single model, but is rather is an estimate for the expected prediction error of \\(\\hg\\) (as an estimator) fit on a training set the size of \\(T\\). This is not necessarily a problem. Ultimately we can obtain better estimates for the expected prediction error of \\(\\hg\\) than of the actual prediction error of our particular realisation of \\(\\hg\\). Cross validation, arguably the most universally applied method for estimating model performance for model selection in predictive modelling, is a very systematic approach to this repeated validation idea: simply put, every point in the sample is used as a validation point (i.e. within one of the validation sets) exactly once. To be precise, cross validation works as follows: Split the sample into \\(K\\) subsets (called validation folds), of roughly equal size. For \\(k = 1, ..., K\\) let the \\(k\\)-th fold be \\(V_k = \\{(y_1^{V_k}, \\x_1^{V_k}), ..., (y_{n_k}^{V_k}, \\x_{n_k}^{V_k})\\}\\) and let \\(T_k\\) be all the points except those in fold \\(k\\). For each \\(k = 1, ..., K\\): fit the model to all except the \\(k\\)-th fold to obtain \\(\\hat g^{T_k}\\) estimate the prediction error of \\(\\hat g^{T_k}\\) using the validation error from fold \\(k\\), \\[ Val(\\hat g^{T_k}) = \\frac{1}{n_k}\\sum_{i=1}^{n_k}L(y_i^{V_k}, \\hat g^{T_k}(\\x_i^{V_k})) \\] Average these to obtain the cross-validation based estimate for expected prediction error of \\(\\hat g\\) \\[ \\widehat{E[PredictionError(\\hat g)]}_{CV} = \\frac{1}{K}\\sum_{k=1}^K Val(\\hat g^{T_k}) \\] Considerations and Limitations Cross validation (CV) is popular for its universality and simplicity, but has some limitations: It is an estimate of expected prediction error, and not prediction error It is a biased estimate of the expected prediction error of \\(\\hg\\) fit using the entire training sample, since each training set used in CV has size (approximately) \\(n\\frac{K-1}{K} &lt; n\\) It depends on the specific splitting of the sample into folds; if the folds had been split differently the output would be different There are two sources of randomness: the randomness in the drawing of our sample from the population (the regular randomness in statistics) and also the randomness in how we split this sample into folds The Effect of Varying \\(K\\) As we can see above, the size of each training set used in cross validation is \\(n\\frac{K-1}{K}\\), which is increasing in \\(K\\). As a result, the larger the number of folds the less is the bias in the cross validation based estimate of expected prediction error of the model trained using the entire sample (size \\(n\\)). The interaction of \\(K\\) with the variance has some subtlety to it: For large \\(K\\) each validation set is small, and so each of the \\(K\\) estimates of prediction error has high variance. However, the final estimate of prediction error is an average of all \\(K\\), and typically when we average a larger number of random variables there is a greater reduction in variance. The main factor which leads the variance of the cross validation estimate to increase with \\(K\\) is the fact that the training sets have large overlap and hence the prediction errors of each of the resulting models are highly correlated. When averaging positively correlated random variables there is a lesser reduction in variance compared with averaging independent (or uncorrelated) ones. There is also the factor of computation, since \\(K\\) separate models need to be trained to obtain an estimate for the expected prediction error. Although there are some special cases where specifically the \\(n\\)-fold (or Leave-One-Out) cross validation estimate can be obtained efficiently. It is also worth considering the relationship between \\(n\\) and \\(K\\). Although it will depend on the particulars of the model being used, as a general rule of thumb if \\(n\\) is smaller it may be preferable to choose a larger value for \\(K\\). This is both because when \\(n\\) is smaller, decreasing it by a fixed proportion will often affect the expected prediction error a greater amount than when \\(n\\) is larger, and also because fitting each model is computationally less demanding and so more total models can be fit in a relatively small amount of time. All things considered, for most problems it has become common to choose \\(K\\) equal to either five or ten, and these give a reasonable tradeoff of bias and variance, as well as not being problematic computationally except when fitting each model is already computationally burdensome. 5.4.1.2 Cross Validation in R One of the benefits of Cross Validation is its simplicity. Nonetheless, whenever existing implementations (which are popular, and so any bugs will almost surely have been identified) are available it is beneficial to leverage this convenience. The caret package (Classification And REgresstion Training) is an extremely popular and versatile general purpose package, which includes links to a very large number of implementations of predictive models. For a list of all the models included in caret see the package documentation at https://topepo.github.io/caret/available-models.html. For those models included in the package, performing cross validation for multiple different versions of the models (e.g. the versions of a simple polynomial regression model could be associated with different polynomial degrees) can be done with a single call to the function train. However as simple polynomial regression is not one of the models included we have to run cross validation separately for each degree (or, as we will cover a little later on, we can create our own method for caret to operate on). When we think of exploring different versions of a model, like different degrees of a polynomial regression model, we often refer to this as model tuning, and the variables (or parameters) which determine the different versions are called tuning parameters or sometimes hyperparameters. Model tuning is also a model selection task, but instead of choosing from among multiple model types we are choosing from different versions of the same model type. Now, ensuring you have the package installed, run the following code chunk. It will run cross validation for estimating the expected prediction error of simple polynomial models on the cars data set we explored previously. ### First we load the package library(caret) ### the function train() is the main workhorse of the package ### it takes formula and data arguments, just as we saw with the ### lm() function. Also, since it offers broad functionality we ### to specify what method (type of model) we want to be using. ### Finally we need to provide an argument called trControl which ### tells train() what exactly we want to do, i.e., just fit one ### model on all the data, or do CV (or something else like the ### bootstrap) for model selection. The trControl object is ### produced by the function trainControl. For details on this ### have a look at help(trainControl), remembering to ensure the ### caret package is loaded. ### let&#39;s start by setting up the trControl object. We want to do ### cross validation, and let&#39;s choose 10 folds trControl &lt;- trainControl(method = &quot;cv&quot;, number = 10) ### For the models linked directly by caret we can perform CV ### across multiple &quot;versions&quot; of the model using a single call. ### However simple polynomial regression is not linked directly ### and so we cannot directly use a single call to run CV for ### all polynomial degrees, and will have to do these in a loop CV_results &lt;- list() for(d in 1:5){ set.seed(12345) form &lt;- as.formula(paste0(&quot;dist~poly(speed,&quot;, d, &quot;)&quot;)) CV_results[[d]] &lt;- train(form, method = &quot;lm&quot;, data = cars, trControl = trControl) } Setting the seed before each call to the function train was important here so that the same CV folds were used each time. In the output of train is a list with a field results which contains the performance statistics from the cross validation. The result RMSE (Root Mean Squared Error) is the square root of the estimate of expected prediction error based on the squared error loss function. CV_results[[1]]$results$RMSE ## [1] 15.11234 If we wish to choose the model which gave the lowest expected prediction error, we can inspect them all unlist(lapply(CV_results, function(l) l$results$RMSE)) ## [1] 15.11234 14.91259 14.93013 14.83372 15.93608 We can see that the degree four polynomial gave the smallest estimated expected prediction error. However, the differences when compared with the lower degree models are relatively small. It is often prudent to opt for a simpler model if the estimate of expected prediction error is similar to that which attained the lowest estimate. This is especially true if the sample is relatively small, and hence the estimate for expected prediction error may be highly variable. Try changing the seed and re-running the above code. A Better Way: Create Your Own Models For Use In caret Instead of looping over the different values of d, the degree of the polynomial, we can have caret do all that work for us. It will also output the results in a far more digestible way than if we just populate a list of outputs as we did above. It will also go ahead and do the model selection for us. The caret package allows us to link to our own models, so that we can use the train function to apply all its wonderful methodology. Instead of setting the method argument to the name of a method/model which caret is already linked to, we can provide a list containing all the objects it needs in order to offer the same functionality. The following video gives a brief demo for how we could implement simple polynomial regression, and select of the tuning parameter d (the degree of the polynomial) automatically. In addition, the script which I go through in the video is available at &lt;https://modules.lancaster.ac.uk/mod/folder/view.php?id=2771508&gt;. Video For those who are interested, some far more advanced examples can be seen in the caret documentation at https://topepo.github.io/caret/using-your-own-model-in-train.html. 5.4.2 Covariance Based Estimates of In-Sample Error An alternative to cross validation for estimating the expected prediction error of a model is that of estimating what is known as in-sample error, and expected in-sample error. The in-sample error of a model is very closely related to its prediction error, except that rather than looking at all possible pairs of \\(X\\) and \\(Y\\) in the population it uses the same \\(X\\) values as in the sample but pairs these with other possible values of \\(Y\\). Specifically, suppose that we have a sample as always, equal to \\((y_1, \\x_1), ..., (y_n, \\x_n)\\). Then the in-sample error of a fitted model \\(\\hg\\) is the average error it would achieve for the same values of \\(\\x_i; i = 1, ..., n\\) but over new potential values of response, say \\(\\tilde Y_i; i =1, ..., n\\) \\[ E_{\\tilde Y_1, ..., \\tilde Y_n}\\left[\\frac{1}{n}\\sum_{i=1}^n L(\\tilde Y_i, \\hg(\\x_i)) \\right]. \\] Recall that in the predictive modelling context we are interested in the conditional distribution(s) of \\(Y|X\\), and in the above the \\(\\tilde Y_i\\)s are paired with the observations of \\(X\\) in the sample, i.e. \\(\\tilde Y_i\\) is from the conditional distribution of \\(Y|X = \\x_i\\). As it is with prediction error, however, estimating in-sample error is not straightforward and we need to rely on estimates for expected in-sample error. This is just the expected value of in-sample error over potential training sets (and hence different fitted models). It turns out that for the squared error loss function we can obtain an unbiased estimate for expected in-sample error from the quantity \\[ \\frac{1}{n}\\sum_{i=1}^n L(y_i, \\hg(\\x_i)) + \\frac{2}{n}\\sum_{i=1}^n Cov(Y_i, \\hg(\\x_i)). \\] The first term above is just the training error. The second term above may be seen as a penalty to avoid selecting models likely to overfit. Models with too much flexibility will fit very closely to the sample, meaning that the fitted values would follow the values of the response if we instead had a different set of realisations (a different sample). This leads to a high covariance between each \\(Y_i\\) and its corresponding fitted value, \\(\\hg(\\x_i)\\). The more flexibility, the more able the model will be able to follow the variations in the data, leading to higher covariance between the actual responses and the fitted values from the model. This doesnt yet, however, tell us about how to compute or estimate these covariances. It turns out, however, that for some models we can obtain analytical expressions for the covariance between the response and fitted values. For example in a simplified setting (like the linear and quadratic models we have seen) the quantity \\(\\sum_{i=1}^n Cov(Y_i \\hat Y_i)\\) is equal to \\(\\sigma^2_{\\epsilon}\\) multiplied by the number of parameters in the model (the model degrees of freedom). As long as we can obtain a reasonable estimate for \\(\\sigma^2_{\\epsilon}\\) we can use this to obtain estimates of the expected in-sample error and use this as an alternative to something like cross validation to select/tune a model. As a rule-of-thumb estimating \\(\\sigma^2_{\\epsilon}\\) with the quantity \\(\\frac{1}{n - df(\\hg^*)}\\sum_{i=1}^n(y_i - \\hg^*(\\x_i))^2\\), where \\(\\hg^*\\) is the most (or one of the most) flexible models which has been fit and \\(df(\\hg^*)\\) is its degrees of freedom. Lets apply this to the cars data set to see how it compares with cross validation ### Let&#39;s create a vector to store the training errors of ### the polynomial models for d = 1, 2, ..., 5 tr_err &lt;- numeric(5) ### Now we can loop of the the degree of the polynomial ### as before for(d in 1:5){ form &lt;- as.formula(paste0(&quot;dist~poly(speed,&quot;, d, &quot;)&quot;)) mod &lt;- lm(form, data = cars) tr_err[d] &lt;- mean((mod$fitted.values - cars$dist)^2) } ### Now we can use the training error from the most flexible ### to estimate the residual variance n &lt;- nrow(cars) sig2_hat &lt;- tr_err[5]/(n - 6)*n # Can you see why this is the appropriate estimate? ### Now let&#39;s estimate the expected in-sample error ### for each model eise &lt;- tr_err + 2*sig2_hat*(2:6)/n eise ## [1] 245.7308 244.4849 250.0081 252.6074 261.2458 ### When we did cross validation the caret package ### returned the square root of the estimated expected ### prediction error unlist(lapply(CV_results, function(l) l$results$RMSE))^2 ## [1] 228.3829 222.3852 222.9089 220.0393 253.9585 We can see that the estimates for expected in-sample error and the cross validation based estimates of expected prediction error are quite similar. 5.4.3 Test Sets Before we continue it is important to touch on an important but subtle point. Using cross validation in order to select a model, based on its estimated expected prediction error, is very well founded. However, if we need both to select a model and to estimate its prediction or expected prediction error we face a little bit of a problem. The reason that training error underestimates prediction error is because the function is chosen based on how well it predicts the training set (its training error) and not based on how well it predicts the entire distribution. Now lets think about the validation error of the selected model in a similar light. We took a set of models, i.e. those models trained on the training split, and chose the one which had the lowest validation error. But this can be seen as another example of training. Instead of selecting from a class of functions \\(\\F\\) based on the error on the training split (maybe with some penalisation) we selected from a much smaller set of models (the models trained on the training split) and just replaced the error on the training split with that on the validation split. But the same source of bias exists; we chose a model because it fit well on a sample of observations (now the validation split) and not necessarily on the whole population. Unfortunately cross validation (as opposed to a single validation split) has the same source of bias, but just to a slightly lesser extent. Now, this does not mean that using validation or cross validation to perform model selection is a bad idea. It only means that we cannot use the validation or cross validation based estimate of the (expected) prediction error as a true reflection of the prediction error of the selected model. If we need perform training AND model selection AND have a reliable estimate of the prediction performance of the selected model we need to first, before any training and validation, separate some of the data as a test set which cannot be touched until we have done all of our training, validation, etc., and is only used as a final step to estimate the prediction error of our final selected model. When both a final model and an estimate for its prediction error is needed then the typical workflow is: Split the entire sample in a training + selection set and a test set. Use the training + selection set in order to both select and train the final model(s) we want for deployment This could be be using cross validation, a single validation split or the expected in sample error based approach above Estimate the prediction error of the selected and trained model(s) by calculating its/their error on the test set. It is important that the test set is kept completely separate from the training + selection set from the very beginning to avoid data leakage. Data leakage is when extra information which would not be available in practice is used in making modelling decisions. What we mean by in practice when we actually take the model(s) we have selected and trained and deploy them for use on new cases. Since the training/validation/testing splitting of the data is ultimately to try and represent the fact that when we actually deploy a model for use on data outside our sample it needs to be fully ready to go long before we see any of those true test cases. Perhaps more importantly, we typically will never know the values of the response variable for the cases on which we need the model to actually make predictions after deployment. This means that any information which relates even indirectly to the response variables in the test set, which is used for any part of the modelling/fitting/selecting tasks is a violation of this representation. Similarly, any information about the response variables in a validation set which is used in training will introduce additional bias to the validation error. 5.5 Summary Overfitting is the term used to describe how overly flexible models may fit the data too well, modelling not only the trend in the relationships between \\(X\\) and \\(Y\\) (the signal), but also the unpredictable noise component \\(\\epsilon\\) Models which have high flexibility have high variance, and typically low bias. The variance and bias combine to define the risk Although we cannot know necessarily when a model is overfitting, we can select an appropriate model (i.e. one which will likely generalise well) by estimating the expected prediction error of a number of models and choose the lowest If we care also about interpretability then we may wish to balance accuracy and interpretability Cross-Validation is a principled and universal approach for estimating expected prediction error. The caret package provides a unified framework for training and tuning models, either by existing links to a very large number of packages and implementations or with the use of our own model implementations. If we need to have an estimate for the prediction error of our final selected model(s) then we need to first set aside a test set, which we dont look at at all until after all modelling, training and selecting has been done. Once the final selected and fitted model(s) are ready for deployment then we can use the test set to estimate prediction error. 5.6 Exercises Refer to the experiments in Chapter 5.3.2.1. Modify these experiments to estimate the quantities \\(E_X[Bias(\\hg(X))^2]\\) and \\(E_X[Var(\\hg(X))]\\) for \\(\\hg\\) being a polynomial regression model with degree set to each of 1, 2, , 6. To do this create a grid of \\(X\\) values from \\(0\\) to \\(1\\) with spacing of 0.01. Then repeatedly generate samples (1000 in total) of \\(X\\) and \\(Y\\), each of size \\(n = 30\\), and fit your polynomial models to each sample before obtaining their predictions for each value in the grid. From these you can estimate the bias and variance at each grid point, and use these to obtain the averages over \\(X\\) (note that we can only use a simple average over the grid points because \\(X\\) is uniformly distributed). Plot the estimates of \\(E_X[Bias(\\hg(X))^2]\\) and \\(E_X[Var(\\hg(X))]\\) as well as their sum, on the same axes, as a function of the degree of the polynomial. Repeat a. and b. for \\(n = 200\\). What do you observe? Refer to the Auto data set in the package ISLR2. Below you will fit and select models for predicting mpg. Start by setting aside \\(30\\%\\) of the data as a test set. You can use the function createDataPartition() from the caret package. By visualising the relationships between mpg and each other numeric variable, choose one to act as a predictor for mpg. Perform five fold cross-validation to estimate the expected prediction errors of polynomial regression models for degrees from 1 to 5. Choose a model from these and assess its performance by computing the average squared error on the test set. Use five fold cross validation to estimate the expected prediction errors of polynomial models of degrees from 1 to 5 applied to each of the possible numeric predictors. Use the estimates of their performance to select a model and use this to assess its predictive performance using the test set. You can do this using either a loop (or nested loop) or, if youd like a challenge, by defining your own model for caret to use which has two parameters (polynomial degree and an index for which variable to use as a predictor). Use estimates arising from expected in-sample error to select the model with the best pair of predictor variable and polynomial degree and compare its performance to that selected in b. "],["linear.html", "6 Linear Regression: Ordinary Least Squares and Regularised Variants 6.1 The Linear Model 6.2 Ordinary Least Squares 6.3 Regularisation for Linear Models 6.4 Summary 6.5 Exercises", " 6 Linear Regression: Ordinary Least Squares and Regularised Variants \\[ \\def\\x{\\mathbf{x}} \\def\\Rr{\\mathbb{R}} \\newcommand{\\argmin}{\\mathop{\\rm argmin}} \\newcommand{\\argmax}{\\mathop{\\rm argmax}} \\def\\F{\\mathcal{F}} \\def\\hbbeta{\\hat{\\boldsymbol{\\beta}}} \\def\\bbeta{\\boldsymbol{\\beta}} \\def\\X{\\mathbf{X}} \\def\\y{\\mathbf{y}} \\def\\hg{\\hat g} \\def\\hbeta{\\hat \\beta} \\def\\I{\\mathbf{I}} \\def\\hbr{\\hbbeta^{(ridge)}} \\def\\hbl{\\hbbeta^{(LASSO)}} \\def\\hbe{\\hbbeta^{e-net}} \\] In this section we will study the (multiple) linear regression model in greater detail, and will formally encounter a number of popular approaches for regularising estimation of the optimal linear model. Despite their simplicity, especially in comparison with modern hyper flexible models, linear models remain extremely important for a number of reasons, not least of all their interpretability. 6.1 The Linear Model As we discussed briefly in Chapter 4, a linear (or more technically affine) function of covariates \\(X_1, X_2, ..., X_p\\) is expressible as \\[ g(X) = \\beta_0 + \\sum_{j=1}^p \\beta_j X_j, \\] where the vector of coefficients \\(\\bbeta = (\\beta_0, \\beta_1, ..., \\beta_p)\\) fully parameterises the function. The description of a linear function conveys a lot of information about the behaviour of the function for changes in the values of the \\(X\\)s. In particular, a one unit change in the value of \\(X_j\\) results in a change in \\(g(X)\\) by an amount \\(\\beta_j\\). In addition the signs (positive or negative) of the coefficients describe the nature of the relationships between the function value and the variables, or more relevantly to our context, relationships between the covariates and the typical values of the response. 6.1.1 A Quick Aside on Assumptions You will see many statistical texts saying that the linear model assumes the relationship between the (mean) response and the covariates is linear. Perhaps more correctly, regardless of the true form of the relationship between the response and covariates we may choose to model the (mean) response as a linear function of the covariates. Why might we wish to do this? The fact of the matter is that, even if the actual relationship between \\(Y\\) and \\(X\\) is not linear, it may very well be prudent to model it as though it is. By doing so we give ourselves the ability to understand the effects of changes in \\(X\\) on the value of the prediction for \\(Y\\) Hopefully it is clear that this is not the same as the effects of changes in \\(X\\) on the actual value of \\(Y\\), or even of the actual value of the expected value of \\(Y\\). That is, how we choose to model the situation doesnt change reality, but a simplified representation of reality which we understand may be more useful than a more accurate representation which we dont. Moreover, if the sample size is not very large then we may not have enough information to be able to accurately enough describe the true relationship between \\(Y\\) and \\(X\\), and in fact the accuracy of a linear model may be better than that of a model with the correct form simply because trying to estimate the right parameters for the correct model would result in very high variance. We saw this in essence (i.e. a model simpler than the true model being preferable) in one of the examples from Chapter 5, where we fit polynomial models to a simple example where we knew the true function was a degree 4 polynomial, but found that only when the sample became quite large did the accuracy of the fitted degree 4 model exceed that of the fitted degree 3 model. Where the assumptions actually do matter, however, is in whether the theoretical results associated with linear models can be applied without error. We will describe some of these theoretical results, as we go forward. 6.2 Ordinary Least Squares The Ordinary Least Squares (OLS) linear model is the direct extension of the simple linear regression model we saw before to the situation where we have multiple covariates More precisely the simple linear regression model we saw before was the OLS model for the particular context where we only have one covariate. Although we have already seen this model briefly, here we will look in more depth. Within the framework we have been considering, when performing training/fitting/estimation, our collection of functions \\(\\F\\) from which to select our fitted model is the collection of all linear (affine) functions. This means that each \\(g \\in \\F\\) can be written as \\[ g(\\x) = \\beta_0 + \\sum_{j=1}^p \\beta_j x_j, \\] for some coefficients \\(\\bbeta \\in \\Rr^{p+1}\\), and our loss function is the squared error loss. As a result we may describe the fitted OLS model as \\[\\begin{align*} \\hg(\\x) =&amp; \\ \\hbeta_0 + \\sum_{j=1}^p \\hbeta_j x_j, \\mbox{ where }\\\\ \\hbbeta =&amp; \\argmin_{\\bbeta \\in \\Rr^{p+1}} \\frac{1}{n}\\sum_{i=1}^n \\left(y_i - \\hbeta_0 - \\sum_{j=1}^p \\hbeta_j x_{ij}\\right)^2. \\end{align*}\\] 6.2.1 Some of the Theory for OLS Partly because of its simplicity the theoretical properties of the OLS model have been studied extensively for many decades. We will barely scratch the surface of what is known about the model, but will cover some important practical aspects. Suppose that the true regression equation is given by \\[ Y = \\beta_0^* + \\sum_{j=1}^p \\beta_j^* X_j + \\epsilon, \\]for some true regression coefficients \\(\\bbeta^*\\), and where \\(\\epsilon \\sim N(0,\\sigma_{\\epsilon}^2)\\). Then conditional on the observations of the covariates (i.e. by treating the observations of \\(X\\) as fixed) we have \\[ \\hbbeta \\sim N\\left(\\bbeta^*, \\sigma^2_{\\epsilon}\\left(\\X^\\top \\X\\right)^{-1}\\right), \\] where \\(\\X\\) is again the design matrix with \\(i\\)-th row \\((1, \\x_i^\\top)\\). That is, \\(\\hbbeta\\) as an estimator has a multivariate normal distribution and \\(E[\\hbbeta] = \\bbeta^*\\), i.e. \\(\\hbbeta\\) is an unbiased estimator of the true coefficients \\(Cov(\\hbbeta) = \\sigma^2_{\\epsilon}\\left(\\X^\\top\\X\\right)^{-1}\\), i.e. the variance of \\(\\hbbeta\\) depends on the variance of the residuals (this should not be surprising; the more noise the harder it is to estimate the signal) and also depends on how spread out and also how correlated the observations of \\(X\\) are It is easy to show that the prediction for the mean of the observations of \\(X\\) is equal to the mean of the observations of \\(Y\\). Loosely speaking we can think of the point \\((\\bar \\x, \\bar y)\\) as a pivot for the fitted function, and in order to fit closely to the observations it is, in a sense, held in place by them. To ensure it doesnt wobble too much on its pivot, we need anchors (observed values for \\(X\\)) which are quite spread out. The following R code will create two scenarios; one with the values of \\(X\\) relatively closer together and the other with them more spread out. Everything else is exactly the same. Run the code multiple times. You should see more variability in the first scenario than the second. ### Scenario 1: X ranges from -0.5 to 0.5 x1 &lt;- seq(-0.5, 0.5, length = 20) ### Scenario2: X ranges from -1 to 1 x2 &lt;- seq(-1, 1, length = 20) ### Residuals: We can even use exactly the same values of the residuals epsilon &lt;- rnorm(20, sd = 0.5) ### Y: We now produce the different observations for Y y1 &lt;- x1 + epsilon y2 &lt;- x2 + epsilon ### Fitted models: Finally we fit the two linear models and plot lm1 &lt;- lm(y1~., data.frame(x1, y1)) lm2 &lt;- lm(y2~., data.frame(x2, y2)) par(mfrow = c(1, 2)) plot(x1, y1, xlim = c(-1, 1), ylim = c(-1.5, 1.5), xlab = &#39;X&#39;, ylab = &#39;Y&#39;, main = &#39;X values close together&#39;) abline(lm1$coefficients) abline(0, 1, col = 2) plot(x2, y2, xlim = c(-1, 1), ylim = c(-1.5, 1.5), xlab = &#39;X&#39;, ylab = &#39;Y&#39;, main = &#39;X values spread out&#39;) abline(lm2$coefficients) abline(0, 1, col = 2) Returning to the distribution of \\(\\hbbeta\\), if we look at each of the coefficients separately we have that \\((\\hat \\beta_j - \\beta^*_j)\\big/\\sigma_{\\epsilon}\\sqrt{(\\X^\\top \\X)^{-1}_{jj}}\\) has a standard normal distribution. But in practice we do not know the value of \\(\\sigma_{\\epsilon}^2\\) and need to estimate it. We will not go into any of the details for why, as these are beyond the scope of the module, but if we estimate \\(\\sigma_{\\epsilon}^2\\) using \\(\\frac{1}{n-p-1}\\sum_{i=1}^n r_i^2\\), where \\(r_1, ..., r_n\\) are the residuals from the fitted model, then \\((\\hat \\beta_j - \\beta^*_j)\\big/\\hat\\sigma_{\\epsilon}\\sqrt{(\\X^\\top \\X)^{-1}_{jj}}\\) has a \\(t\\)-distribution with \\(n-p-1\\) degrees of freedom. This becomes important when we want to make inference about the true values of the coefficients since it allows us to obtain confidence intervals for the regression coefficients. Specifically we can obtain a \\((1-\\alpha)\\times 100\\%\\) confidence interval using \\[ \\left(\\hat \\beta_j + t_{\\alpha/2}\\hat\\sigma_{\\epsilon}\\sqrt{(\\X^\\top \\X)^{-1}_{jj}}, \\hat \\beta_j + t_{1-\\alpha/2}\\hat\\sigma_{\\epsilon}\\sqrt{(\\X^\\top \\X)^{-1}_{jj}}\\right), \\] where \\(t_{q}\\) is the \\(q\\) quantile of the \\(t\\)-distribution with \\(n-p-1\\) degrees of freedom. Hopefully it is clear that this is only a valid confidence interval if the modelling assumptions hold. 6.2.2 Ordinary Least Squares in R We already saw that the lm function allowed us to fit linear models, and in fact it does so based on the squared error loss function and hence the coefficients we get out are the \\(\\hbbeta\\) above. The output of lm contains much more than just the regression coefficients, however, and here we will briefly discuss some of these by way of an example. The Auto data set in the ISLR2 package contains information on 392 cars developed between 1970 and 1982. We may be interested in the relationship between its fuel efficiency (miles per gallon, mpg) and various other characteristics. ### Load the ISLR2 library and then the data set library(ISLR2) data(Auto) ### Inspect the data set variables head(Auto) ## mpg cylinders displacement horsepower weight acceleration year origin ## 1 18 8 307 130 3504 12.0 70 1 ## 2 15 8 350 165 3693 11.5 70 1 ## 3 18 8 318 150 3436 11.0 70 1 ## 4 16 8 304 150 3433 12.0 70 1 ## 5 17 8 302 140 3449 10.5 70 1 ## 6 15 8 429 198 4341 10.0 70 1 ## name ## 1 chevrolet chevelle malibu ## 2 buick skylark 320 ## 3 plymouth satellite ## 4 amc rebel sst ## 5 ford torino ## 6 ford galaxie 500 ### We see the response variable mpg, and eight other variables ### which could be used as predictors. Most of these appear sensibly ### treated as numeric, and potentially useful predictors. However ### the car name should not be predictive (although the make of car ### could be, and as an extension we could create a factor variable ### which groups the cars by make). In addition the origin variable ### is curious. If you call help(Auto) you will see this is a numeric ### encoding of the region in which the car was developed. We should ### certainly treat this as a factor variable. Auto$origin &lt;- factor(Auto$origin, levels = c(1, 2, 3), labels = c(&quot;American&quot;, &quot;European&quot;, &quot;Japanese&quot;)) ### We can now fit a model but exclude the name covariate lin_mod &lt;- lm(mpg~., data = Auto[,names(Auto)!=&quot;name&quot;]) ### The summary function will provide much of the information ### we may wish to inspect for understanding the properties ### of the model summary(lin_mod) ## ## Call: ## lm(formula = mpg ~ ., data = Auto[, names(Auto) != &quot;name&quot;]) ## ## Residuals: ## Min 1Q Median 3Q Max ## -9.0095 -2.0785 -0.0982 1.9856 13.3608 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -1.795e+01 4.677e+00 -3.839 0.000145 *** ## cylinders -4.897e-01 3.212e-01 -1.524 0.128215 ## displacement 2.398e-02 7.653e-03 3.133 0.001863 ** ## horsepower -1.818e-02 1.371e-02 -1.326 0.185488 ## weight -6.710e-03 6.551e-04 -10.243 &lt; 2e-16 *** ## acceleration 7.910e-02 9.822e-02 0.805 0.421101 ## year 7.770e-01 5.178e-02 15.005 &lt; 2e-16 *** ## originEuropean 2.630e+00 5.664e-01 4.643 4.72e-06 *** ## originJapanese 2.853e+00 5.527e-01 5.162 3.93e-07 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 3.307 on 383 degrees of freedom ## Multiple R-squared: 0.8242, Adjusted R-squared: 0.8205 ## F-statistic: 224.5 on 8 and 383 DF, p-value: &lt; 2.2e-16 In addition to the estimated coefficients, we see a number of other columns in table output from summary. These include Std. Error (the estimated standard error of the coefficients), and two additional columns named t value and Pr(&gt;|t|). We saw previously that the estimated standard errors are the quantities \\(\\hat \\sigma_{\\epsilon}\\sqrt{(\\X^\\top\\X)^{-1}_{jj}}\\). The other two columns are associated with statistical hypothesis tests for whether or not the true coefficients are zero or not. The reason we are interested in this is that if a coefficient is zero then the associated covariate does not have any effect on the model. Now, since we know (if the assumptions hold) that our regression estimates are realisations of continuous random variables, the probability that we see \\(\\hat \\beta_j\\) exactly equal to zero is zero, regardless of whether or not the true coefficient is zero or not. So we need to ask ourselves is our observed value of \\(\\hat \\beta_j\\) sufficiently far from zero to conclude that the true value is not zero?. Although we can never be absolutely certain, we may be able to quantify how unlikely it is to see an estimated coefficient \\(\\hat \\beta_j\\) like ours if \\(\\beta_j^*\\) is actually equal to zero. If it is very unlikely, then we could say with reasonable confidence that the true value is not zero. So how could we achieve such a thing? Well, we could look to the sampling distribution of \\(\\hat \\beta_j\\). We already know that if our modelling assumptions hold then \\[ \\frac{\\hat\\beta_j - \\beta_j^*}{\\hat \\sigma_{\\epsilon}\\sqrt{(\\X^\\top\\X)^{-1}_{jj}}} \\sim t_{n-p-1}. \\] And herein lies a beautiful thing about hypothesis tests. The above expression depends on \\(\\beta_j^*\\), which we dont know. But because we are only interested in this sampling distribution under the specific scenario where \\(\\beta_j^*\\) is equal to zero, that doesnt matter. That is, if \\(\\beta^*_j = 0\\) then \\(\\hat \\beta_j\\big/\\hat \\sigma_{\\epsilon}\\sqrt{(\\X^\\top\\X)^{-1}_{jj}}\\) has a \\(t\\) distribution with \\(n-p-1\\) degrees of freedom. We can therefore evaluate the probability \\[\\begin{align*} P\\Bigg(&amp;\\mbox{I could have seen a sample with } \\frac{\\hat \\beta_j}{\\hat \\sigma_{\\epsilon}\\sqrt{(\\X^\\top\\X)^{-1}_{jj}}} \\mbox{ further from zero}\\\\ &amp; \\mbox{than the one from my specific sample} \\Bigg | \\beta_j^* = 0\\Bigg)\\\\ &amp;= P\\left(T &gt; \\left|t_{obs}\\right|\\right), \\end{align*}\\] where \\(t_{obs}\\) is the observed value of \\(\\hat \\beta_j\\big/\\hat \\sigma_{\\epsilon}\\sqrt{(\\X^\\top\\X)^{-1}_{jj}}\\) from my sample and \\(T\\) is a random variable with a \\(t_{n-p-1}\\) distribution. These probabilities, or \\(p\\)-values, are the quantities reported in the Pr(&gt;|t|) column in the linear model summary, and the statistics \\(t_{obs}\\) are reported in the column headed t value. The summary also indicates the significance level of these \\(p\\)-values where *** signifies a negligible probability (i.e. it is extremely unlikely that \\(\\beta_j^*\\) is zero, since if it was we would almost never see an observed coefficient so far from zero); ** signifies that less than \\(1\\%\\) of samples would lead to an observation at least as far from zero as our own; * signifies less than \\(5\\%\\) of samples; and . signifies less than \\(10\\%\\) of samples with an estimated coefficient as far from zero. Looking at the above output we see that only cylinders (the number of cylinders in the engine); horsepower (a measure of the total engine power output); and acceleration (the time, in seconds, to accelerate from standstill to \\(60mph\\)) do not have any of these significance codes. This may well be surprising, since we certainly associate larger, more powerful engines with worse fuel efficiency. We will park that for now, and return to it a little later. Now, note that These \\(p\\)-values are only appropriate reflections of significance if the modelling assumptions hold. Hypothesis tests are notoriously hard to grasp when first encountered, and so if you found the above (very brief) overview of what is going on a little overwhelming, you are not alone. Another, possibly far more accessible approach for achieving essentially the same practicalities is to decide for yourselves what constitutes a significant finding from the point of view of what we referred to previously as a level of confidence in your conclusions. You can then construct an associated confidence interval based on this confidence level, and check whether zero lies within it. If not, then you should conclude that zero is not a plausible value for the coefficient. \\(R^2\\) and the F-statistic In addition to the tabular summary from the linear model is a number of comments in the footnotes. The \\(R^2\\) (R-squared) statistic quantifies the proportion of the variation in the responses which are captured by the model. This is a standardised measure of how well the model fits the data, since if we can explain the vast majority of the differences (variations) in the responses through the model, then it must be fitting well to the data. Formally it is defined as the ratio \\(\\sum_{i=1}^n r_i^2/\\sum_{i=1}^n (y_i - \\bar y)^2\\), where again \\(r_1, ..., r_n\\) are the residuals from the model. The adjusted \\(R^2\\) (Adjusted R-squared) takes into account the amount of flexibility in the model, where the more coefficients we have the better we will be able to fit to the data, potentially leading to overfitting. Adjusted \\(R^2\\) is therefore a more appropriate statistic from an inference point of view, whereas raw \\(R^2\\) is more of a descriptive statistic. It is very much possible, especially when there is a reasonably large number of covariates, that none of the individual variables contributes substantially to the prediction of the response, and that none is significant in the model (based on the \\(p\\)-value meaning of significance). We may in such situations question whether the model is actually capturing any relationship at all, or if the fit is just picking up on the noise. We can always look at the adjusted \\(R^2\\) for an indication of whether the model captures an appreciable amount of the variation in the responses, however any general threshold above which we decide the model is picking up on some relationship between the covariates and the response would be fairly arbitrary. But as with all other statistics, we could turn to the sampling distribution of the \\(R^2\\) and/or adjusted \\(R^2\\) to see if it is significantly above zero. As it turns out, if the modelling assumptions hold, then the quantity \\(\\left(1/(1 - R^2) - 1\\right)\\frac{n-p-1}{p}\\) has what is known as an \\(F\\) distribution, with \\(p\\) numerator degrees of freedom and \\(n-p-1\\) denominator degrees of freedom. This statistic is reported as the F-statistic in the linear model summary, and its significance can be interpreted as indicating that the covariates overall (i.e. combined) are relevant to the prediction of the response, or that the model is not just capturing noise. 6.2.3 (Some) Regression Diagnostics As we have hinted at, reliance on the confidence intervals and \\(p\\)-values should be done with caution due to their validity relying on the modelling assumptions that \\(Y\\) is expressible as a linear function of the covariates plus a normally distributed residual. There are very many diagnostics which can be considered which can, to some extent, test the validity of these assumptions. Linearity If the model has appropriately captured the relationship between \\(X\\) and \\(E[Y|X]\\), then plotting (estimates of) \\(Y-E[Y|X]\\) against \\(E[Y|X]\\) should just look like noise, with no discernible relationships between the signal component \\(E[Y|X]\\) and the noise (residual) component \\(Y-E[Y|X]\\). If there is a relationship then what we think are just the residuals actually contain some of the signal. ### Let&#39;s return to the Auto data set and the linear model we fit plot(lin_mod$fitted.values, lin_mod$residuals, xlab = &quot;Fitted values&quot;, ylab = &quot;Residuals&quot;) It should be clear that there is a relationship between the fitted values (estimated signal component) and the residuals (estimated noise component). In addition to the U-shape, we can also see that the residual variance is larger for larger fitted values, which would also violate the assumption that the residuals are independent of \\(X\\). Note that although we can use such a plot to determine if the linear model is incorrect, we cannot reasonably rely on such a plot to confirm that the true model is linear. Nonetheless, if there is no apparent relationship present then it is certainly not unreasonable to rely on the model for prediction. Normality of Residuals Even if we cannot see any clear relationship between the fitted values and the residuals, if the residual distribution is far from normal then the confidence intervals and hypothesis tests are no longer valid. We can assess normality either with the use of a histogram or, more appropriately, with a quantile-quantile or QQ plot, which plots the residuals (or indeed any vector of numbers) sorted in increasing order against the quantiles of a normal distribution. If the residuals are approximately normally distributed we should see a more-or-less straight line. par(mfrow = c(1, 2)) hist(lin_mod$residuals, main = &quot;Residual distribution&quot;) qqnorm(lin_mod$residuals, main = &quot;Normal Quantile-Qauntile plot of residuals&quot;) Both the histogram and the QQ-plot show deviations from normality, in particular with a long right tail. 6.2.4 Multicollinearity Recall that the covariance of the regression coefficients \\(\\hbbeta\\) is \\(\\sigma_{\\epsilon}^2 (\\X^\\top\\X)^{-1}\\). This comes from the fact that the coefficients can be expressed as \\(\\hbbeta = (\\X^\\top\\X)^{-1}\\X^\\top \\mathbf{Y}\\), where \\(\\mathbf{Y}\\) is the vector of responses \\((Y_1, ..., Y_n)\\). However, we have had a hidden assumption all along that \\(\\X^\\top \\X\\) is actually invertible. Otherwise there OLS coefficients are not well defined. The matrix \\(\\X^\\top\\X\\) will be invertible as long as none of the columns of \\(\\X\\) can be expressed as a linear combination of the others. However, even when \\(\\X^\\top\\X\\) is invertible, it may be that some of the columns are almost expressible as linear combinations of the others. What this would mean is that at least one of the columns (say the column associated with \\(X_j\\)) can be predicted with low error by a linear model using the other covariates as predictors. Intuitively if one of the covariates can be predicted almost exactly using the others, then that covariate doesnt provide much unique information for predicting \\(Y\\). When this occurs, it leads to the corresponding diagonal element in \\((\\X^\\top\\X)^{-1}\\) being potentially very small, and therefore the variance of \\(\\hat \\beta_j\\) being very large. The presence of covariates which are highly linearly dependent on the rest is known as multicollinearity and can wreak havoc on our inference. Most often this will simply, due to it leading to very large variance in the regression coefficients, mask potential significance of some of the covariates. However, it can also have the opposite effect, that it leads to some covariates being seen as significant in the model even when they are not, and to a much greater extent than would happen by chance if actually this multicollinearity were not present. Variance Inflation Factors (VIFs) can be used to assess the level of multicollinearity, and the VIF for variable \\(X_j\\) is equal to \\(1/(1-R^2_j)\\) where \\(R^2_j\\) is the R-squared value from regression \\(X_j\\) on the other covariates using a linear model. The function vif in the package car computes Generalised VIFs which, for numeric covariates are the same as standard VIFs, but for categorical covariates combine the VIFs from each of the associated dummy variables. ### Load the car library library(car) ### Compute the (Generalised) VIFs from our linear model from the Auto data vif(lin_mod) ## GVIF Df GVIF^(1/(2*Df)) ## cylinders 10.737771 1 3.276854 ## displacement 22.937950 1 4.789358 ## horsepower 9.957265 1 3.155513 ## weight 11.074349 1 3.327814 ## acceleration 2.625906 1 1.620465 ## year 1.301373 1 1.140777 ## origin 2.096060 2 1.203236 Although there is no universally agreed upon threshold for when VIFs indicate problems, a rough rule of thumb is GVIF\\(^{1/2df} &gt; 3\\) may be a problem where \\(df\\) is the number of degrees of freedom introduced by the variable, and is \\(1\\) for numeric covariates and the number of dummy variables used for categorical variables. In the above example we see that the first four covariates have large (G)VIFs, and this could be a reason why some of the otherwise apparently relevant covariates (cylinders and horsepower) were not significant in the model. However, in the exercises at the end of this chapter you will investigate some extensions of this model and see if there is another cause. 6.3 Regularisation for Linear Models Regularisation typically refers to modifications of an estimator which reduces its variance; or makes the realisations of the estimator more regular. Although the linear model is a relatively simple model, and typically has comparatively low variance, this does not mean that regularisation of linear models will not still be beneficial. Moreover, as we just discussed, the variance of the linear regression coefficients may be very large when there is multicollinearity between the covariates. Here we investigate some common approaches for inducing regularisation in linear models. However, the high level ideas are not restricted to linear models and in fact the same sorts of ideas are applicable more generally. 6.3.1 Regularisation Through Constraints A very direct approach to reduce the variance of the regression coefficients is simply to force them not to take very large values (either positive or negative). For example, suppose that instead of choosing \\(\\hbbeta\\) to minimise the training error over all possible vectors of coefficients, we could instead consider the constrained optimisation problem \\[\\begin{align*} \\min_{\\bbeta} &amp; \\ \\frac{1}{n}\\sum_{i=1}^n \\left(y_i - \\beta_0 - \\sum_{j=1}^p \\beta_j x_{ij}\\right)^2\\\\ s.t. &amp; \\ \\sum_{j=1}^p \\beta_j^2 \\leq t, \\end{align*}\\] for some chosen value of \\(t &gt; 0\\). This constrained optimisation says that we choose as our estimate the coefficients which minimise the training error, but not from all vectors in \\(\\Rr^{p+1}\\) as in the OLS case, instead only from among those for which the constraint \\(\\sum_{j=1}^p \\beta_j^2 \\leq t\\) is satisfied. The value of \\(t\\) must be chosen somehow, but we have already seen how we might go about selecting from multiple models and each setting of \\(t\\) corresponds with a different model Smaller \\(t\\) constrains the regression coefficients to a greater extent, leading to smaller variance As \\(t \\to \\infty\\) we simply return to the OLS solution. 6.3.2 Regularisation Through Penalisation Although intuitively setting up constraints in this way may be effective in inducing regularity in an estimator, it turns out there is a much more convenient approach. Mathematically there is a duality between constrained optimisation and so-called penalised optimisation. Specifically, if the functions \\(f\\) and \\(h\\) are nice (we wont go into details of what that means here since all we need is the intuition) then for every \\(t &gt; 0\\) there is a \\(\\lambda(t) \\geq 0\\) for which the solutions to \\[\\begin{align*} \\min_{\\bbeta} &amp; \\ f(\\bbeta)\\\\ s.t. &amp; \\ h(\\bbeta) \\leq t \\end{align*}\\] and \\[ \\min_{\\bbeta} f(\\bbeta) + \\lambda(t) h(\\bbeta) \\] are exactly the same. How this translates to our context is that instead of solving the problem of minimising the Least Squares objective subject to the constraint \\(\\sum_j \\beta_j^2 \\leq t\\) we could be solving \\[ \\min_\\bbeta \\frac{1}{n}\\sum_{i=1}^n \\left(y_i - \\beta_0 - \\sum_{j=1}^p \\beta_j x_{ij}\\right)^2 + \\lambda \\sum_{j=1}^p \\beta_j^2 \\] for some value of \\(\\lambda\\). This is a penalised optimisation problem, where instead of explicitly excluding the possibility of having an output/solution not satisfying the constraint, instead there is a penalty associated with solutions which might violate the constraint (i.e. with a large value of \\(\\sum_{j} \\beta_j^2\\)). The optimisation theory essentially says that if the OLS solution satisfied the constraint, then \\(\\lambda = 0\\), and otherwise there is an appropriate amount of penalisation (value for \\(\\lambda\\)) to ensure the constraint is met exactly and the solutions will be the same But here is the crux: We dont know a priori what an appropriate value for \\(t\\) (in the constrained formulation of the problem) is anyway. In practice we would consider a variety of values for \\(t\\) and select one using one of the model selection approaches we covered in the previous lecture. But if we would be trying out a whole lot of values for \\(t\\), we could equally just bypass this step and jump straight to the penalised formulation above; trying out a range of values for \\(\\lambda\\) and selecting using one of our model selection techniques. There are a number of benefits of jumping straight to the penalised formulation: (i) solving the penalised problem is much easier than solving the constrained formulation (in fact typically constrained optimisation is solved by converting to a penalised problem); and (ii) the statistical properties of the solution to the penalised problem are much better understood than those of the constrained problem. Arguably the main reason for describing the constraint formulation at all is that it conveys an intuitive interpretation of how regularisation is achieved more directly: We simply trap the potential solutions inside a small ball, so they cant vary too much. However, the penalised formulation can also offer some intuition: Informally we may think of the effort in optimising the penalised objective as being split between minimising the training error and minimising the term \\(\\sum_j \\beta_j^2\\), and the larger the value of \\(\\lambda\\) the more effort is placed on minimising \\(\\sum_j \\beta_j^2\\). We also saw that overfitting and high variance estimation comes about if we focus too heavily on the details (i.e. the noise) in the observations. By diverting some of the attention away from minimising the training error the estimation is less affected by the minutiae in the observations, and so less prone to overfitting. A Quick Comment on Standardisation Notice that the scale of the regression coefficients is typically inversely related to the scale on which the corresponding covariates are measured. For example, suppose we were using peoples biometric information in order to assess their risk of certain conditions developing. If we had stored their weight in grams we would need to have a much smaller coefficient for weight then if we had stored their weight in kilograms. If we apply the same amount of penalisation to a coefficient which should be very large and one which should not, purely because of how we stored the data and not because of their intrinsic relevance to the model, then this will have a much greater effect on the first than the second. When performing regularisation in this way it is therefore important to first standardise the coefficients to have similar (or the same) scale/variance. This must of course be taken into account when it comes to predicting the response on new test cases. 6.3.2.1 Visualising the Effects of Regularisation Lets consider just a very simple scenario, where \\(Y = X_1 + X_2 + \\epsilon\\), and \\(X_1\\) and \\(X_2\\) are uncorrelated. The true coefficients are therefore each equal to one (and the intercept is equal to zero). At a population level the true regression coefficients are equal to \\(\\argmin_{\\bbeta} E[(Y-\\beta_1 X_1 - \\beta_2 X_2)^2]\\), and the following plot shows the contours of the function \\(E[(Y-\\beta_1 X_1 - \\beta_2 X_2)^2]\\). We can see the optimal solution at \\((1, 1)\\) and the value of the objective increasing quadratically as we move away from this point in any direction. Any sample of observations of \\(X\\) and \\(Y\\), say \\(\\{(y_1, \\x_n), ..., (y_n, \\x_n)\\}\\), will, however, not show us these population level contours but only those of the training error objective. The following figure shows three potential training error contours: In each case the population level contours are overlayed with the training error contours in red, and the minimisers of the training error are the corresponding OLS solutions. If we had included a penalty on the training error, then this would modify the objective function and its contours. For example, the following figure shows both the population level and training error contours overlayed with the contours of the penalised training error with \\(\\lambda = 1\\), shown in green: We can see the effect is that the OLS solutions have been shifted closer to the origin. Although from only these three instances we cannot clearly see the effect on the bias and variance, in the following figure the OLS and penalised solutions are shown from 100 different samples: The OLS solutions in the left plot are distributed more or less symmetrically about the true coefficients, consistent with the fact we know they are unbiased. However, they are quite spread out. The solutions from the penalised training error are substantially biased, underestimating the true coefficients (they have been shrunken towards the origin), but they have considerably less variance than the OLS solutions. If we had used a smaller value for \\(\\lambda\\) then there would have been less bias, but not as drastic a reduction in variance. If \\(\\lambda\\) had been larger then there would have been even more bias, but even less variance. For example, below are the solutions for \\(\\lambda = 0.1\\) (left) and \\(\\lambda = 10\\) (right): We are unlikely to be able to see clearly the advantages of regularisation in a two-dimensional example, however these have hopefully given some intuition for how regularisation affects the bias and variance of the estimators. In the next subsection we also explore the effects of regularisation in a more appropriate setting, where the number of covariates is much larger. 6.3.3 Ridge Regression The regression model associated with the penalised version of the OLS objective which we saw above is more commonly known as ridge regression. As mentioned before, the penalised approach to regularisation has a number of advantages over the constrained approach, and in the case of ridge regression these largely stem (at least indirectly) from the fact that it has a closed form solution; \\[ \\hbr(\\lambda) = \\left(\\X^\\top \\X + n \\lambda \\I_0\\right)^{-1}\\X^\\top \\y \\] where as before \\(\\X\\) is the design matrix and \\(\\y\\) is the vector of observed responses, and here \\(\\I_0\\) is equal to the identity matrix except that it has a zero in the first diagonal. 6.3.3.0.0.1 Notes The reason for the zero in the first diagonal of \\(\\I_0\\) is that we typically do not constrain/penalise/regularise the intercept term The formulation above may differ slightly in how it is expressed from some other texts. For example one can equivalently formulate the ridge and OLS procedures by first centering both the covariates and response as this has the effect of implicitly setting the intercept term. As long as everything is done correctly the resulting models will be the same, and the centering is typically done as a mathematical convenience more than anything else. Some formulations also describe the ridge objective as the total training loss (not the average) plus the penalty term, in which case there will be just \\(\\lambda \\I_0\\) (or \\(\\lambda \\I\\) if centering is done) instead of \\(n\\lambda\\I_0\\). Again, since we dont know the best value of \\(\\lambda\\) anyway, these are ultimately equivalent. One of the benefits of ridge over OLS is that the solution is always unique (provided \\(\\lambda &gt; 0\\) and none of the covariates are exactly constant). Operationally this is important since if we try to find the OLS solution by setting \\(\\hbbeta = (\\X^\\top \\X)^{-1}\\X^\\top \\y\\) when the solution isnt unique we will get an error since \\(\\X^\\top\\X\\) is not invertible. On the other hand (as long as none of the covariates is constant) \\(\\X^\\top \\X + n\\lambda \\I_0\\) is invertible. 6.3.3.1 Ridge Regression in R The glmnet package includes hyper efficient implementations of ridge regression as well as the variants which follow later in the lecture. It effectively solves the entire path of solutions, for all values of \\(\\lambda\\) over a broad range. The package also includes the function cv.glmnet which can be used for efficient cross-validation for ridge and variants. We can also access glmnet functionality using caret. Example: High Dimensional Regression Simulation Here we will look at the estimation of regression coefficients in a situation where the number of observations is only moderately greater than the number of covariates. The reason for this is so that we can compare the ridge estimates with the OLS estimates, and the OLS solution does not exist (or more precisely there is no unique solution) when \\(n \\leq p\\). In particular in the following R code we will simulate \\(n = 250\\) observations with \\(p = 200\\) covariates, and in such a way that only twenty covariates have a substantial influence on the response (with coefficients equal to 1); eighty have a very slight influence (with coefficients equal to 0.1) and the remaining 100 are not related to the response (with coefficients equal to 0). ### We need to load the glmnet library library(glmnet) ### As always we also need to start by setting up what is constant n &lt;- 250 p &lt;- 200 beta_true &lt;- rep(c(1, 0.1, 0), c(20, 80, 100)) sigma_residual &lt;- 3 ### Now we can simulate observations X &lt;- matrix(rnorm(n*p), n, p) y &lt;- X%*%beta_true + rnorm(n, sd = sigma_residual) ### ... and fit OLS and ridge models ols &lt;- lm(y~., data.frame(X, y)) ridge &lt;- glmnet(X, y, alpha = 0, ) # Notice that the glmnet function does not take a formula argument but rather requires # the matrix or data frame of covariates and vector of responses separately. # You may ignore the alpha = 0 component for now. Ultimately this is just telling glmnet to # fit ridge models and not LASSO or other elastic net models (which we still have to cover) ### We can now investigate the estimated coefficients by plotting them along with the actual values par(mfrow = c(1, 2)) plot(ols$coefficients[-1], ylim = c(-2, 3), xlab = &#39;index&#39;, ylab = expression(beta), main = &#39;OLS coefficients&#39;) # we exclude the intercept as we are not interested in that for now lines(beta_true, col = 2, lwd = 2) # glmnet fits a large number of models by default, for a range of 100 different values for lambda # These are stored in the columns of the output field $beta # For example plot(ridge$beta[,90], ylim = c(-2, 3), xlab = &#39;index&#39;, ylab = expression(beta), main = &#39;Ridge coefficients&#39;) lines(beta_true, col = 2, lwd = 2) It should be clear that the ridge estimates are much closer to the actual values of the coefficients. However, choosing an inappropriate value for \\(\\lambda\\) may lead to either too little or too much shrinkage. As mentioned previously the glmnet package also provides the function cv.glmnet which will run cross-validation and perform model selection for us. ### First let&#39;s simply run cross-validation with the default settings (except ### that we set alpha = 0). ### As always you can learn more about these using help(glmnet) ridge_cv &lt;- cv.glmnet(X, y, alpha = 0) ### Plotting the ridge_cv object will produce a visualisation of the (square root of the) ### estimated prediction error for different lambda, along with standard error bars ### The two vertical lines show the value for lambda which gave the smallest estimate ### of prediction error and the simplest model (largest lambda) with estimated ### prediction error within one standard error of the lowest. ### Note that lambda is plotted on a -log(lambda) transformation, and so the values ### close to the left correspond with the largest values of lambda (most regularisation) plot(ridge_cv) ### We can extract these two values of lambda ridge_cv$lambda.min # minimum estimated prediction error ## [1] 2.206135 ridge_cv$lambda.1se # using 1 standard error rule ## [1] 9.774551 ### To extract a specific solution we can use the function coef(model, s = lambda). ### Let&#39;s plot the solutions selected using cross-validation par(mfrow = c(1, 2)) plot(coef(ridge_cv, s = ridge_cv$lambda.min)[-1], ylim = c(-2, 3), xlab = &#39;index&#39;, ylab = expression(beta), main = &#39;Ridge by minimum CV error&#39;) lines(beta_true, col = 2) plot(coef(ridge_cv, s = ridge_cv$lambda.1se)[-1], ylim = c(-2, 3), xlab = &#39;index&#39;, ylab = expression(beta), main = &#39;Ridge by 1 standard error rule&#39;) lines(beta_true, col = 2) We can see that the solutions are heavily shrunk compared with the OLS solution, but this has resulted in overall much better estimation. We can, for example, consider the differences between the estimated coefficients and the true coefficients ### Squared error of OLS coefficients sum((ols$coefficients[-1] - beta_true)^2) ## [1] 45.42456 ### Squared error of ridge coefficients from minimum CV error sum((coef(ridge_cv, s = ridge_cv$lambda.min)[-1] - beta_true)^2) ## [1] 8.318845 ### Squared error of ridge coefficients from 1 standard error rule sum((coef(ridge_cv, s = ridge_cv$lambda.1se)[-1] - beta_true)^2) ## [1] 12.44804 Degrees of Freedom/Covariance Penalties for Ridge Regression Because the ridge model has a nice (closed form) solution, many of its statistical properties are relatively well understood. In particular, when the linear model assumptions are met it is known that \\[ \\frac{1}{\\sigma^2_{\\epsilon}}\\sum_{i=1}^n Cov(Y_i, \\hat Y_i) = 1 + \\sum_{j=1}^p \\frac{d_j^2}{d_j^2 + n\\lambda}, \\] where \\(d_1 &gt; d_2 &gt; ... &gt; d_p\\) are the singular values of the \\(\\X_0\\), the design matrix but excluding the column of ones (i.e. just the matrix of observations of the covariates). However, because estimation is conducted after standardising the covariates this needs to be taken into account. Lets see how selection based on the covariance penalty approach we looked at in the previous section performs: ### First we standardise the covariates X_stand &lt;- scale(X, center = FALSE) ### Now we compute the singular value decomposition SVD &lt;- svd(X_stand) ### We can now compute estimates for the expected in sample error ### over the range of values by computing the training error for each ### value of lambda and then adding the covariance penalty training_errors &lt;- apply(predict(ridge, X), 2, function(yhat) mean((y-yhat)^2)) ### To find the penalties we need both the degrees of freedom and the ### estimate for residual variance dfs &lt;- sapply(ridge$lambda, function(lambda){ (1+sum(SVD$d^2/(SVD$d^2 + n*lambda))) }) sig2_hat &lt;- min(training_errors)/(n-max(dfs))*n cov_penalties &lt;- 2*dfs*sig2_hat/n ### We can then select the value for lambda which minimises the sum of the ### training error and covariance penalty lambda_select &lt;- ridge$lambda[which.min(training_errors + cov_penalties)] lambda_select ## [1] 0.5997574 ### Finally we can compute the error in estimating the true coefficients sum((coef(ridge, s = lambda_select)[-1] - beta_true)^2) ## [1] 9.823201 The accuracy of the solution is similar to that selected using cross validation, but requires much less computation. 6.3.4 The Least Angle Shrinkage and Selection Operator (LASSO) The LASSO has become one of the most influential developments in modern statistics. Although already almost thirty years old it remains extremely influential and has spawned innumerable other developments since. At face value the formulations of LASSO and the ridge appear extremely similar. Ultimately whenever there was a \\(\\sum_j \\beta_j^2\\) in the formulation of the ridge (either in a constraint or penalty), in the LASSO we replace this with another quantification of the magnitude of the coefficients through \\(\\sum_j |\\beta_j|\\). For example, in the penalised form we have \\[ \\hbl(\\lambda) = \\argmin_\\bbeta \\frac{1}{n} \\sum_{i=1}^n \\left(y_i - \\beta_0 - \\sum_{j=1}^p \\beta_j x_{ij}\\right)^2 + \\lambda \\sum_{j=1}^p |\\beta_j|. \\] Yet, despite their marked similarity at face value, the solutions one actually obtains from the LASSO and ridge are typically quite different. The main reason for this is that the absolute value penalty used by the LASSO has the ability to implicitly select covariates for inclusion in/exclusion from the final model. How this is achieved is that as \\(\\lambda\\) is increased the LASSO absolute value penalty has the effect of actually setting some coefficients to exactly zero. A coefficient equal to zero means the corresponding covariate has no influence on the outputs of the model; it has been excluded or unselected. Although we have seen the ridge squared penalty shrinks the coefficients towards zero as \\(\\lambda\\) is increased, none of them actually every reaches exactly zero. As a result all covariates are still in the model; none have been excluded. Because of its popularity and influence, the LASSO has also been extensively studied. However, as it does not have a nice (closed form) solution the statistical properties of the LASSO estimator are harder to pin down than those of the ridge. 6.3.4.1 Visualising the LASSO Shrinkage and Selection The following plots show the same three examples we saw for the ridge penalisation, but with the absolute penalty of the LASSO and with \\(\\lambda = 0.5\\). Although subtle, it is possible to see that, unlike for the OLS and ridge contours, the LASSO contours are not quite elliptical. Again, however, only three examples does not clearly show the effect on the distribution of the LASSO coefficients as an estimator. The following figure therefore shows the results from the same 100 samples we saw previously, and their comparison with the OLS solutions: Even in this small example, there is some evidence of the variable selection capabilities of the LASSO, where some of the solutions have lined up along the horizontal \\(\\hat \\beta_2 = 0\\) axis. This effect will be more pronounced if we increase \\(\\lambda\\), whereas for smaller \\(\\lambda\\) fewer instances will have coefficients shrunk to zero. The following figure has the corresponding sets of solutions for \\(\\lambda = 0.1\\) (left) and \\(\\lambda = 2\\) (right): 6.3.4.2 The LASSO in R Fitting LASSO models in R is done exactly as it was for ridge except we set the argument alpha to one, rather than to zero. Lets again fit models to our simulated high dimensional data in order to (better than in only two dimensions) visualise the selection capabilities of the LASSO, and also see how accurate models selected with cross-validation are in comparison with OLS and ridge. The following chunk of R code is analogous to what we did earlier, with only the very minor modification to alpha ### We start by performing cross validation in order to select appropriate ### values for lambda lasso_cv &lt;- cv.glmnet(X, y, alpha = 1) ### As before, plotting the lasso_cv object will produce a visualisation ### of the (square root of the) estimated prediction error for different ### lambda, along with standard error bars, with vertical lines showing ### the solution which minimises estimated prediction error and the ### solution using the 1 standard error rule ### As before lambda is plotted on a -log(lambda) transformation plot(lasso_cv) ### We can extract these two values of lambda lasso_cv$lambda.min # minimum estimated prediction error ## [1] 0.1748325 lasso_cv$lambda.1se # using 1 standard error rule ## [1] 0.3055247 ### To extract a specific solution we can use the function coef(model, s = lambda). ### Let&#39;s plot the solutions selected using cross-validation par(mfrow = c(1, 2)) plot(coef(lasso_cv, s = lasso_cv$lambda.min)[-1], ylim = c(-2, 3), xlab = &#39;index&#39;, ylab = expression(beta), main = &#39;LASSO by minimum CV error&#39;) lines(beta_true, col = 2, lwd = 2) plot(coef(lasso_cv, s = lasso_cv$lambda.1se)[-1], ylim = c(-2, 3), xlab = &#39;index&#39;, ylab = expression(beta), main = &#39;LASSO by 1 standard error rule&#39;) lines(beta_true, col = 2, lwd = 2) It should be clear that in both solutions many of the coefficients have been set exactly to zero. It may also be apparent that those associated with the large true coefficients are typically less shrunken compared to the ridge counterparts. The overall squared estimation error also shows the good performance of the LASSO ### Squared error of LASSO coefficients from minimum CV error sum((coef(lasso_cv, s = lasso_cv$lambda.min)[-1] - beta_true)^2) ## [1] 4.914901 ### Squared error of ridge coefficients from 1 standard error rule sum((coef(lasso_cv, s = lasso_cv$lambda.1se)[-1] - beta_true)^2) ## [1] 5.817716 Degrees of Freedom and Covariance Penalties? It has been shown that for the LASSO with fixed \\(\\lambda\\) one has \\[ \\sum_{i=1}^n Cov(Y_i, \\hat Y_i) = E\\left[\\sum_j I\\left(\\hbl_j(\\lambda) \\not = 0\\right)\\right], \\] that is, the covariance between the responses and the fitted values is equal to the expected number of non-zero coefficients (including the intercept). We can in principle use the number of non-zero coefficients in the solution we obtained as an estimate for this: ### As before we compute the training error and covariance penalties ### However we haven&#39;t simply fit the LASSO for a range of lambda values ### as we only called to cv.glmnet so far. However, the lasso_cv object ### has a field $glmnet.fit which contains exactly the same as the output ### from a call to glmnet training_errors &lt;- apply(predict(lasso_cv$glmnet.fit, X), 2, function(yhat) mean((y-yhat)^2)) ### Now the degrees of freedom depend on the estimated coefficients dfs &lt;- apply(lasso_cv$glmnet.fit$beta, 2, function(beta){ 1+sum(beta!=0) }) # note above we have 1 + sum(beta!=0) since the intercept is not included in $beta sig2_hat &lt;- min(training_errors)/(n-max(dfs))*n cov_penalties &lt;- 2*sig2_hat*dfs/n ### We can then select the value for lambda which minimises the sum of the ### training error and covariance penalty lambda_select &lt;- lasso_cv$lambda[which.min(training_errors + cov_penalties)] lambda_select ## [1] 0.1322543 ### Finally we can compute the error in estimating the true coefficients sum((coef(lasso_cv, s = lambda_select)[-1] - beta_true)^2) ## [1] 5.031624 6.3.4.3 Ridge or LASSO? As with all things there is no one size fits all and whether the ridge or LASSO should be preferred is very much dependent on the situation. We saw in the previous simulations that for that context the LASSO gave more accurate estimation. However, we had simulated the situation where some of the coefficients (in fact half of them) are actually exactly zero and so the LASSOs selection capabilities made it superior. Some relevant points are, however, that If a simpler/more interpretable model is desired, the LASSO is preferable as it can eliminate covariates if they are not contributing sufficiently to the prediction of the response Fewer non-zero coefficients means understanding the predictions from the model is more straightforward, and inclusion of a variable in the model gives a sense of its significant relevance to the response Having said this, when there is strong correlation between multiple variables all of which are actually related to the response (i.e. their true coefficients are non-zero) the LASSO will often eliminate most of them and only keep one or a few as that is all which is needed for predicting the response The non-zero coefficients are also typically closer to their OLS counterparts than in the ridge If it is believed all covariates contribute at least somewhat to the response, then the ridge may be preferable The ridge also tends to group related covariates, assigning them coefficients with similar magnitude rather than eliminating most of them like the LASSO In the absence of any such knowledge or objective, cross-validation can be used to estimate which offers better predictive ability If the accuracy of the coefficients themselves (and not the accuracy of prediction) is what is wanted then the bootstrap can be used to estimate the accuracy of the coefficients, however we do not cover this topic here. 6.3.5 Alternatives Perhaps a better answer to whether one should choose the ridge or LASSO is that it is a false dilemma; there are alternatives. The Elastic Net The elastic net is a combination of the LASSO and ridge, in that it includes both penalties in the objective. Specifically, for \\(\\lambda \\geq 0\\) and \\(\\alpha \\in [0, 1]\\) (remember the parameter alpha from glmnet, well here it is) the elastic net solution is \\[ \\hbe(\\lambda, \\alpha) = \\argmin_\\bbeta \\frac{1}{n}\\sum_{i=1}^n \\left(y_i - \\beta_0 - \\sum_{j=1}^p \\beta_j x_{ij}\\right)^2 + \\lambda\\left(\\alpha \\sum_{j=1}^p |\\beta_j| + (1-\\alpha)\\sum_{j=1}^p\\beta_j^2\\right). \\] For \\(\\alpha = 0\\) we simply have the ridge and for \\(\\alpha = 1\\) we have the LASSO For appropriate values of \\(\\alpha\\) and \\(\\lambda\\) the elastic net can have the benefits of both It can perform variable selection by shrinking coefficients to exactly zero It can avoid losing relevant covariates due to correlation, and can also group correlated variables by the magnitude of the coefficients (like the ridge) The glmnet function is actually always fitting an elastic net model, and in our previous calls to the function we had simply chosen the specific settings of \\(\\alpha\\) which correspond to the ridge and LASSO models. The Relaxed LASSO Another alternative is to use the LASSO purely for variable selection, and then subsequently fit the OLS model only on those covariates not eliminated through the LASSO. This is known as the relaxed LASSO since it is less constrained than the standard LASSO. Even though the same covariates are included/excluded in the LASSO and its relaxation (whereas the selection of variables with elastic net may differ for intermediate values of \\(\\alpha\\)), those which are included do not have their coefficients shrunk. There is also a spectrum of models in between the LASSO and its relaxation, by taking a weighted average of the LASSO and relaxed LASSO coefficients. All of these are implemented in the glmnet package. Subset Selection An alternative to the penalisation/constraint approach to regularisation is to use some other strategy for variable selection. We will not cover these here, and for the interested student please look to Chapter 6.1 in An Introduction to Statistical Learning. 6.4 Summary Linear models have a number of advantages over some of the more complex models we will encounter later on They are generally more intuitive and their predictions and the influence of each of the variables are interpretable They are also comparatively low variance estimators and may lead to better accuracy either when the true regression function is close to linear or when the sample size is not large enough to reliably estimate a more complex function Regularisation (through constraints or more often penalties on the optimisation of the training error) can have remarkable effect on the accuracy of regression models Although we only looked at the standard regression problem here, the principles of regularisation as a way to traverse the bias-variance tradeoff is broadly applicable Different forms of the regularisation can dramatically change the fitted models even for similar overall bias and variance E.g. the selection capabilities of the LASSO No single formulation will always be superior to others, and knowing what is needed from the model and/or any relevant domain knowledge should be incorporated when available Remember that it is important to standardise covariates when regularising a linear model The glmnet package is a fantastic piece of software 6.5 Exercises Refer to the Auto data set in the ISLR2 package, and the task of predicting mpg from the other variables (excluding name). Obtain approximate \\(95\\%\\) confidence intervals for the regression coefficients using (i) the outputs from summary(model) where model is obtained with the function lm; (ii) the bootstrap. Compare the results from these two approaches. Again with reference to the Auto data set, the objective of this task is to compare ordinary least squares regression models for modelling mpg, log(mpg) and sqrt(mpg), using the other variables (except name). Start by setting aside \\(30\\%\\) of the data as a test set. Use 10-fold cross validation to estimate the expected prediction error for predicting mpg with each of the models. You will need to somehow incorporate the back-transformation into the cross validation, where appropriate. You could do this by setting up your own caret model, or by writing your own cross validation code. There may also be other ways you could do this. Estimate the prediction error of the model selected from the cross validation results, using the test set. Download the Genes.RData file from the R Scripts folder on Moodle. This is a simulation based modification of the Khan Gene Data included in the ISLR2 package. There is a total of 2308 covariates related to gene expression measurements associated with cancerous tumour tissues, and a response variable (y) derived from information related to cancer type, which can be treated as continuous. Start by setting aside \\(30\\%\\) of the data for testing. Using both cross validation based estimates of expected prediction error and covariance based estimates of expected in-sample error, select appropriate values for \\(\\lambda\\) for use within a ridge regression model. Do the same as above for the LASSO, and also select appropriate values for \\(\\alpha\\) and \\(\\lambda\\) for an elastic net model using cross validation. Choose a single model from all of these options. THEN evaluate the performance of all of them on the test set. What do you observe? "],["glms.html", "7 Generalised Linear Models 7.1 Generalised Predictive Modelling 7.2 Classification and Logistic Regression 7.3 Summary 7.4 Exercises", " 7 Generalised Linear Models \\[ \\def\\x{\\mathbf{x}} \\def\\Rr{\\mathbb{R}} \\newcommand{\\argmin}{\\mathop{\\rm argmin}} \\newcommand{\\argmax}{\\mathop{\\rm argmax}} \\def\\F{\\mathcal{F}} \\def\\hbbeta{\\hat{\\boldsymbol{\\beta}}} \\def\\bbeta{\\boldsymbol{\\beta}} \\def\\X{\\mathbf{X}} \\def\\y{\\mathbf{y}} \\def\\hg{\\hat g} \\] In the previous chapter we looked at the standard linear model, and a variety of regularised variants of the Ordinary Least Squares (OLS) model. Linear models have the advantage of being interpretable, they are computationally efficient to compute/estimate and have comparatively low variance. However, there are some obvious situations where a standard linear model is not appropriate Naturally when the true function \\(g^*\\) is very far from linear, we may be sacrificing significantly on accuracy by choosing to fit a linear model. But there is something even more fundamental: What if the response variable, \\(Y\\), is innately unsuited to the standard regression context \\(Y = g^*(X) + \\epsilon\\), regardless of the form of \\(g^*\\)? Example: Type II Diabetes Risk Suppose we collected information from a group of people not diagnosed with Type II diabetes; pertaining to genetic, biometric and lifestyle factors. We then followed them for a period of ten years and noted whether or not they subsequently developed the condition during that period. As a statistician or data scientist we may wish to use these data in order to understand the key factors (covariates) which made people more likely to develop the condition, or to fit a model which could predict whether or not other individuals in the population are likely to become diabetic in the next decade. Our response variable is categorical: Either an individual developed the condition (category 1) or they didnt (category 0). If we fit a linear model to predict \\(Y\\) from the covariates we collected, the outputs/predictions from the model would not neatly fall into these categories, but would instead span some interval and could even be negative. Although there are some simple heuristics we could apply to transform the outputs from a linear model into such a categorisation, and perhaps you have even thought of a few yourself, it should be clear that these are not statistically sound and are certainly far from optimal. Fortunately, there is a far more elegant approach and is the topic of this chapter. 7.1 Generalised Predictive Modelling We now consider a more general setting than we encountered before, and take a step back from the standard regression context to the general predictive modelling set-up. That is, we have at our disposal a sample of observations of covariates (still called \\(X\\)) and a response variable (still called \\(Y\\)), and our objective is to use these data to obtain a model which we can use to predict likely/appropriate values for the response when given new sets of values for the covariates. Whether such a model is useful or not will depend strongly on how we choose to model the situation, so lets cast our minds back to our introduction to probability and statistics. There we encountered multiple random variables, and their probability distributions, and situations where they may be appropriate for modelling. We could model our response variable as having one of these distributions, for example. 7.1.1 Link Functions As we are in the predictive modelling context, we are ultimately interested in how the response is related to the covariates, and how to use these relationships for prediction. As described in Chapter 4, predictive modelling is ultimately about modelling features of the conditional distribution(s) of \\(Y|X\\). In particular we may be interested in how the expected value of \\(Y\\) depends on \\(X\\), however whereas in the standard regression setting where \\(Y = g^*(X) + \\epsilon\\) we did not have any limitations on what values \\(E[Y|X]\\) could be (or at least we assumed there was no such limitation), in a more general setting we can connect the valid range of values that \\(E[Y|X]\\) could potentially take to the outputs from a regression function through a link function \\(h\\), i.e., through a more generalised regression equation \\[ h\\left(E[Y|X]\\right) = g^*(X). \\] It should be clear that the standard regression setting is a special case of this where the link function is simply the identity \\(h(y) = y\\). If we knew that \\(E[Y|X]\\) had to be positive then we could set, for example, \\(h(z) = \\exp(z)\\). There are thus two main ingredients beyond what we had for the standard regression problem: We need an appropriate probability distribution with which to model the response We need to choose an appropriate link function, \\(h\\). Importantly we need \\(h\\) to be invertible so that \\(E[Y|X] = h^{-1}(g^*(X))\\) is well defined. We can assume without loss of generality that \\(h^{-1}\\) should be strictly increasing over the range of \\(g^*\\). We should choose \\(h\\) so that the outputs of \\(h^{-1}\\) cover only the plausible values for \\(E[Y|X]\\). For example \\(h\\) could be the exponential function if we only want positive estimates for \\(E[Y|X]\\) or \\(h\\) could be the logit function if \\(E[Y|X]\\) must lie between zero and one (we will come back to this when we look at logistic regression a little bit later). We also have the ingredients from the standard regression problem: (i) from which collection of functions, \\(\\F\\), should I select my estimate(s) for \\(g^*\\); and (ii) what loss function should we use for training/fitting/estimation? For (i), the same considerations in choosing \\(\\F\\) apply regardless of context: If we choose a limited set of functions then we may not be able to pick up on the complexity of \\(g^*\\), leading to bias. If \\(\\F\\) is a very rich collection of functions, including some with very complex structure, we may suffer from high variance in any fitted models unless we include some regularisation strategy. The particular setting where \\(\\F\\) is the collection of all linear (or affine) functions then we have what are known as Generalised Linear Models (GLMs). 7.1.2 Loss Functions from the Likelihood As statisticians and/or statistical data scientists we should always have at the back of our minds the potential of using maximum likelihood to perform estimation. The same applies in a predictive modelling context. Lets suppose that the density function or mass function (depending on whether \\(Y\\) is continuous or discrete) for \\(Y\\) is given by \\(f_Y\\) and is parameterised by its mean value which we will call \\(\\mu\\). From an estimation perspective, maximising the log-likelihood, over choices of estimates for \\(g^*\\), would lead to an estimate \\[ \\hg = \\argmax_{g \\in \\F} \\sum_{i=1}^n \\log\\left(f_Y(y_i | \\mu = h^{-1}(g(\\x_i)))\\right). \\] When we talk about training, however, we typically focus on minimising a loss function, and so a natural choice for \\(L(y, \\hat y)\\) can be given by \\(L(y, \\hat y) = -\\log\\left(f_Y(y_i| \\mu = \\hat y)\\right)\\). In fact it can easily be shown that maximising the Gaussian likelihood is equivalent to minimising the squared error loss function, and so we had already actually been doing this for the standard regression context. 7.2 Classification and Logistic Regression We spoke briefly about the classification problem when we introduced the predictive modelling framework explicitly in Chapter 4. In particular classification refers to the situation where the response variable is categorical, and the different categories are typically referred to as classes and the values of the response are often called the class labels. The diabetes example we described above is an example of a binary classification problem since there are exactly two classes. When there are more than two classes we refer to this as a multiclass problem and often we combine the outputs of multiple binary classification models in order to perform multiclass classification. A lot of focus, therefore, is placed on binary classification models. 7.2.1 Logistic Regression Logistic regression is arguably the most commonly encountered generalised linear model, and is a natural application of the generalised predictive modelling framework described above to the binary classification problem. Specifically, when a random experiment has only two potential outcomes (e.g. the allocation to one of two classes) then the situation may be modelled using a Bernoulli random variable, i.e. \\(Y = 0\\) if allocation is to the first class and \\(Y = 1\\) if allocation is to the second class, and we have \\(P(Y=1) = 1-P(Y=0).\\) Recall that when we introduced the binomial and Bernoulli random variables we spoke of outcome \\(1\\) being a success, however not with any implication that this outcome is necessarily favourable compared with the other outcome, only that it may represent an outcome of interest compared with the alternative. Nonetheless in the classification problem it is ultimately arbitrary which class we label with the value \\(1\\) and which we label with the value \\(0\\) as long as we are consistent and in the end communicate our findings appropriately. We return to this a little later, since different functions in R sometimes handle the allocation of which outcome is of interest differently. Now, notice that when \\(Y\\) has a Bernoulli distribution then \\(E[Y] = P(Y=1)\\), and so within the GLM framework when we are modelling in terms of \\(E[Y|X]\\) we are actually modelling in terms of \\(P(Y = 1|X)\\). Since \\(E[Y|X]\\) is equal to a probability we therefore want our inverse link function to map all potential outcomes of the underlying linear model (whose output is the entire real line) to the space of probabilities. All increasing functions with this property are just the cumulative distribution functions of some random variables, and the most popular choices are to use the cdfs of the standard normal (leading to probit regression) and the so-called logistic distribution, leading to name logistic regression. The Logistic and Logit Functions The logistic function (cdf of the logistic distribution) is also commonly known as the sigmoid function and plays an important role in neural networks (especially earlier versions), and is given by \\(f(z) = \\frac{1}{1 + \\exp(-z)}\\), with inverse \\(f^{-1}(z) = \\log\\left(\\frac{z}{1-z}\\right)\\) known as the logit function. Since it is the inverse link function which takes the form of the logistic cdf, we have \\[\\begin{align*} h(z) = \\log\\left(\\frac{z}{1-z}\\right), \\ h^{-1}(z) = \\frac{1}{1+\\exp(-z)}. \\end{align*}\\] ### Plotting the logit and logistic functions par(mfrow = c(1, 2)) z &lt;- seq(0, 1, length = 100) plot(z, log(z/(1-z)), ylab = expression(h(z)), main = &quot;Logit link function&quot;, type = &#39;l&#39;) z &lt;- seq(-5, 5, length = 100) plot(z, 1/(1+exp(-z)), ylab = expression(h^-1*(z)), main = &quot;Logistic inverse link function&quot;, type = &#39;l&#39;) Estimation in Logistic Regression Now, in the context of GLMs recall that we are modelling \\(E[Y|X]\\) from the perspective that \\(h^{-1}(g^*(X)) = E[Y|X]\\), and we are selecting our estimate for \\(g^*(X)\\) from the collection of all linear (affine) functions of \\(X\\). For convenience lets use the notation \\(q(X) = E[Y|X]\\), which in the context of logistic regression is also equal to \\(P(Y = 1|X)\\). We may therefore write \\[\\begin{align*} \\hat q(X) = h^{-1}\\left(\\hat \\beta_0 + \\sum_{j=1}^p \\hat \\beta_j X_j\\right) = \\frac{1}{1 + \\exp(-\\hat \\beta_0 - \\sum_{j=1}^p \\hat \\beta_j X_j)}, \\end{align*}\\] where \\(\\hbbeta\\) is the solution arising from maximising the log-likelihood (or minimising the negative log-likelihood if we want to think of this as a loss function). Interpreting the Logistic Regression Coefficients The logit function has a convenient interpretation from the point of view of the odds of an outcome. The odds of an outcome is simply the ratio of the probability that the outcome occurs and the probability it doesnt occur. For example, in the sporting context we may refer to the odds of one team winning being five to one which, assuming no ties or draws, means the team is five times more likely to win than to lose or that the ratio of the probability they win and the probability they lose is equal to five. Now, recall that for \\(z \\in (0, 1)\\) the logit link function is equal to \\(h(z) = \\log(\\frac{z}{1-z})\\). When applying this function to \\(q(X) = P(Y=1|X)\\) and asserting that our estimate for \\(h(q(X))\\) takes the form of a linear function, we have \\[ h(\\hat q(X)) = \\log\\left(\\frac{\\hat q(X)}{1-\\hat q(X)}\\right) = \\hat \\beta_0 + \\sum_{j=1}^p \\hat \\beta_j X_j, \\] that is, the linear function on the right hand side above represents the estimate of the log-odds of the event \\(Y=1\\) given \\(X\\). So what exactly does this mean for our coefficients? Firstly, the sign of the coefficients have an analogous interpretation to what they had in the ordinary linear regression context, where now a positive coefficient tells us that the corresponding covariate is positively related to the probability that \\(Y\\) takes the value \\(1\\). The magnitude in the change of this probability, however, is better interpreted from the point of view of the odds, where a one unit increase in variable \\(X_j\\) is associated with an increases in the log-odds of \\(Y\\) being equal to 1 by an amount equal to \\(\\hat \\beta_j\\). Alternatively, and equivalently, we could say that a one unit increase in variable \\(X_j\\) changes the odds of \\(Y = 1\\) by a factor \\(\\exp(\\hat \\beta_j)\\). 7.2.1.1 Logistic Regression in R The function glm allows us to fit generalised linear models directly from the contents of Rs base distribution. The main arguments are the same as those passed to the lm function, i.e. a formula, describing which is the response and which are the covariates to be used, and a data frame containing all associated variables. However, we now also need to specify which distribution (or more precisely which distribution family) we are using to model the response. The glm function has a default link function for each family of distributions, but this can also be specified. For example when fitting a logistic regression model this can be done by either setting family = \"binomial\" or family = binomial(link = \"logit\"). Lets look at a simple example. The Pima.tr data set in the MASS package (included in Rs base distribution) contains data on a group of native American women, including whether or not they have diabetes according to the criteria set out by the World Health Organisation. Note that the .tr refers to training and there is an additional Pima.te data set for testing, however we will only look at this later on. First lets load the data and see what variables are included. ### First load the MASS library and then the data library(MASS) data(Pima.tr) ### We can then look at the names of the variables, and the first few entries str(Pima.tr) ## &#39;data.frame&#39;: 200 obs. of 8 variables: ## $ npreg: int 5 7 5 0 0 5 3 1 3 2 ... ## $ glu : int 86 195 77 165 107 97 83 193 142 128 ... ## $ bp : int 68 70 82 76 60 76 58 50 80 78 ... ## $ skin : int 28 33 41 43 25 27 31 16 15 37 ... ## $ bmi : num 30.2 25.1 35.8 47.9 26.4 35.6 34.3 25.9 32.4 43.3 ... ## $ ped : num 0.364 0.163 0.156 0.259 0.133 ... ## $ age : int 24 55 35 26 23 52 25 24 63 31 ... ## $ type : Factor w/ 2 levels &quot;No&quot;,&quot;Yes&quot;: 1 2 1 1 1 2 1 1 1 2 ... As always if we want more information on something we can use the help function. For example, full descriptions of these variables can be seen by calling help(Pima.tr). Lets now fit a logistic regression model. Note that the response variable is called type and is already encoded as a factor variable ### We can now fit a logistic regression model to the data logistic_mod &lt;- glm(type~., data = Pima.tr, family = &quot;binomial&quot;) # we could equally have used glm(type~., data = Pima.tr, family = binomial(link = &quot;logit)) The output from glm has many of the same contents as that from lm, including information on the significance of each of the variables. However, it should be noted that the same potential issues of multicollinearity apply in the context of all GLMs and care should always be taken when interpreting these. summary(logistic_mod) ## ## Call: ## glm(formula = type ~ ., family = &quot;binomial&quot;, data = Pima.tr) ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -9.773062 1.770386 -5.520 3.38e-08 *** ## npreg 0.103183 0.064694 1.595 0.11073 ## glu 0.032117 0.006787 4.732 2.22e-06 *** ## bp -0.004768 0.018541 -0.257 0.79707 ## skin -0.001917 0.022500 -0.085 0.93211 ## bmi 0.083624 0.042827 1.953 0.05087 . ## ped 1.820410 0.665514 2.735 0.00623 ** ## age 0.041184 0.022091 1.864 0.06228 . ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 256.41 on 199 degrees of freedom ## Residual deviance: 178.39 on 192 degrees of freedom ## AIC: 194.39 ## ## Number of Fisher Scoring iterations: 5 We can see that a cursory inspection shows the significant covariates being glu (a plasma glucose based measurement), ped (a variable derived from the diabetes status of the individuals relatives), and to a lesser extent bmi (body mass index) and age. The coefficients of all of these variables are also positive, showing they are positively associated with the likelihood of having diabetes. If we consider the age variable in isolation, we can say that (all other variables being kept fixed) with each passing year the odds of having diabetes typically increases by a factor exp(coef(logistic_mod)['age']) = 1.042043. A Quick Aside on the Reference Class Note that by default glm will treat the first factor level of the response as the so-called reference class, associated with \\(Y = 0\\), and the other class is treated as \\(Y = 1\\). Unless told otherwise, the levels of a factor will be sorted alphabetically if they are characters, or increasing numerically if they are numerical. So for the variable Pima.tr$type the first level is No since this is prior to Yes alphabetically. This is very important when it comes to interpretation since if the type variable had been stored differently we may have had an inverse interpretation to what is true. If you are ever unsure you can check the order of the levels of a factor variable using levels(factor_variable) and the first level listed will be treated as the reference. Alternatively you can tell R explicitly how to choose the reference class using the function relevel(factor_variable, ref = \"&lt;name of factor level you want as reference&gt;\"). For example ### So as not to overwrite the contents of Pima.tr, let&#39;s copy the type variable type &lt;- Pima.tr$type ### We check how its values/levels have been ordered by default levels(type) ## [1] &quot;No&quot; &quot;Yes&quot; ### Now we created a modified factor variable with the reference class changed ### and check that it has worked as we want type_releveled &lt;- relevel(type, ref = &quot;Yes&quot;) levels(type_releveled) ## [1] &quot;Yes&quot; &quot;No&quot; Unfortunately different functions across different packages sometimes handle the reference class differently, and so it is always important to validate what the functions youve used have done before reaching any conclusions. 7.2.1.2 Evaluation Criteria for Classification Models Probabilistic classification models, like logistic regression models, do not produce an explicit classification but rather, as we have seen, produce estimates of the probability of membership to class 1. Using the notation from before, it is natural to convert a probability \\(q(X) = P(Y = 1|X)\\) into an explicit classification by simply using the rule \\[ \\hat Y(q(X)) = \\left\\{\\begin{array}{ll} 1, &amp; q(X) &gt; 0.5\\\\ 0, &amp; q(X) \\leq 0.5. \\end{array}\\right. \\] That is, we should allocate the predicted class label, \\(\\hat Y\\), according to which of the two classes is more likely. However depending on the context we may wish to use a different threshold than 0.5 as we may attribute more importance to some misclassifications over others. For example, in the context of treatable but degenerative diseases, a misclassification that someone who is not afflicted (\\(\\hat Y = 0\\)) when they actually are (\\(Y = 1\\)) may have far reaching consequences since this would delay their treatment and cause unnecessary damage. On the other hand, although there may be psychological impact if someone is diagnosed with a condition when they dont actually have it and they may be (at least temporarily) administered treatment for a condition they do not have, often this is less problematic than the reverse. To limit the number of false negatives (i.e. a prediction of \\(\\hat Y = 0\\) when \\(Y = 1\\)) we may wish to choose a lower threshold than 0.5 and conservatively propose treatment (or at least further tests) even if the estimated probability, \\(\\hat q(X)\\), is substantially lower than 0.5. It is generally not the decision of the statistician or data scientist what threshold to use, nor what emphasis to place on the different types of misclassification, but rather it is their job to ascertain from the problem owner what the objectives are (recall the predictive modelling pipeline). For the same reason, just reporting the classification accuracy (proportion of correctly classified instances) or misclassification rate (proportion of incorrectly classified instances) may not be very meaningful. Returning to the context of illnesses typically the baseline incidence is very low. For example in our Pima.tr data set about one third of instances were diabetic, but in the general population the incidence of diabetes is more like one in ten. A classifier which simply says nobody is diabetic will be right \\(90\\%\\) of the time, i.e. will have classification accuracy of \\(0.9\\). But hopefully it is clear that such a classifier is useless practically. It is therefore prudent to communicate which classes are being correctly/incorrectly identified. For this we frequently use the precision and recall statistics. Suppose, as it was alluded to earlier, that we refer to the reference class as a negative prediction/outcome and the other class (class 1) as a positive prediction/outcome. The confusion matrix is a tabulation of all the intersections of positive and negative predictions and true classes/outcomes: \\(Y = 0\\) \\(Y = 1\\) \\(\\hat Y = 0\\) True Negative (TN) False Negative (FN) \\(\\hat Y = 1\\) False Positive (FP) True Positive (TP) Within this terminology the Positive/Negative refers to the prediction and the True/False refers to the actual class label. The function confusionMatrix (provided in the caret package) will produce a confusion matrix from a set of class predictions and associated true class labels. Returning to our logistic_mod from before, lets make predictions on the test set Pima.te and for now simply use the threshold of 0.5 to decide on final class predictions. Note that the function confusionMatrix will treat the first level of the factor as the positive class, which may contrast with how glm treats the different factor levels. We can therefore simply be explicit about which we want to be the positive class: ### As always we first load the library/ies we need library(caret) ### When calling predict on a glm object we need to tell it whether ### we want the outputs from the underlying linear model (type = &quot;link&quot;) ### or to be the estimates for E[Y|X] (type = &quot;response&quot;) logistic_preds &lt;- predict(logistic_mod, Pima.te, type = &quot;response&quot;) logistic_class &lt;- factor(logistic_preds &lt;= 0.5, levels = c(TRUE, FALSE), labels = c(&quot;No&quot;, &quot;Yes&quot;)) ### We can now produce the confusion matrix, where the first argument ### is the predicted classes and the second is the actual classes confusionMatrix(logistic_class, Pima.te$type, positive = &quot;Yes&quot;) ## Confusion Matrix and Statistics ## ## Reference ## Prediction No Yes ## No 200 43 ## Yes 23 66 ## ## Accuracy : 0.8012 ## 95% CI : (0.7542, 0.8428) ## No Information Rate : 0.6717 ## P-Value [Acc &gt; NIR] : 1.116e-07 ## ## Kappa : 0.5271 ## ## Mcnemar&#39;s Test P-Value : 0.01935 ## ## Sensitivity : 0.6055 ## Specificity : 0.8969 ## Pos Pred Value : 0.7416 ## Neg Pred Value : 0.8230 ## Prevalence : 0.3283 ## Detection Rate : 0.1988 ## Detection Prevalence : 0.2681 ## Balanced Accuracy : 0.7512 ## ## &#39;Positive&#39; Class : Yes ## The output from this function contains a large number of statistics which are calculated from the confusion matrix table itself. Of particular interest are The overall accuracy (slightly more than \\(80\\%\\) of test cases were correctly classified). Although, as described above, the accuracy alone may be meaningless, reported along with other statistics tells a more nuanced story The No Information Rate is the accuracy which would be achieved by a model which just predicts everything is in the most common class overall. The P-Value above gives an indication whether the classifier is actually doing any better than the no information classifier. The Sensitivity is defined as the proportion of actual positives which were detected (or predicted), and is given by TP/(TP + FN) The Specificity is analogously defined as the proportion of actual negatives which are correctly identified/predicted as such, and is given by TN/(TN + FP) The balanced accuracy is the average of sensitivity and specificity. The Receiver Operating Characteristic (ROC) Curve Even though the statistics computed from the confusion matrix give considerably more information than just the classification accuracy, they do require commitment to a single threshold for classification (we used 0.5 in the previous example). However it may not always be known a priori what the most appropriate threshold is. It should be clear, however, that as we decrease the threshold we have more probabilities exceeding the threshold, and hence more predictions of \\(\\hat Y = 1\\). This leads to higher and higher sensitivity, but lower and lower specificity. If we increase the threshold we have the reverse occurring. What is pertinent, however, is that for each value of the threshold we obtain a pair of values for specificity and sensitivity. The Receiver Operating Characteristic curve is a functional representation of the relationship between specificity and sensitivity as the threshold for classification is varied (typically specificity is plotted from 1 to 0 rather than 0 to 1 so that the function is shown as increasing). If there is a threshold leading to both very high sensitivity and very high specificity then the curve will trace close to the top left of the plot before flattening out. The area underneath it will be close to one. On the other hand, a totally random classifier will have an Area Under the Curve (AUC) close to a half. The function roc in the package pROC will compute the ROC information given a vector of actual class labels and a vector of probabilities arising from a classifier. ### First we load the pROC package library(pROC) ### We then apply the function roc to the predictions from ### our logistic regression model and the actual labels from the ### test data set logistic_roc &lt;- roc(Pima.te$type, logistic_preds) ## Setting levels: control = No, case = Yes ## Setting direction: controls &lt; cases plot(logistic_roc) logistic_roc ## ## Call: ## roc.default(response = Pima.te$type, predictor = logistic_preds) ## ## Data: logistic_preds in 223 controls (Pima.te$type No) &lt; 109 cases (Pima.te$type Yes). ## Area under the curve: 0.8659 As alluded to above a perfect classifier will have an AUC of one, whereas a totally random classifier will have an AUC close to a half. As a rule of thumb, anything above 0.75 is considered good; above 0.85 is very good; and above 0.95 is excellent. The AUC of 0.8659 therefore shows that the logistic regression model achieved very good overall performance on the diabetes prediction task. 7.2.1.3 GLMs Within caret If we want to estimate the performance of a GLM (e.g. a logistic regression model) we can do this very easily using caret. Recall that the function train performs all the heavy lifting for us, and all we have to do is tell it how to model the problem (through a formula object), which data set to use, which method to use, and ultimately what we want from it, i.e., just fit a model or do cross-validation, etc. For the last of these we use the trainControl function. We can also use trainControl to tell it what performance metrics we want using the summaryFunction argument. In particular if we want to produce ROC statistics we need to set summaryFunction = twoClassSummary, and in order for it to produce ROC statistics we need to ensure predicted probabilities are computed and saved. Lets run ten fold cross validation to estimate the performance of the logistic regression model. Note that although we have already checked its performance on the test data (normally we would not do things in this order), but for the purpose of model selection we may wish to compare the ROC statistics as estimated by cross validation, from multiple models. trControl &lt;- trainControl(method = &quot;cv&quot;, number = 10, summaryFunction = twoClassSummary, classProbs = TRUE, savePredictions = TRUE) We can now pass this as our trControl argument to the train function. However, it is important to note that (for some reason) caret and glm handle the reference class differently, essentially reversing them. As a result the sensitivity and specificity values returned by train will be swapped. Importantly the ROC score is independent of the ordering of factor levels. ### We need to tell train that we want to fit a glm and also specify the family logistic_cv &lt;- train(type~., data = Pima.tr, method = &quot;glm&quot;, family = binomial(link = &quot;logit&quot;), trControl = trControl, metric = &quot;ROC&quot;) logistic_cv$results ## parameter ROC Sens Spec ROCSD SensSD SpecSD ## 1 none 0.8323129 0.8796703 0.5404762 0.08823444 0.107995 0.1937711 We can see that the estimated ROC is actually below what we saw on the test data set. This could be a result of bias, since our training sets used in cross validation are slightly smaller than the complete training set, but can also just be a result of the particular training/test split of the overall data set. Regularisation in GLMs Hopefully it is unsurprising that the same techniques we used in the linear regression framework for regularising estimation of the regression coefficients, are applicable here as well. Perhaps some of you had already expected that the package name glmnet refers to the fact that more than just the standard linear model framework is relevant. Lets fit a LASSO logistic regression model to the Pima.tr data set, and perform cross validation to select an appropriate value for \\(\\lambda\\). We could use either glmnet or caret, and we will use both for illustrative purposes. We can tell cv.glmnet to use the ROC for selection by setting type.measure = \"auc\" ### As always we first load the library/ies we need library(caret) library(glmnet) ### With glmnet recall that we do not provide a formula, but a matrix x ### and vector y. Pima.tr.x &lt;- as.matrix(Pima.tr[,names(Pima.tr)!=&#39;type&#39;]) Pima.tr.y &lt;- Pima.tr$type logistic_lasso_cv_glmnet &lt;- cv.glmnet(Pima.tr.x, Pima.tr.y, family = &quot;binomial&quot;, alpha = 1, type.measure = &quot;auc&quot;) ### With caret, by default both lambda and alpha (the elastic net) will be ### tuned. We can specify only to consider alpha = 1 (the LASSO) by choosing ### our own &quot;tuning grid&quot; as follows tune_grid &lt;- expand.grid(alpha = 1, lambda = 2^(-10:10)) logistic_lasso_cv_caret &lt;- train(type~., data = Pima.tr, method = &quot;glmnet&quot;, family = &quot;binomial&quot;, trControl = trControl, tuneGrid = tune_grid, metric = &quot;ROC&quot;) Although the precise values of \\(\\lambda\\) used in each of the implementations will be slightly different, and also the cross validation folds will almost certainly differ because of the random splitting, we should expect fairly similar values to have been selected. logistic_lasso_cv_glmnet$lambda.min ## [1] 0.01528595 logistic_lasso_cv_caret$bestTune$lambda ## [1] 0.015625 We can also inspect the coefficients in the final fitted models: ### For glmnet we have already seen how to extract the coefficients coef(logistic_lasso_cv_glmnet, s = &quot;lambda.min&quot;) ## 8 x 1 sparse Matrix of class &quot;dgCMatrix&quot; ## lambda.min ## (Intercept) -8.36944839 ## npreg 0.07721663 ## glu 0.02800434 ## bp . ## skin . ## bmi 0.06236416 ## ped 1.35098882 ## age 0.03425585 ### For caret the final fitted model is stored in the field $finalModel coef(logistic_lasso_cv_caret$finalModel, s = logistic_lasso_cv_caret$bestTune$lambda) ## 8 x 1 sparse Matrix of class &quot;dgCMatrix&quot; ## s=0.015625 ## (Intercept) -8.33928046 ## npreg 0.07670067 ## glu 0.02793236 ## bp . ## skin . ## bmi 0.06202888 ## ped 1.34212777 ## age 0.03415721 Both models included/excluded the same variables, and if we recall the significant variables in the original fit all are included as well as npreg (the number of pregnancies had). Finally we can compare the performance on the test data using roc as we did previously. ### glmnet performance Pima.te.x &lt;- as.matrix(Pima.te[,names(Pima.te)!=&#39;type&#39;]) glmnet_predictions &lt;- predict(logistic_lasso_cv_glmnet, Pima.te.x, type = &quot;response&quot;, s = &quot;lambda.min&quot;) roc_glmnet &lt;- roc(Pima.te$type, glmnet_predictions) ## Setting levels: control = No, case = Yes ## Setting direction: controls &lt; cases roc_glmnet ## ## Call: ## roc.default(response = Pima.te$type, predictor = glmnet_predictions) ## ## Data: glmnet_predictions in 223 controls (Pima.te$type No) &lt; 109 cases (Pima.te$type Yes). ## Area under the curve: 0.866 ### caret performance. Note that logistic regression prediction in caret does ### not take type = &quot;response&quot; but rather type = &quot;prob&quot;. It also will ### return a matrix with probabilities for all classes caret_predictions &lt;- predict(logistic_lasso_cv_caret, Pima.te, type = &quot;prob&quot;)[,1] roc_caret &lt;- roc(Pima.te$type, caret_predictions) ## Setting levels: control = No, case = Yes ## Setting direction: controls &gt; cases roc_caret ## ## Call: ## roc.default(response = Pima.te$type, predictor = caret_predictions) ## ## Data: caret_predictions in 223 controls (Pima.te$type No) &gt; 109 cases (Pima.te$type Yes). ## Area under the curve: 0.8662 Probably unsurprisingly the performance of both models is very similar, and also very similar to the original fitted model. As we will see in the exercises, however, when the number of variables is considerably larger the benefits of regularisation really begin to show. 7.2.2 Multiclass Classification Logistic regression is a binary classification model, and does not directly allow us to handle problems with more than two classes. As mentioned previously, however, one can combine multiple binary classification models in order to perform multiclass classification. In fact there are multiple ways in which this can be done. Here we discuss a probabilistic approach which relies on the independence of irrelevant alternatives (IIA) assumption. This assumption essentially says that for any three classes, lets call them A, B and C for now, the odds of seeing class A over B is independent of whether or not C is also an option. Lets consider the example of travelling to university. The odds that someone walks to university tomorrow instead of cycling, i.e. \\(P(walk)/P(cycle)\\), should be the same whether or not taking the bus is also an option. The usefulness of this assumption is that if it holds for all classes we can fit models which estimate \\(P(walk)/P(cycle)\\) and \\(P(bus)/P(cycle)\\) and use these to find \\(P(bus)/P(walk) = (P(bus)/P(cycle))/(P(walk)/P(cycle))\\), and using the fact that all probabilities must sum to one we can also evaluate \\(P(walk), P(cycle)\\) and \\(P(bus)\\). In multinomial regression in general we have a total of \\(K\\) classes, say \\(1, 2, ..., K\\). One of these is set to the reference class (lets say \\(K\\) is chosen as the reference class for this multiclass situation), and then a logistic regression model is fit comparing each of classes \\(1, 2, ..., K-1\\) with class \\(K\\). Using a similar notation from before, lets suppose that \\(q_i(X) = P(Y = i|X, Y \\in \\{i, K\\})\\), i.e. the probability of class \\(i\\) instead of class \\(k\\), given \\(X\\). The logistic regression model comparing class \\(i\\) with class \\(K\\) will provide an estimate for \\(q_i(X)\\), say \\(\\hat q_i(X)\\). Very importantly \\(\\hat q_i(X)\\) is not an estimate for \\(P(Y = i|X)\\) since only classes \\(i\\) and \\(K\\) were available as options. However, the independence of irrelevant alternatives allows us to resolve the issue of how to combine these since they tell us how much more/less preferable each class is than class \\(K\\). We also know that by definition \\(q_K(X) = 1\\). Combining this with the fact that all probabilities must sum to one, we have \\[\\begin{align*} q_i(X) &amp;= P(Y=i|X, Y\\in\\{i, K\\}) = \\frac{P(Y = i|X)}{P(Y=i|X)+P(Y=K|X)}\\\\ \\Rightarrow P(Y=i|X) &amp;= \\frac{q_i(X)}{1-q_i(X)}P(Y=K|X)\\\\ \\Rightarrow 1 &amp;= P(Y=K|X)\\left(1 + \\sum_{j=1}^{K-1}\\frac{q_j(X)}{1-q_j(X)}\\right)\\\\ \\Rightarrow P(Y=K|X) &amp;= \\frac{1}{1 + \\sum_{j=1}^{K-1}\\frac{q_j(X)}{1-q_j(X)}},\\\\ P(Y = i|X) &amp;= \\frac{q_i(X)}{(1-q_i(X))\\left(1 + \\sum_{j=1}^{K-1}\\frac{q_j(X)}{1-q_j(X)}\\right)}, i \\not = K. \\end{align*}\\] Recall also that the logarithms of the quantities \\(q_i(X)/(1-q_i(X))\\) are just the linear components of each of the logistic regression models. (Another) Aside on Assumptions Although IIA is stated as an assumption, practically we can still fit a multinomial regression model even if the assumption is not valid. It will still produce predictions. However, whether or not we should trust the probabilities as reflecting the actual likelihood of each class is called into question if the IIA may not hold. Multinomial Regression in R It is hopefully apparent that one could relatively easily use the function glm(family = \"binomial\") multiple times in order to fit a multinomial regression model. However, this would require us to code-up all the necessary splitting of the data into different subsets to fit all the \\(K-1\\) models, and then carefully managing the predictions from all of these models to produce a final prediction. Fortunately the glmnet package has the family = multinomial option already implemented for us. However it is worth pointing out that how this is implemented is slightly different, since a model is fit for every class and not for all but the reference class. Since the models are fit with regularisation, the reference class can be favoured/unfavoured somewhat arbitrarily. The glmnet package resolves this issue by estimating a full \\(K\\) binary classification models, under constraints which ensure the probabilities sum to one. We will use the satimage data set, which can be loaded from the pmlbr package. This package does not actually store its data sets, but rather provides functionality for downloading them from the Penn Machine Learning Benchmarks repository [https://epistasislab.github.io/pmlb/]. Lets start by loading the package and fetching the data set. ### Loading library library(pmlbr) ### The fetch_data function will download and load data sets by name satimage &lt;- fetch_data(&quot;satimage&quot;) ## Download successful. ### All data sets loaded using pmlbr have the response variable named &quot;target&quot; table(satimage$target) ## ## 1 2 3 4 5 7 ## 1533 703 1358 626 707 1508 Now lets begin by splitting the data into training and test sets using carets createDataPartition ### The function createDataPartition requires the response variable ### and will split the data to approximately respect the class ### proportions. Let&#39;s select 70% of the data for training and ### model selection and leave 30% for testing. train_ix &lt;- createDataPartition(satimage$target, p = 0.7, list = FALSE) ### We can now index the satimage data set to produce train and test sets ### and since we will be using glmnet we explicitly produce the matrix ### of covariates and vector of responses satimage.tr.x &lt;- as.matrix(satimage[train_ix,names(satimage)!=&quot;target&quot;]) satimage.tr.y &lt;- as.factor(satimage$target[train_ix]) satimage.te.x &lt;- as.matrix(satimage[-train_ix,names(satimage)!=&quot;target&quot;]) satimage.te.y &lt;- as.factor(satimage$target[-train_ix]) glmnet_satimage &lt;- cv.glmnet(satimage.tr.x, satimage.tr.y, family = &quot;multinomial&quot;) confusionMatrix(as.factor(predict(glmnet_satimage, satimage.te.x, type = &quot;class&quot;)), satimage.te.y) ## Confusion Matrix and Statistics ## ## Reference ## Prediction 1 2 3 4 5 7 ## 1 451 0 6 5 13 0 ## 2 0 190 0 0 7 0 ## 3 8 0 375 43 0 14 ## 4 0 1 23 69 4 31 ## 5 6 14 1 1 155 14 ## 7 0 0 2 79 23 393 ## ## Overall Statistics ## ## Accuracy : 0.847 ## 95% CI : (0.8301, 0.8628) ## No Information Rate : 0.2412 ## P-Value [Acc &gt; NIR] : &lt; 2.2e-16 ## ## Kappa : 0.8097 ## ## Mcnemar&#39;s Test P-Value : NA ## ## Statistics by Class: ## ## Class: 1 Class: 2 Class: 3 Class: 4 Class: 5 Class: 7 ## Sensitivity 0.9699 0.92683 0.9214 0.35025 0.76733 0.8695 ## Specificity 0.9836 0.99594 0.9573 0.96592 0.97914 0.9295 ## Pos Pred Value 0.9495 0.96447 0.8523 0.53906 0.81152 0.7907 ## Neg Pred Value 0.9904 0.99133 0.9785 0.92889 0.97294 0.9588 ## Prevalence 0.2412 0.10633 0.2111 0.10218 0.10477 0.2344 ## Detection Rate 0.2339 0.09855 0.1945 0.03579 0.08039 0.2038 ## Detection Prevalence 0.2464 0.10218 0.2282 0.06639 0.09907 0.2578 ## Balanced Accuracy 0.9767 0.96138 0.9393 0.65808 0.87323 0.8995 7.2.3 Class Imbalance We briefly touched on the fact that sometimes we may see certain classification errors as more important than others, and also that classification accuracy may be totally misleading if one of the classes represents the vast majority of cases. Although we described some metrics which are more appropriate than classification accuracy when it comes to the assessment of a model, we have not yet considered how we actually go about improving these metrics in the context of class imbalance. Although some models are innately better at handling class imbalance than others, there are also some generic approaches which can be applied (almost) universally. 7.2.3.1 Case Weights When a model is fit by minimising the training error (perhaps with the addition of a penalty term for inducing regularisation), \\[\\begin{align*} \\hg = \\argmin_{g \\in \\F} \\frac{1}{n}\\sum_{i=1}^n L(y_i, g(\\x_i)) + P(g), \\end{align*}\\] where \\(P(g)\\) is just an arbitrary penalty term (which could be simply equal to zero if no regularisation is being applied), a straightforward approach for emphasising the relative importance of some observations (e.g. those in the minority class(es)) over others, is with the use of case weights (or simply weights) in the objecive function. Recall that when we introduced regularisation through penalisation, we thought of the optimisation above as placing some of its effort on minimising the training error and some of its effort ensuring the penalty term doesnt get too large. We could take this a step further and think of the above objective as \\(\\frac{1}{n} L(y_1, g(\\x_1)) + \\frac{1}{n} L(y_2, g(\\x_2)) + ... + \\frac{1}{n} L(y_n, g(\\x_n)) + P(g)\\), i.e. that an equal effort is placed on minimising each term in the training error and the rest of the effort is placed on the penalty term. But there is nothing saying we have to devote the same amount of effort to each term in the training error. We could instead have a vector of weights \\(\\mathbf{w} = (w_1, ..., w_n)^\\top\\), one for each observation in our training set, where we allocate larger weights to those observations we want to focus on more. Our fitted model is then given by \\[\\begin{align*} \\hg = \\argmin_{g \\in \\F} \\frac{1}{\\sum_{j=1}^n w_j}\\sum_{i=1}^n w_i L(y_i, g(\\x_i)) + P(g). \\end{align*}\\] In carets train function one can set the case weights using the argument weights. However, it is important to note that not all models implemented will allow this and so it is necessary to check the documentation [https://topepo.github.io/caret/train-models-by-tag.html#accepts-case-weights]. 7.2.3.2 Up/Downsampling Similar in spirit to setting the weights for the training observations is the process of either upsampling (i.e. replicating some of the) observations in the minority class(es) or downsampling (i.e. removing some of the) observations in the majority class(es). There are also combinations of up/downsampling as well as a number of methods which create artificial training cases in the minority classes by adding a small amount of noise to the upsampled points. 7.2.3.3 Changing Classification Threshold Although not all models directly allow for this, whenever a model produces a probability for membership to a class, like a logistic regression model, the threshold for classification can always be modified from the natural classify to the class with the highest probability approach. Example: Credit Default The Default data set in the ISLR2 package is a simulated data set containing pronounced class imbalance, and supposed to represent a typical (bank) credit default scenario. There are only three covariates, student: a factor variable stating whether the hypothetical individuals are students or not; balance: the average balance on the individuals credit card after monthly payment; and income: the individuals annual income. ### First load the package, which will automatically load the data set library(ISLR2) ### The data set has quite pronounced imbalance, with only about 3.3% in the minority class table(Default$default) ## ## No Yes ## 9667 333 As always we will start by splitting the data set into training and test sets. We will then fit logistic regression models as normal and then by setting the case weights. We will also explore modifying the threshold in the exercises. ### First we split the data and fit a baseline logistic regression model to ### the training set train_ix &lt;- createDataPartition(Default$default, p = 0.7, list = FALSE) Default.tr &lt;- Default[train_ix,] Default.te &lt;- Default[-train_ix,] logistic_default0 &lt;- glm(default~., Default.tr, family = &quot;binomial&quot;) ### We can now assess its test performance logistic_default0_preds &lt;- predict(logistic_default0, Default.te, type = &quot;response&quot;) logistic_default0_class &lt;- factor(logistic_default0_preds &gt;.5 , levels = c(TRUE, FALSE), labels = c(&quot;Yes&quot;, &quot;No&quot;)) ### Using the natural cutoff of 0.5 leads to poor sensitivity ### and not great balanced accuracy confusionMatrix(logistic_default0_class, Default.te$default, positive = &quot;Yes&quot;) ## Confusion Matrix and Statistics ## ## Reference ## Prediction No Yes ## No 2889 69 ## Yes 11 30 ## ## Accuracy : 0.9733 ## 95% CI : (0.9669, 0.9788) ## No Information Rate : 0.967 ## P-Value [Acc &gt; NIR] : 0.02642 ## ## Kappa : 0.4173 ## ## Mcnemar&#39;s Test P-Value : 1.856e-10 ## ## Sensitivity : 0.30303 ## Specificity : 0.99621 ## Pos Pred Value : 0.73171 ## Neg Pred Value : 0.97667 ## Prevalence : 0.03301 ## Detection Rate : 0.01000 ## Detection Prevalence : 0.01367 ## Balanced Accuracy : 0.64962 ## ## &#39;Positive&#39; Class : Yes ## ### A high AUC score suggests there is a cutoff which ### would give better balanced accuracy logistic_default0_roc &lt;- roc(Default.te$default, logistic_default0_preds) ## Setting levels: control = No, case = Yes ## Setting direction: controls &lt; cases logistic_default0_roc ## ## Call: ## roc.default(response = Default.te$default, predictor = logistic_default0_preds) ## ## Data: logistic_default0_preds in 2900 controls (Default.te$default No) &lt; 99 cases (Default.te$default Yes). ## Area under the curve: 0.9424 The fact that the AUC is high suggests, as mentioned above, that there is a cutoff at which the balanced accuracy would be substantially better. It is very important, however, not to use the test set in order to choose this threshold since this would constitute data leakage. One can, however, of course use the ROC curve from the training set in order to select a threshold and there are multiple criteria beyond balanced accuracy which combine both sensitivity and specificity. It should also be noted that choosing the threshold to optimise a certain criterion represents an additional model parameter, and so increases the complexity of the model. To see the effect of modifying the case weights, we will simply set the weights for points in each class inversely proportional to the size of the class. This has the effect that the effort in optimisation is applied equally to each class, as opposed to equally to each observation. ### Weights wts &lt;- numeric(nrow(Default.tr)) wts[Default.tr$default==&quot;Yes&quot;] &lt;- 1/sum(Default.tr$default==&quot;Yes&quot;) wts[Default.tr$default==&quot;No&quot;] &lt;- 1/sum(Default.tr$default==&quot;No&quot;) logistic_default_wt &lt;- glm(default~., Default.tr, family = &quot;binomial&quot;, weights = wts) logistic_default_wt_preds &lt;- predict(logistic_default_wt, Default.te, type = &quot;response&quot;) logistic_default_wt_class &lt;- factor(logistic_default_wt_preds &gt;.5 , levels = c(TRUE, FALSE), labels = c(&quot;Yes&quot;, &quot;No&quot;)) ### The sensitivity is now far superior to what it was when we didn&#39;t include ### case weights confusionMatrix(logistic_default_wt_class, Default.te$default, positive = &quot;Yes&quot;) ## Confusion Matrix and Statistics ## ## Reference ## Prediction No Yes ## No 2525 11 ## Yes 375 88 ## ## Accuracy : 0.8713 ## 95% CI : (0.8588, 0.8831) ## No Information Rate : 0.967 ## P-Value [Acc &gt; NIR] : 1 ## ## Kappa : 0.2737 ## ## Mcnemar&#39;s Test P-Value : &lt;2e-16 ## ## Sensitivity : 0.88889 ## Specificity : 0.87069 ## Pos Pred Value : 0.19006 ## Neg Pred Value : 0.99566 ## Prevalence : 0.03301 ## Detection Rate : 0.02934 ## Detection Prevalence : 0.15438 ## Balanced Accuracy : 0.87979 ## ## &#39;Positive&#39; Class : Yes ## ### The AUC score is very similar, however, and this is largely due to the fact ### that for a simple model like logistic regression changing the case weights ### will have a very similar effect to changing the threshold for classification ### and hence the entire ROC curve will be very similar to that from the ### previous model logistic_default_wt_roc &lt;- roc(Default.te$default, logistic_default_wt_preds) ## Setting levels: control = No, case = Yes ## Setting direction: controls &lt; cases logistic_default_wt_roc ## ## Call: ## roc.default(response = Default.te$default, predictor = logistic_default_wt_preds) ## ## Data: logistic_default_wt_preds in 2900 controls (Default.te$default No) &lt; 99 cases (Default.te$default Yes). ## Area under the curve: 0.942 7.3 Summary In this section we saw how the standard regression framework can be modified with the use of link functions in order to provide a general approach for predictive modelling, when treating the response as though \\(Y = g^*(X) + \\epsilon\\) is inappropriate we focused on the specific examples of logistic regression for binary classification and its extension multiclass classification with multinomial regression we explored numerous assessment criteria for classification models we considered how class imbalance can make detection of the minority classes challenging, and explored remedies such as case weights and up/down sampling 7.4 Exercises Download the Titanic.RData file from the R Scripts folder on Moodle. The objective is to predict whether individuals on the Titanic survived or not, based on age, sex and passengerClass. Start by splitting the data into \\(70\\%\\) training/selection and \\(30\\%\\) testing. The data contain missing entries and these will need to be imputed. You can simply impute before doing the train/test splitting but it is important to be aware of the subtleties here since generally we should only use an imputation model based on the training data to perform imputation on both training and testing sets. Ideally one should include imputation as part of the model fitting procedure, in that it should also respect the train/validation context (i.e. imputation on validation sets should not use validation data), however we will not go into any detail on this. Fit an appropriate generalised linear model in order to predict survival using the other variables. Provide an interpretation of the coefficient estimate associated with age. Report the performance on the test set as you think is most appropriate. Load the pmlbr library and then fetch the sonar data set using sonar &lt;- fetch_data(\"sonar\"). This is a binary classification problem to detect whether sonar signals have bounced off rock ($target = 1) or metal ($target = 0). The covariates correspond with the energy within different frequency bands, integrated over a fixed time interval, as a result there is a high degree of correlation between the covariates. Use five fold cross validation in order to select An appopriate value of \\(\\lambda\\) for use within a LASSO logistic regression model. Appropriate values of \\(\\lambda\\) and \\(\\alpha\\) for use within an elastic net logistic regression model. Plot and compare the estimated coefficient vectors from each of the selected models. Refer to the Default data set in the package ISLR2. The task is to select a threshold for classification when using a logistic regression model. Use 10 fold cross validation to estimate the balanced accuracy for each threshold in seq(0.1, 0.9, by = 0.1), and select the threshold which gives the highest balanced accuracy. You can use the solutions to the exercises from the previous chapter to help you implement your own cross validation, which may be the simplest way to do this. Start by setting aside \\(30\\%\\) of the data for testing, and perform cross validation on the remaining \\(70\\%\\). After selecting a threshold and obtaining a model trained on the entire \\(70\\%\\) not set aside for testing, compare the balanced accuracy on the test set of this model with that of the logistic regression model using the case weights as described in this section. "],["nonlinear1.html", "8 Nonlinearity Part I 8.1 Basis Expansions 8.2 Support Vector Methods 8.3 Summary", " 8 Nonlinearity Part I \\[ \\def\\hbeta{\\hat \\beta} \\def\\R{\\mathbb{R}} \\def\\I{\\mathbf{I}} \\def\\x{\\mathbf{x}} \\def\\Rr{\\mathbb{R}} \\newcommand{\\argmin}{\\mathop{\\rm argmin}} \\newcommand{\\argmax}{\\mathop{\\rm argmax}} \\def\\F{\\mathcal{F}} \\def\\hbbeta{\\hat{\\boldsymbol{\\beta}}} \\def\\bbeta{\\boldsymbol{\\beta}} \\def\\X{\\mathbf{X}} \\def\\y{\\mathbf{y}} \\def\\hg{\\hat g} \\def\\a{\\mathbf{a}} \\def\\K{\\mathbf{K}} \\def\\B{\\mathbf{B}} \\def\\b{\\mathbf{b}} \\def\\w{\\mathbf{w}} \\] In the last two chapters we looked in reasonable depth at linear and generalised linear models, in which the relationships between \\(Y\\) and \\(X = (X_1, X_2, ..., X_p)\\) are characterised only by a vector of regression coefficients, which concisely capture how changes in the predictors correspond to changes in the expectation of the response (possibly via a link function). This simple structure makes linear and generalised linear models highly interpretable, and their statistical properties well understood. However, many modern applications involve situations where far more flexibility is needed in order to accurately capture the relationships between \\(Y\\) and \\(X\\). In Chapters 4 and 5 we encountered a simple illustrative example approach by which non-linearity can be introduced in a principled manner, by means of polynomials. 8.1 Basis Expansions The polynomials we saw previously are an example of a basis expansion. In linear algebra we can think of a basis for a vector space as a set of vectors, say \\(\\mathcal{B}\\), with the property that every element of the vector space can be expressed as a linear combination of these basis vectors. The same idea can be applied to spaces of functions (like our set \\(\\F\\)). Specifically, suppose \\(\\{b_1, ..., b_q\\}\\) is a basis for \\(\\F\\) (the value \\(q\\) is the dimension of the function space). Then this means that every \\(g \\in \\F\\) can be written in the form \\[ g(\\x) = \\sum_{j=1}^q \\beta_j b_j(\\x), \\] for some coefficients \\(\\beta_1, ..., \\beta_q\\). Example: Polynomials For the particular case of the degree \\(d\\) polynomials in a single variable \\(x\\), the natural basis is \\(b_j(x) = x^j; j = 0, ..., d\\). If we want to describe degree \\(d\\) polynomials in more than one variable, say \\(p\\) of them, then the natural basis includes all functions of the form \\[ b(\\x) = \\prod_{j=1}^p x_j^{d_j}, \\] where the \\(d_j\\)s are natural numbers with \\(\\sum_{j=1}^p d_j \\leq d\\). For example the space of degree three polynomials in variables \\(x_1\\) and \\(x_2\\) are \\[ 1; \\ x_1; \\ x_2; \\ x_1^2; \\ x_2^2; \\ x_1x_2; \\ x_1^3; \\ x_2^3; \\ x_1^2x_2; \\ x_1x_2^2. \\] 8.1.1 Model Training Just as we described it for linear models and simple polynomials, since every function in \\(\\F\\) can be expressed as a linear combination of the basis functions, the same must be true of our fitted model, i.e. \\[ \\hg(\\x) = \\sum_{j=1}^q \\hbeta_j b_j(\\x), \\] where we have \\[ \\hbbeta = \\argmin_{\\bbeta \\in \\R^q} \\frac{1}{n}\\sum_{i=1}^n L\\left(y_i, \\sum_{j=1}^q \\beta_j b_j(\\x_i)\\right) + P(\\bbeta), \\] where \\(P(\\bbeta)\\) is here just some arbitrary regularisation penalty (which could be zero). This clearly has remarkable similarity with linear models, with the only difference ultimately being that instead of the terms \\(x_{ij}; i = 1, ..., n; j = 1, ..., p\\) we have the terms \\(b_j(\\x_i); i = 1, ..., n; j = 1, ..., q\\). In a practical setting we could therefore simply create the matrix \\(\\mathbf{B}\\) with \\(i,j\\)-th element \\(b_j(\\x_i)\\) and use this in place of the matrix \\(\\X\\) we had for linear models. Choosing a Basis For a given \\(\\F\\) it may not always be clear how to construct a basis. However, we dont need to approach the problem from this point of view. We can instead start by choosing our basis functions, in which case we will be implicitly choosing \\(\\F\\) as the set of all functions generated by this basis; i.e. all functions expressible as linear combinations of the basis functions. We could essentially choose almost any functions to include in a basis, as long as there isnt a lot of redundancy among them. The following example shows a simple simulated data set containing a single covariate \\(X\\) and response variable \\(Y\\) Lets consider two potential bases. The first row of plots below shows (i) the basis functions for the degree seven polynomials; (ii) these basis functions multiplied by the optimal coefficients; and (iii) the resulting fit to the observations. The second row shows the same plots but for a quadratic spline basis (dont worry, you dont need to know about splines, this is just another example). Both are able to fit very well to the data. Generally speaking polynomials are not a preferred basis largely due to issues of high variance and because their extrapolation (prediction beyond the range of the observations) is very poorly controlled. Splines typically will have much lower variance and their extrapolation can be even better controlled using what are known as natural splines where extrapolation is forced to be linear (again, dont worry about any details on splines, theyre just here as an alternative to ploynomials). However, we will focus on a special class of bases which we do not have to engage with explicitly. 8.1.2 Kernels One of the hallmarks of modern statistical learning is the remarkable capabilities of overparameterised models whose training has been appropriately regularised to avoid overfitting. What this essentially means is the use of extremely flexible model classes (\\(\\F\\)), with penalties on the training error to regularise the fitting process. In the present context this corresponds with extremely large bases (sometimes with infinitely many basis functions). However, it should be clear that from a practical standpoint actually applying the approach described previously would require the use of matrices \\(\\mathbf{B}\\), as before with \\(i,j\\)-th element equal to \\(b_j(\\x_i)\\), with extremely large numbers of columns. Even fitting simple linear models would then become computationally intractable. Moreover, when it comes to infinite bases we can only engage with such matrices conceptually, but clearly cannot actually evaluate them. Kernels are ways around this. In the context of basis expansions a kernel is a function \\(K\\), which takes two arguments \\(\\x, \\x&#39;\\) (these could be a pair of observations, for example) and returns a real number, and has the property that for any collection of points \\(\\x_1, ..., \\x_n\\) the matrix \\(\\K \\in \\R^{n \\times n}\\), with \\(i,j\\)-th element equal to \\(K(\\x_i, \\x_j)\\) is symmetric and positive semi-definite (dont worry about what this means if it is not something youre familiar with, as we wont be engaging much with the maths). Some popular examples are The (Gaussian) radial basis kernel; \\(K(\\x, \\x&#39;) = \\exp\\left(-||\\x - \\x&#39;||^2/\\sigma^2\\right)\\), where \\(\\sigma\\) is a hyperparameter of the kernel. Note that there are other parameterisations of the Gaussian kernel. The polynomial kernel(s); \\(K(\\x, \\x&#39;) = (c + \\x^\\top \\x&#39;)^d\\), where we now have two hyperparameters \\(c\\) and the degree \\(d\\). 8.1.2.1 The Kernel Trick To see what makes kernels special, we need to be aware of an important fact. Lets consider the ridge regression model, and for simplicity lets focus on fitting the model without an intercept (recall that we can center the observations first which allows us to account for the intercep indirectly). In this case we have \\(\\hbbeta = (\\X^\\top\\X + n \\lambda \\I)^{-1}\\X^\\top \\y\\), where \\(\\X\\) now does not contain the column of ones (these are what fits the intercept usually). To make a prediction from the resulting model, we have \\(\\hg(\\x) = \\hbeta_0 + \\sum_{j=1}^p \\hbeta_j\\x_j\\), where the intercept is not included in \\(\\hbbeta\\) as it was before since we are estimating it separately, which can also be written as \\(\\hbeta_0 + \\x^\\top \\hbbeta\\). Now, it turns out that we can write \\(\\x^\\top \\hbbeta\\) in a different form, as \\(\\sum_{i=1}^n a_i \\x_i^\\top \\x = \\a^\\top \\X\\x\\) where the vector \\(\\a = (a_1, ..., a_n)^\\top\\) is equal to \\((\\X\\X^\\top + n\\lambda \\I)^{-1}\\y\\). If we first converted our \\(\\X\\) to a \\(\\B\\), i.e. by applying all of the basis functions to each of our observations, then we would simply replace every instance of \\(\\X\\) with \\(\\B\\) and every instance of \\(\\x_i\\) or \\(\\x\\) with \\(\\b_i = (b_1(\\x_i), ..., b_q(\\x_i))^\\top\\) or \\(\\b = (b_1(\\x), ..., b_q(\\x))^\\top\\). Now, this may not look like any progress, however what is important is that with this formulation everything is expressed in terms of inner products between pairs of observations (the \\(i,j\\)-th element of \\(\\X\\X^\\top\\) is \\(\\x_i^\\top \\x_j\\)) and between the observations and the query point, \\(\\x\\). It is here that the kernel trick comes in. For any kernel, \\(K\\), there is an associated feature map, \\(\\phi_K\\), with the property that for every pair \\(\\x, \\x&#39;\\) we have \\(K(\\x, \\x&#39;) = \\phi_K(\\x)^\\top \\phi_K(\\x&#39;)\\). That is, evaluating the kernel on two points is equivalent to first mapping those points to the feature space and then taking the inner product between them. But this feature space can essentially just be thought of as the outputs of a collection of basis functions, i.e. we may think of \\(\\phi_K(\\x)^\\top \\phi_K(\\x&#39;)\\) as \\((b_1(\\x), ..., b_q(\\x))^\\top(b_1(\\x&#39;), ..., b_q(\\x&#39;))\\). The matrix \\(\\mathbf{K}\\), with \\(i,j\\)-th element \\(K(\\x_i, \\x_j)\\) is therefore analogous to the matrix \\(\\B\\B^\\top\\). But by using a kernel we dont ever have to evaluate the basis functions explicitly, since we only ever care about inner products between their outputs, and the kernel does all that behing the scenes. A Quick Summary Lets quickly take stock. We want to move beyond linearity, and have learned that one way to do this is to first transform the observations by applying a collection of (potentially) non-linear functions to them. After this, we can fit a linear model on the transformed data. If we want a high degree of flexibility this may require a large number of such non-linear basis functions, and a lot of computation to evaluate them all. A kernel is a way of bypassing a lot of this computation, since if we only ever need inner products between vectors of the basis functions evaluated on pairs of points then we get all the calculations done for us, simply by evaluating the kernel on the same pairs of points. So, if we wanted to fit a (kernel) ridge regression model for a given value of \\(\\lambda\\), we would do the following Compute the Gram matrix \\(\\K\\) (analogous to \\(\\X\\X^\\top\\)). Compute the vector \\(\\a = (\\K + n\\lambda \\I)^{-1}\\y\\) (analogous to \\((\\X\\X^\\top + n \\lambda \\I)^{-1}\\y\\)). To make a prediction, i.e. evaluate \\(\\hg(\\x)\\), we use \\(\\a^\\top(K(\\x,\\x_1), ..., K(\\x, \\x_n))^\\top\\) (analogous to \\(\\a^\\top\\X\\x\\)) Lets apply this to the simple simulated example from before. Note that in the above examples one of the basis functions was just a constant \\(1\\), which is how the intercept is fit. When using the Gaussian kernel it is more straightforward to first subtract the mean of the values of \\(Y\\) and then add these to the predictions afterwards ### First set up constants (we can try varying these below) lambda &lt;- 0.001 sigma &lt;- 0.15 ### Now construct the Gram matrix and the vector a ### The function dist() will compute all pairwise distances K &lt;- exp(-as.matrix(dist(x)^2)/sigma^2) a &lt;- solve(K + length(x)*lambda*diag(length(x)))%*%(y-mean(y)) # Note we fit to y-mean(y) here, and will add back mean(y) later ### Now let&#39;s get the predictions on a grid x_grid &lt;- seq(0, 1, length = 100) ### See if you can work out why the following is ### computing the pairwise (squared) distances ### between the observations and the grid. ### Note that this formulation will only work for ### a single covariate. dx_grid &lt;- (xs%*%t(rep(1, length(x))) - rep(1, length(xs))%*%t(x))^2 ### Now put these into the kernel and make predictions kK &lt;- exp(-dx_grid/sigma^2) yhat &lt;- kK%*%a + mean(y) # here is where we add back the mean of the y&#39;s plot(x, y, ylim = c(-1.5, 2), xlab = &quot;X&quot;, ylab = &quot;Y&quot;) lines(xs, yhat, col = 2, lwd = 2) We can see the model fits very nicely to the observations, and it may also be clear that the gradient is less steep at the edges of the plot which will typically lead to less problematic extrapolation. Now lets see the effect of changing the two hyperparameters, \\(\\sigma\\) and \\(\\lambda\\). It may already be clear to you what changing \\(\\lambda\\) will do, whereas the effect of \\(\\sigma\\) may not yet be clear. par(mfrow = c(2, 2)) ### Keeping gamma fixed, but changing lambda lambda1 &lt;- .01 lambda2 &lt;- .1 sigma &lt;- 0.15 ### The following is essentially as it was above K &lt;- exp(-as.matrix(dist(x)^2)/sigma^2) a1 &lt;- solve(K + length(x)*lambda1*diag(length(x)))%*%(y-mean(y)) kK &lt;- exp(-dx_grid/sigma^2) yhat &lt;- kK%*%a1 + mean(y) plot(x, y, ylim = c(-1.5, 2), xlab = &quot;X&quot;, ylab = &quot;Y&quot;, main = expression(sigma~&quot;= 0.15,&quot;~lambda~&quot;= 0.01&quot;)) lines(xs, yhat, col = 2, lwd = 2) ### Now for the other value of lambda a2 &lt;- solve(K + length(x)*lambda2*diag(length(x)))%*%(y-mean(y)) kK &lt;- exp(-dx_grid/sigma^2) yhat &lt;- kK%*%a2 + mean(y) plot(x, y, ylim = c(-1.5, 2), xlab = &quot;X&quot;, ylab = &quot;Y&quot;, main = expression(sigma~&quot;= 0.15,&quot;~lambda~&quot;= 0.1&quot;)) lines(xs, yhat, col = 2, lwd = 2) ################################################# ### Now keeping lambda fixed, but changing gamma lambda &lt;- .001 sigma1 &lt;- 0.05 sigma2 &lt;- 0.5 K1 &lt;- exp(-as.matrix(dist(x)^2)/sigma1^2) a1 &lt;- solve(K1 + length(x)*lambda*diag(length(x)))%*%(y-mean(y)) kK1 &lt;- exp(-dx_grid/sigma1^2) yhat &lt;- kK1%*%a1 + mean(y) plot(x, y, ylim = c(-1.5, 2), xlab = &quot;X&quot;, ylab = &quot;Y&quot;, main = expression(sigma~&quot;= 0.05,&quot;~lambda~&quot;= 0.001&quot;)) lines(xs, yhat, col = 2, lwd = 2) ### Now for the other value of sigma ### We have a different Gram matrix now since it depends on sigma K2 &lt;- exp(-as.matrix(dist(x)^2)/sigma2^2) a2 &lt;- solve(K2 + length(x)*lambda*diag(length(x)))%*%(y-mean(y)) kK2 &lt;- exp(-dx_grid/sigma2^2) yhat &lt;- kK2%*%a2 + mean(y) plot(x, y, ylim = c(-1.5, 2), xlab = &quot;X&quot;, ylab = &quot;Y&quot;, main = expression(sigma~&quot;= 0.5,&quot;~lambda~&quot;= 0.001&quot;)) lines(xs, yhat, col = 2, lwd = 2) The effects of changing each parameter may or may not be clear visibly. Increasing \\(\\lambda\\) will shrink the coefficients of the basis functions, and lead to less pronounced (lower amplitude) ups and downs, but the number of ups and downs will be the same (same frequncy). On the other hand, increasing \\(\\gamma\\) will lead to a more wiggly fit (higher frequency variation), as the fitted model is able to focus on more subtle, local variations in the data. On the other hand a smaller value of \\(\\gamma\\) has the opposite effect; generally fewer ups and downs (lower frequency variation), and a fitted function which captures the broader range shape of the data. Kernel Regression in R The caret package allows us to link to multiple kernel methods, including method = \"krlsRadial\" which aligns closely with the kernel ridge regression model described above. In addition, an excellent approximate method exists in the liquidSVM package (no longer on CRAN but available here). ### Start by loading the library (this may ask you to install ### a few other packages) library(caret) ### Let&#39;s do ten fold cross validation to select values of ### sigma and lambda trControl &lt;- trainControl(method = &quot;cv&quot;, number = 10) ### The krls method has an automatic way of selecting lambda ### but we can override this by specifying values in our ### tuning grid tuneGr &lt;- expand.grid(sigma = 2^(-4:2), lambda = c(2^(-6:0))) ### Now we can fit the model as we normally would. When running ### krls will print a lot to the console unless we set ### print.level = 0 model &lt;- train(y~x, data = data.frame(x, y), method = &quot;krlsRadial&quot;, trControl = trControl, tuneGrid = tuneGr, print.level = 0) ### Now we can check the selected hyperparameters model$bestTune ## lambda sigma ## 22 0.015625 0.5 ### ... and see how the fit looks by eye. Of course this is ### just a single covariate and otherwise we would need more ### abstract diagnostics to assess fit plot(x, y, xlab = &quot;X&quot;, ylab = &quot;Y&quot;) lines(x_grid, predict(model, data.frame(x = x_grid)), col = 2, lwd = 2) Note that when there are multiple covariates it is important to standardise them. Some kernel methods will have this built into how they are fit, but within caret you can always ensure this by setting train(..., preProcess = \"scale\"). In the above you will see that the value of sigma which is selected by cross validation is very large compared with the value which previously gave a pleasing fit by eye. This is because the krls method automatically standardises the variables before fitting. If we were to multiply the covariate values by some fixed constant then we should equally multiply the value of sigma by the same constant. Since the values of \\(X\\) are uniformly distributed on \\((0, 1)\\) they have standard deviation approximately \\(1/\\sqrt{12}\\), and so compared with how we had implemented it manually before, where we did not standardise \\(X\\), we should multiply the selected value of sigma by \\(1/\\sqrt{12}\\), giving us model$bestTune[\"sigma\"]/sqrt(12) = 0.1443376, which is close to the value of \\(0.15\\) we saw gave a good fit before. The Representer Theorem Although we saw explicitly how the solution arises for ridge regression, there is a theorem (the Representer Theorem) which says that the solution in the form \\(\\hg = \\a^\\top (K(\\x,\\x_1), ..., K(\\x, \\x_n))^\\top\\) holds for a very large collection of problems, and only the values in \\(\\a\\) differ across settings. Computational Complexity One of the major limitations of these kernel methods is that they become very computationally intensive for large \\(n\\) (scaling quadratically), except when there is only a single covariate. For more than about ten thousand observations it is common to use some sort of approximation methods. 8.2 Support Vector Methods There are many ways in which Support Vector Methods can be introduced, but one is that they are essentially regularised linear models (which, like other linear models we have seen can be applied on basis expansions or kernelised) with loss functions which are designed in such a way that many of the terms in the vector \\(\\a\\) (recall the formulation of the opimal solution and w.r.t. the Representer Theorem) are exactly zero. What this means is that these methods can be far more computationally efficient than methods like the kernel ridge regression model above (where \\(\\a\\) is dense). 8.2.1 Support Vector Classifiers Typically the term Support Vector Machine (SVM) applies to the classification model, which is similar to logistic regression with a ridge penalty except the loss function is given by \\[ L(y, \\hat y) = \\left\\{\\begin{array}{ll} \\max\\{0, 1-\\hat y\\}; &amp; y = 1\\\\ \\max\\{0, 1+\\hat y\\}; &amp; y = 0, \\end{array}\\right. \\] where \\(\\hat y\\) is the output from the linear (or kenelised) model, i.e. without any link function. Note that typically in the literature, when describing SVMs, the binary response is encoded as taking values in \\(\\{-1, 1\\}\\) instead of \\(\\{0, 1\\}\\). This allows for a more concise description of the loss function as \\(L(y, \\hat y) = \\max\\{0, 1-y \\hat y\\}\\). Also, instead of the parameters \\(\\beta_0, ..., \\beta_p\\), SVM is usually described in terms of \\(b, w_1, ..., w_p\\) where the \\(w_j\\)s are directly analogous to the \\(\\beta_j\\)s and \\(b = -\\beta_0\\), so that \\(\\hat y = \\w^\\top \\x - b\\). These details are unimportant for the purpose of this module, and what is important is the difference in the loss function and how this affects estimation and computation. To begin to understand the difference in the loss function, the following shows the shape of the loss function for a case with \\(Y = 1\\) (if \\(Y = 0\\) the loss would be mirrored in the vertical axis), with the corresponding loss function from logistic regression. ### Potential values of yhat yhat &lt;- seq(-3, 3, length = 500) ### SVM loss specifically for case where y = 1 loss_svm &lt;- (1-yhat)*(yhat&lt;1) ### logistic regression loss (from the negative log-likelihood) ### specifically for the case where y = 1 loss_logistic &lt;- log(1+exp(-yhat)) plot(yhat, loss_svm, type = &quot;l&quot;, xlab = expression(hat(y)), ylab = &quot;Loss&quot;, main = &quot;Loss functions for SVM and Logistic Regression for Y = 1&quot;) lines(yhat, loss_logistic, lty = 2) The way to intuit this plot is to recognise that for those cases with \\(Y = 1\\) we want to predictions (outputs of the linear/kernelised model) to be positive. The SVM loss (solid line above) simply says as long as the prediction is good enough (i.e. achieves a margin of at least 1 away from being misclassified) the loss is zero, and otherwise there is a loss which increases linearly. On the other hand, the logistic loss is non-zero everywhere, which means that no matter how far a prediction is from being incorrect the logistic regression model will still try to make it even more correct. Arguably this effort is wasted, and we should only focus on achieving predictions with a reasonable margin for error (since there is noise in the observations and other potential observations will only tend to be similar to those in the sample) and place all other emphasis on minimising the error on those which are actually being misclassified. In addition to this arguably justifiable motivation, it is precisely because the loss function will be exactly zero for many observations which makes the vector \\(\\a\\) sparse; only those points with non-zero error or a margin exactly equal to one will have non-zero entries in \\(\\a\\), these are the so-called support vectors. 8.2.1.1 Multi-class SVM It is only the logistic regression model, because of the interpretation in terms of the log-odds, which admits the multinomial solution for multiclass classification. With SVM it is necessary to use either the one-vs-one or one-vs-all approach, with the one-vs-one being the most popular. 8.2.1.2 SVM in R Unfortunately, as has been the case with a few other topics, different implementations of the same or similar methods are not always consistent with one another. As mentioned previously there are multiple parameterisations of the Gaussian kernel and the most straightforward implementation of SVM linked to with caret uses a different parameterisation from that linked to for kernel regression. In particular, when using train(method = \"svmRadial\"), the formulation of the Gaussian kernel is \\(K(\\x, \\x&#39;) = \\exp(-\\sigma ||\\x-\\x&#39;||^2)\\). As long as an appropriate range of values is considered, then cross validation will typically select a good value without you having to engage directly with this parameterisation, but it is nonetheless important to be aware of. Lets use the satimage data set again, which we initially saw with multinomial regression. We can fit both a linear SVM and a kernelised variant. ### load our library library(pmlbr) ### Fetch the satimage data set and convert response to a factor variable satimage &lt;- fetch_data(&quot;satimage&quot;) ## Download successful. satimage$target &lt;- as.factor(satimage$target) ### Create training/test split train_ix &lt;- createDataPartition(satimage$target, p = 0.7, list = FALSE) satimage.tr &lt;- satimage[train_ix,] satimage.te &lt;- satimage[-train_ix,] ### The default call to linear SVM will only use a single value of ### the regularisation parameter (here called C and ### acting similarly to 1/lambda). ### We can override this by specifying a tuning grid svm_lin &lt;- train(target~., data = satimage.tr, method = &quot;svmLinear&quot;, trControl = trainControl(&quot;cv&quot;, 10), tuneGrid = expand.grid(C = 2^(-5:5))) ### We can check the highest accuracy based on cross validation ### and the hyperparameter which led to this max(svm_lin$results[,&quot;Accuracy&quot;]) ## [1] 0.8773289 svm_lin$bestTune ## C ## 4 0.25 ### Finally we can assess the performance on the test data confusionMatrix(predict(svm_lin, satimage.te), satimage.te$target) ## Confusion Matrix and Statistics ## ## Reference ## Prediction 1 2 3 4 5 7 ## 1 452 2 2 3 12 0 ## 2 0 195 2 0 6 0 ## 3 4 0 395 49 0 7 ## 4 0 1 7 80 3 55 ## 5 3 12 0 6 175 20 ## 7 0 0 1 49 16 370 ## ## Overall Statistics ## ## Accuracy : 0.8651 ## 95% CI : (0.849, 0.88) ## No Information Rate : 0.2382 ## P-Value [Acc &gt; NIR] : &lt; 2.2e-16 ## ## Kappa : 0.8329 ## ## Mcnemar&#39;s Test P-Value : NA ## ## Statistics by Class: ## ## Class: 1 Class: 2 Class: 3 Class: 4 Class: 5 Class: 7 ## Sensitivity 0.9847 0.9286 0.9705 0.42781 0.82547 0.8186 ## Specificity 0.9871 0.9953 0.9605 0.96207 0.97609 0.9553 ## Pos Pred Value 0.9597 0.9606 0.8681 0.54795 0.81019 0.8486 ## Neg Pred Value 0.9952 0.9913 0.9918 0.93992 0.97838 0.9450 ## Prevalence 0.2382 0.1090 0.2112 0.09704 0.11002 0.2346 ## Detection Rate 0.2346 0.1012 0.2050 0.04152 0.09081 0.1920 ## Detection Prevalence 0.2444 0.1053 0.2361 0.07577 0.11209 0.2263 ## Balanced Accuracy 0.9859 0.9620 0.9655 0.69494 0.90078 0.8869 ### Similarly with the Gaussian kernel only a very limited tuning ### is done, and we will expand on this. Note that running the following ### will take quite a bit of time, and so if you are just playing around ### with this method it is advisable to use a smaller training set ### As an alternative consider exploring liquidSVM, which is approximate but ### much faster. It takes a little more work to apply, but not ### insurmountable svm_rbf &lt;- train(target~., data = satimage.tr, method = &quot;svmRadial&quot;, trControl = trainControl(&quot;cv&quot;, 10), tuneGrid = expand.grid(sigma=2^(-4:-1), C=2^(-3:3))) ### As before we can check the highest accuracy based on cross validation ### and the hyperparameters which led to this max(svm_rbf$results[,&quot;Accuracy&quot;]) ## [1] 0.9208083 svm_rbf$bestTune ## sigma C ## 20 0.25 4 ### and we can assess the performance on the test data confusionMatrix(predict(svm_rbf, satimage.te), satimage.te$target) ## Confusion Matrix and Statistics ## ## Reference ## Prediction 1 2 3 4 5 7 ## 1 448 0 3 0 4 0 ## 2 3 205 3 4 3 1 ## 3 5 0 397 42 1 14 ## 4 0 1 3 120 1 18 ## 5 3 3 0 1 197 16 ## 7 0 1 1 20 6 403 ## ## Overall Statistics ## ## Accuracy : 0.9185 ## 95% CI : (0.9054, 0.9304) ## No Information Rate : 0.2382 ## P-Value [Acc &gt; NIR] : &lt; 2.2e-16 ## ## Kappa : 0.8993 ## ## Mcnemar&#39;s Test P-Value : NA ## ## Statistics by Class: ## ## Class: 1 Class: 2 Class: 3 Class: 4 Class: 5 Class: 7 ## Sensitivity 0.9760 0.9762 0.9754 0.64171 0.9292 0.8916 ## Specificity 0.9952 0.9918 0.9592 0.98678 0.9866 0.9810 ## Pos Pred Value 0.9846 0.9361 0.8649 0.83916 0.8955 0.9350 ## Neg Pred Value 0.9925 0.9971 0.9932 0.96244 0.9912 0.9672 ## Prevalence 0.2382 0.1090 0.2112 0.09704 0.1100 0.2346 ## Detection Rate 0.2325 0.1064 0.2060 0.06227 0.1022 0.2091 ## Detection Prevalence 0.2361 0.1136 0.2382 0.07421 0.1142 0.2237 ## Balanced Accuracy 0.9856 0.9840 0.9673 0.81425 0.9579 0.9363 The kernelised variant svm_rbf achieves substantially higher performance, although it is noteworthy that the linear SVM achieves slightly better performance than the multinomial model we fit in Chapter 7, however this could be down to the difference in train/test splits. It is also the case that the kernelised model is considerably more computationally demanding, especially when we consider that multiple hyperparameters need to be tuned. From a prediction perspective it is arguably always the case that one should use the Gaussian kernel rather than the linear kernel since for very small \\(\\sigma\\) (with this parameterisation, or very large with the parameterisation described in relation to kernel ridge regression) the fitted model is very close to linear. 8.2.2 Support Vector Regression Before finishing this chapter we will briefly look at regression within the support vector framework. The typical loss function used in Support Vector Regression (SVR) is known as the \\(\\epsilon\\)-insensitive loss, and takes the form \\(L(y, \\hat y) = \\max\\{0, |y-\\hat y| - \\epsilon\\}\\). What this means is that if the prediction, \\(\\hat y\\), is within \\(\\epsilon\\) of the true value, \\(y\\), then the loss is zero, and otherwise the loss increases linearly with the difference between the two. ### Potential values for the estimated residual, y - yhat rhat &lt;- seq(-2, 2, length = 500) ### Let&#39;s plot the loss for two values of epsilon eps1 &lt;- 0.1 eps2 &lt;- 0.5 plot(rhat, (abs(rhat) &gt; eps1)*(abs(rhat)-eps1), type = &quot;l&quot;, xlab = expression(&quot;y - &quot;~hat(y)), ylab = &quot;Loss&quot;) lines(rhat, (abs(rhat) &gt; eps2)*(abs(rhat)-eps2), lty = 2) ### We can also add the more common loss functions: the absolute and squared error loss lines(rhat, abs(rhat), lty = 3) lines(rhat, rhat^2, lty = 4) In the above the \\(\\epsilon\\)-insensitive loss is shown for \\(\\epsilon = 0.1\\) (solid line) and \\(\\epsilon = 0.5\\) (long dashes). In addition the absolute loss (short dashes) and squared error (dashes and dots) are shown for comparison. The squared error loss is very heavily penalising of large errors, and this makes training using this loss function very sensitive to potential outliers. The absolute and \\(\\epsilon\\)-insensitive losses are far more robust. Similar to SVM above, it is only the observations which have non-zero loss or have a residual exactly equal to either \\(\\epsilon\\) or \\(-\\epsilon\\) which have non-zero entries in the vector \\(\\a\\). 8.2.2.1 Support Vector Regression in R Calling train(method = \"svmRadial\") (or method = \"svmLinear\") can be used for either classification or regression and caret will determine which depending on whether the response is numeric or categorical (factor). There are multiple versions of SVR, however by default caret will use the \\(\\epsilon\\)-insensitive loss. The actual value of \\(\\epsilon\\) is less important than the other hyperparameters, and in fact caret does not allow you to tune this parameter directly and to deviate from the default of epsilon = 0.1 one must specify this as train(method = \"smvRadial\", epsilon = eps). Lets look at the simple simulated data set from before. ### We&#39;ll use the same cross validation set up as before trControl &lt;- trainControl(method = &quot;cv&quot;, number = 10) ### Recall that sigma acts like the inverse square root compared ### with how it was applied in kernel ridge regression ### and C tuneGr &lt;- expand.grid(sigma = 2^(-4:4), C = c(2^(-4:4))) ### Now we can fit the model as we normally would. When running ### krls will print a lot to the console unless we set ### print.level = 0 model &lt;- train(y~x, data = data.frame(x, y), method = &quot;svmRadial&quot;, trControl = trControl, tuneGrid = tuneGr) ### Now we can check the selected hyperparameters model$bestTune ## sigma C ## 61 4 4 ### ... and see how the fit looks by eye. Of course this is ### just a single covariate and otherwise we would need more ### abstract diagnostics to assess fit plot(x, y, xlab = &quot;X&quot;, ylab = &quot;Y&quot;) lines(x_grid, predict(model, data.frame(x = x_grid)), col = 2, lwd = 2) 8.3 Summary Basis expansions are a natural way to introduce non-linearity into predictive models By first transforming the covariates by applying a collection of basis functions, a linear model is then applied to the transformed data This is equivalent to setting \\(\\hg = \\sum_{j=1}^q \\hat \\beta_j b_j\\), where the coefficients come from the linear model applied on the transformed covariates and \\(b_j\\) is the \\(j\\)-th basis function Kernels are an elegant way to shortcut much of the needed computation, as evaluating \\(K(\\x, \\x&#39;)\\) is equivalent to first transforming each of the arguments using all of the (potentially infinitely many) basis functions, and then taking their inner product Support vector methods are comparatively computationally efficient alternatives to the standard kernelised linear models In addition to being comparatively computationally efficient, support vector models are fairly robust (especially in the case of the regression models) In reading literature (and documentation of some software) you may find reference to least squares support vector methods; these are essentially the same as kernel ridge regression "],["nonlinearity2.html", "9 Nonlinearity Part II 9.1 Smoothing as Local Averaging 9.2 Nearest Neighbours 9.3 Decision Trees 9.4 Summary 9.5 Exercises", " 9 Nonlinearity Part II \\[ \\def\\R{\\mathcal{R}} \\def\\x{\\mathbf{x}} \\def\\hg{\\hat g} \\def\\w{\\mathbf{w}} \\] So far in all the models we have investigated there has been (at least implicitly) some parameterised structure. In the (generalised) linear models the regression, coefficients and the distribution family and link function used to model the response fully characterised the model(s). When we looked at basis expansions, even when we applied the kernel trick to bypass many of the calculations, within the feature space there was still the implicit setting of a set of optimal coefficients for the basis functions. In this chapter we look at an alternative way of fitting models using what is known as nonparametric smoothing. We then go on to look at a very important group of models known as Decision Trees (DTs), which may be seen as combining the optimisation approach used in parametric models with the local averaging approach of nonparametric smoothing. DTs are highly nonlinear models yet despite this they can be highly interpretable. In addition DTs are by far the most popular models used within ensemble predictive models, which we cover in the next chapter. 9.1 Smoothing as Local Averaging The appropriateness of estimating a function for prediction using a local average is most intuitively communicated from the point of view of regression. Recall that in the standard regression setting we have \\[ Y = g^*(X) + \\epsilon, \\] where the residual, \\(\\epsilon\\), has mean zero. Lets also assume that the function \\(g^*\\) is nicely behaved in that there is a constant \\(L\\) for which, given any two values \\(\\x_1, \\x_2\\) in the domain of \\(g^*\\), we have \\(|g^*(\\x_1)-g^*(\\x_2)| \\leq L d(\\x_1, \\x_2)\\) where \\(d(\\x_1,\\x_2) = \\sqrt{(\\x_1-\\x_2)^\\top (\\x_1-\\x_2)}\\). The quantity \\(d(\\x_1, \\x_2) = \\sqrt{(\\x_1-\\x_2)^\\top (\\x_1-\\x_2)}\\) may be seen as the distance between \\(\\x_1\\) and \\(\\x_2\\) and so this condition ultimately says that the gradient of \\(g^*\\) cannot exceed \\(L\\). A function with this condition is called Lipschitz continuous with Lipschitz constant \\(L\\) (but we dont need to know what its called for this module). Now lets suppose that we are interested in estimating \\(g^*\\) at a particular value for \\(X\\), say \\(\\x\\). Then lets suppose we look inside our data set and find all the observations \\((y_i, \\x_i)\\) for which \\(d(\\x, \\x_i)\\) is less than some value \\(h\\). We could then define our estimate for \\(g^*(\\x)\\) as the average value of the associated \\(y_i\\)s. That is, if \\(n_{\\x,h}\\) is the number of sample points within a distance \\(h\\) from \\(\\x\\) then we have \\[ \\hg(\\x):= \\frac{1}{n_{\\x,h}}\\sum_{i:d(\\x_i, \\x)&lt;h} y_i. \\] Why might this be a sensible thing to do? Lets start by looking at its mean and variance, and we will assume for simplicity that the sample is fixed and were looking at the conditional mean and variance, sort of like we did when looking at the theory for the linear model. If we revert to the random sample situation we would have \\[\\begin{align*} E[\\hg(\\x)] &amp;= \\frac{1}{n_{\\x,h}}\\sum_{i:d(\\x_i,\\x)&lt;h} E[Y_i]\\\\ &amp;= \\frac{1}{n_{\\x,h}}\\sum_{i:d(\\x_i,\\x)&lt;h}E[Y|X=\\x_i]\\\\ &amp;= \\frac{1}{n_{\\x,h}}\\sum_{i:d(\\x_i,\\x)&lt;h} g^*(\\x_i). \\end{align*}\\] But note that this average only includes the function values \\(g^*(\\x_i)\\) for those \\(\\x_i\\) which are close to \\(\\x\\), and we are assuming that the function doesnt change dramatically over any small ranges. In particular we have filtered our sample so that among those \\(\\x_i\\) included in the above average we know \\(|g^*(\\x_i)-g^*(\\x)| &lt; hL\\). This tells us immediately that the absolute value of the bias of \\(\\hg(\\x_i)\\) is no greater than \\(hL\\) (in fact typically in practice it will be considerably smaller than this). We also have, since each of the sample points is assumed to be independent, that \\[\\begin{align*} Var(\\hg(\\x)) &amp;= \\frac{1}{n_{\\x,h}^2}\\sum_{i:d(\\x_i,\\x)&lt;h} Var(Y_i)\\\\ &amp;= \\frac{\\sigma_{\\epsilon}^2}{n_{\\x,h}}. \\end{align*}\\] As always there is a tension between these two quantities. In order to decrease the variance we need to increase \\(n_{\\x, h}\\). In order to do this we need to increase \\(h\\) so that more points will be within a distance \\(h\\) from \\(\\x\\). This means we increase the bias since we need to look further away, and the function value may change more from \\(g^*(\\x)\\) (our target) if we include in our average values \\(g^*(\\x_i)\\) where \\(d(\\x, \\x_i)\\) is larger. Lets quickly look at a small example to see whats going on. The following will simulate \\(n=50\\) observations where \\(X\\) is uniformly distributed in \\((0, 1)\\) and \\(Y = \\sin(2\\pi X) +2X^2 +\\epsilon\\). We will use a setting of \\(h = 0.05\\) to see the effects on estimation of \\(g^*(X) = \\sin(2\\pi X) + 2X^*2\\). ### Set up constants n &lt;- 50 h &lt;- 0.05 sigma_resids &lt;- 1/5 x_grid &lt;- seq(0, 1, length = 1000) # values at which we will estimate the function g &lt;- function(x) sin(2*pi*x) + 2*x^2 ### Simulate observations x &lt;- runif(n) y &lt;- g(x) + rnorm(n, sd = sigma_resids) ### The object dmat contains zeroes and ones, with a one in row i column j ### if the i-th element in x_grid is within a distance h from the j-th ### observation of x dmat &lt;- 1*(abs(x_grid%*%t(rep(1, n))-rep(1, length(x_grid))%*%t(x)) &lt; h) ### See if you can understand why the evaluation of yhat has the correct form ### What does dmat%*%y evaluate to? yhat &lt;- (dmat%*%y)/rowSums(dmat) ### Now we plot the observations and the estimated function, as well as the true function plot(x, y) lines(x_grid, yhat, col = 2, lwd = 2) # fitted function lines(x_grid, g(x_grid)) # true function Run the above code multiple times. Change some of the settings, e.g. n and h, and see what happens to the fitted function. The fitted values using this simple smoothing technique behaves fairly erratically, but nonetheless is capable of fitting reasonably well to the data. However the filtering idea of including only those observations within a distance h from each point where we want to evaluate the function leads to a blocky appearance. We also face the difficulty of how to define \\(\\hg\\) if there are no points within a distance \\(h\\) from \\(\\x\\). More generally we can modify this estimation strategy using a weight function \\(\\w(\\x) = (w_1(\\x), w_2(\\x), ..., w_n(\\x))^\\top\\) which takes an input \\(\\x\\), and returns a vector of non-negative weights which determine the similarity between \\(\\x\\) and each of the observations. In the above example our weight function just had zeroes in the positions where the distance from \\(\\x\\) to an observation was greater than \\(h\\) and ones elsewhere. But we could instead use a smooth weight function which gives larger weights to observations close to \\(\\x\\) but more gradually decreases to zero for points further away. In the following we modify the dmat object to instead use a weight function with \\(i\\)-th entry \\(\\exp(-(\\x-\\x_i)^2/2h^2)\\), the Gaussian kernel: ### The &quot;smoother&quot; dmat dmat &lt;- exp(-(x_grid%*%t(rep(1, n))-rep(1, length(x_grid))%*%t(x))^2/2/h^2) ### Evaluating the fitted function uses the same idea. It is a weighted average of the ### observations of the response, placing more weight on those from points nearby yhat_new &lt;- (dmat%*%y)/rowSums(dmat) ### Now we plot the observations and the estimated function, as well as the true function plot(x, y) lines(x_grid, yhat, col = 2, lwd = 2) # old fitted function lines(x_grid, yhat_new, col = 4, lwd = 2) lines(x_grid, g(x_grid)) # true function The red line (initial estimate) and the blue line (using the Gaussian kernel) follow very similar trajectories, but the blue line is smoother. It also has the property that there will always be non-zero weights and so even if there are no points within a distance \\(h\\) from one of the evaluation points, there is no lack of definition. The tuning parameter \\(h\\), also called the bandwidth, is in a more general sense known as the smoothing parameter. Larger values of the smoothing parameter lead to overall smoother (less wiggly/erratic) fits, but at the expense of more bias. Choosing an appropriate smoothing parameter can be very challenging. Although in this simple univariate example we could get quite nice looking fits to the data with relatively little effort, the problem becomes far more difficult in multivariate settings (with multiple covariates) and when the sample becomes larger (for the resulting computational cost). 9.2 Nearest Neighbours The kernel smoothing described above is highly principled, but there are numerous challenges associated with the approach. As mentioned choosing the bandwidth parameter \\(h\\) is crucial to obtaining a good estimate, and because the computational cost of fitting kernel smoothing models is relatively high, performing cross validation may not be feasible. It may also be that a single value for \\(h\\) doesnt allow for a good fit over the whole function, especially when the density of observations is not uniform in \\(X\\). Nearest neighbours based estimators are arguably the most popular in the class of simple nonparametric smoothers. They are able to overcome some of the limitations of kernel smoothers: (i) they are practically much more computationally efficient, especially in dimensions between 2 and about 20 and (ii) they have an effectively adaptive smoothing effect, which varies depending on the density of the observations of \\(X\\). The weight function used in \\(k\\) Nearest Neighbours (\\(k\\)NN) simply has a one in the positions of the \\(k\\) nearest points to \\(\\x\\), and zero elsewhere. This non-smooth weight function also results in a blocky appearance, like the initial smoothing estimator described above, but has the property that where the observations of \\(X\\) are more dense, i.e. where there is more more information from the sample, the distance between \\(\\x\\) and the observations with non-zero weights will be smaller. Conversely where the observations of \\(X\\) are more sparse and there is less information, the \\(k\\)NN estimator will reach further for its neighbour points to account for this lack of information. The result is that the variance of the estimator is the same (\\(\\sigma^2_\\epsilon/k\\)) over the whole domain. There are multiple implementations of \\(k\\)NN in R, and we will simply use the method linked to by the caret package. ### Load the library library(caret) ### Fit the model knn_model &lt;- train(y~x, data = data.frame(x, y), method = &quot;knn&quot;, trControl = trainControl(&quot;none&quot;), tuneGrid = expand.grid(k=5)) plot(x, y) lines(x_grid, predict(knn_model, data.frame(x = x_grid)), col = 2, lwd = 2) lines(x_grid, g(x_grid)) 9.2.1 Classification with \\(k\\)NN The same ideas of local estimation are used when performing classification as regression, however unlike in a regression context we cannot simply take the average of the responses when they represent class labels If we had a problem where our classes were encoded \\(\\{cat = 1, dog = 2, goat = 3\\}\\) we shouldnt be thinking that the average of a cat and a goat is a dog (\\((1+3)/2 = 2\\)). Instead what we can do is compute the class proportions among the neighbours of an evaluation point \\(\\x\\). That is, suppose \\(\\{\\x_{(1)}, ..., \\x_{(k)}\\}\\) are the \\(k\\) nearest sample observations of \\(X\\) to the reference point \\(\\x\\). We then compute the proportions of \\(\\{y_{(1)}, ..., y_{(k)}\\}\\) which fall in each class. A final classification can then be achieved by taking the class with the highest proportion, or, as we described in Chapter 7, depending on the relative importance of some misclassifications over others we may wish to adjust the thresholds for classification depending on the problem context. Categorical Covariates and Scaling Nonparametric smoothing techniques rely on distances between observations, where typical distance metrics require only numerical data. Although there are some distance functions which can combine numeric and categorical variables a straightforward approach is to use dummy variables as we described in Chapter 4. Since the \\(k\\)NN model linked to in caret does not automatically handle this for us we need to explicitly do so. In addition, if variables are captured on extremely varying scales then those with larger scale will dominate the distances between points. In order to combat this it is preferable to scale the covariates when using purely distance based approaches. 9.2.2 Tuning \\(k\\)NN models in R As may have been very apparent already, we can tune/perform model selection for \\(k\\)NN models using the train function in caret. We will perform classification on the SAheart data set, which can be downloaded from &lt;https://hastie.su.domains/ElemStatLearn/datasets/SAheart.data&gt; directly from within R using the readr package. ### Load the libraries needed and then load the data library(readr) library(caret) SAheart &lt;- read_csv(&quot;https://hastie.su.domains/ElemStatLearn/datasets/SAheart.data&quot;, show_col_types = FALSE)[,-1] # remove first column as this is just a row index ### In order for train to perform classification we need to ensure ### the response is a factor variable. In addition, caret (as well ### as some other packages) do not like for us to use numeric values ### for the class labels/names. ### Also recall that caret treats the reference class differently ### from some other packages/functions. Specifically the outcome ### of interest and not the baseline must be set to the reference class ### otherwise we may have the sensitivity and specificity muddled ### As always, as long as you as the practicioner are aware how to ### interpret the results then it doesn&#39;t matter how R actually ### treats them y &lt;- relevel(factor(SAheart$chd, levels = c(0, 1), labels = c(&quot;Neg&quot;, &quot;Pos&quot;)), ref = &quot;Pos&quot;) ### The data contains one categorical variable (famhist) which has ### not already been converted to numeric. Note that sometimes ### data sets will have their categorical variables encoded with ### integers and it is important wherever possible to investigate ### what the variables actually represent to determine if any conversion is needed. ### In this instance all other categorical variables are binary and so it is not ### important. ### We can use the dummyVars function to encode our data using predict ### Remember not to include the response variable! X &lt;- predict(dummyVars(~., data = SAheart[,names(SAheart)!=&quot;chd&quot;]), newdata = SAheart[,names(SAheart)!=&quot;chd&quot;]) ### Now we can split our data into training and test splits and then perform ### cross validation for selection. train_ix &lt;- createDataPartition(y, p = 0.7, list = FALSE) X.tr &lt;- X[train_ix,] y.tr &lt;- y[train_ix] X.te &lt;- X[-train_ix,] y.te &lt;- y[-train_ix] ### Let&#39;s consider a number of neighbours from 1 to 20 tune_grid &lt;- expand.grid(k = 1:20) ### We will perform ten fold cross validation trControl &lt;- trainControl(method = &quot;cv&quot;, number = 10) ### As we described in a previous chapter all preprocessing should ideally be ### done within each train/validation step in cross validation, and this can ### be achieved with the argument preProcess. knn_model &lt;- train(y~., data = data.frame(X.tr, y=y.tr), method = &quot;knn&quot;, tuneGrid = tune_grid, trControl = trControl, preProcess = &quot;scale&quot;) knn_model$results ## k Accuracy Kappa AccuracySD KappaSD ## 1 1 0.6234069 0.1434737 0.07249592 0.1450507 ## 2 2 0.6303142 0.1585898 0.06991250 0.1733136 ## 3 3 0.6451983 0.1794026 0.09397408 0.2142529 ## 4 4 0.6448251 0.1825550 0.07905567 0.1885208 ## 5 5 0.6667892 0.2025897 0.07378304 0.1767374 ## 6 6 0.6602551 0.1870856 0.06952426 0.1650441 ## 7 7 0.6785428 0.2325305 0.07977068 0.1771769 ## 8 8 0.6692625 0.2102652 0.07396056 0.1528275 ## 9 9 0.6756072 0.2047206 0.06257057 0.1373077 ## 10 10 0.6948195 0.2409109 0.05067085 0.1429092 ## 11 11 0.6977551 0.2386893 0.06530966 0.1711290 ## 12 12 0.7197193 0.2986621 0.05845123 0.1726712 ## 13 13 0.7009748 0.2379184 0.03738369 0.1126380 ## 14 14 0.6977607 0.2420640 0.05470759 0.1519123 ## 15 15 0.7228498 0.2998751 0.08131876 0.2217990 ## 16 16 0.7353498 0.3271001 0.07238906 0.2063745 ## 17 17 0.7167892 0.2743918 0.07779734 0.2080527 ## 18 18 0.7323195 0.3213519 0.09045258 0.2327175 ## 19 19 0.7134748 0.2732200 0.07651079 0.1957107 ## 20 20 0.7166945 0.2697813 0.07401205 0.2007661 We can now inspect the performance of the model on the test data using the statistics from the confusion matrix confusionMatrix(predict(knn_model, X.te), y.te, positive = &quot;Pos&quot;) ## Confusion Matrix and Statistics ## ## Reference ## Prediction Pos Neg ## Pos 19 11 ## Neg 29 79 ## ## Accuracy : 0.7101 ## 95% CI : (0.6269, 0.7842) ## No Information Rate : 0.6522 ## P-Value [Acc &gt; NIR] : 0.08870 ## ## Kappa : 0.2998 ## ## Mcnemar&#39;s Test P-Value : 0.00719 ## ## Sensitivity : 0.3958 ## Specificity : 0.8778 ## Pos Pred Value : 0.6333 ## Neg Pred Value : 0.7315 ## Prevalence : 0.3478 ## Detection Rate : 0.1377 ## Detection Prevalence : 0.2174 ## Balanced Accuracy : 0.6368 ## ## &#39;Positive&#39; Class : Pos ## As is often the case with imblanced data where the outcome of interest is less prevalent than the baseline, the sensitivity is quite low. When fitting \\(k\\)NN models using case weights and/or upsampling the minority class is less effective than it is in models based on optimisation. In such cases one can consider changing the threshold for classification or using a different criterion for selecting a model. The latter can be achieved by designing ones own summaryFunction. 9.3 Decision Trees Decision trees also use the idea of a local average in order to fit predictive models, however instead of using a fixed smoothing parameter (like the \\(k\\) in \\(k\\)NN), they split up the input space into non-overlapping regions in a semi-optimal way; using the training data in order to choose how the splits are determined. They then use the averages from the training data in each of these regions in order to make predictions. It is certainly possible to have more complex models within each of the regions than just choosing the average value, but this is beyond the scope of the course The reason the models are referred to as decision trees, is that the partitioning of the covariate (input) space can be described in relation to a tree in the graph theory sense (dont worry, you dont have to know anything about graph theory). We can also think of them as trees in that, starting from a root node, observations are subjected to a rule/decision which results in a branching (splitting the observations based on the outcome of the rule/decision), after which the observations face another rule/decision which leads to further branching, and a further division of the input space, etc. The following figure (taken from Introduction to Statistical Learning, James et al.) shows (left) a division of the two-dimensional space in \\(X_1\\) and \\(X_2\\) into five non-overlapping rectangular regions; (middle) a pictoral representation of the decisions/rules which lead to this partition (like an upside-down tree, with the root node at the top and the branches heading downwards); (right) the corresponding fitted function where the vertical direction shows the values the function assumes in each of the five regions. If you choose a pair of values for \\(X_1\\) and \\(X_2\\) and then apply the rules in the tree in the middle figure above (starting from the top), following the left branch if the result of applying the rule is true and the right branch if it is false, until you reach one of the terminal nodes (called leaves), you will see that the tree representation agrees with the flat representation of the different regions in the left figure. Notice that the rectangular regions into which a decision tree partitions the input space have their sides parallel with the variable axes. The reasons for this, as opposed to allowing diagonal splits, are (i) it is much more computationally efficient to fit trees in this way even if it may not lead to as good a fit to the data and (ii) the interpretation of the outputs of decision trees is made far simpler when each of the decisions/rules is based only on one of the variables. 9.3.1 Fitting and Pruning Decision Trees As mentioned previously decision trees are fit in a semi-optimal manner. What this means is that the pairs of variables and split points which determine the different regions, are chosen in order to minimise a loss function. However the final fitted model is very far from guaranteed to contain the best possible splitting of the input space even if we are restricted to axis-parallel rectangles. The reason for this is that the Classification And Regression Trees (CART) algorithm uses a greedy optimisation approach in which the rules in the tree are determined sequentially, and once a rule/split is added it cannot be removed. That is, first the pair of covariate and split point for the root node is chosen, and then it is kept fixed as the next splits are chosen, which are then fixed, and the next are chosen, etc. Lets suppose we have the regions in a tree denoted as \\(\\R_1, ..., \\R_R\\), and for any potential vector of covariates, \\(\\x\\), lets write \\(\\R(\\x)\\) to be the region into which \\(\\x\\) falls. Since there is a single fixed value predicted in each region (determined from the averages of the responses from the points falling in these regions), we can write the training error as \\[ \\frac{1}{n}\\sum_{i=1}^n L(y_i, \\hat y_{\\R(\\x_i)}), \\] where \\(\\hat y_{\\R}\\) is the fitted value in region \\(\\R\\). But since \\(\\hat y_{\\R(\\x)}\\) is the same for all \\(\\x \\in \\R\\) we can also write this as \\[ \\frac{1}{n}\\sum_{r=1}^R \\sum_{i:\\x_i \\in \\R_r} L(y_i, \\hat y_{\\R_r}). \\] In other words the training error can be split into the errors/losses from each of the regions/leaf nodes in the tree. The total loss from a leaf node is often referred to as the impurity of the node. When choosing which split to add next during the sequential fitting of a tree, then, one just needs to find the region whose impurity can be improved the most by splitting it into two new regions. It should be clear that as more and more splits/regions are added to a tree, the resulting model will be able to fit better and better to the training data. Indeed if eventually every single point is in its own leaf node, then the training error will be zero. However, we know that fitting too well to the training data will very likely lead to overfitting and poor generalisation performance. The simplest approach for limiting the complexity of a decision tree is simply to terminate the sequential splitting either when a maximum number of leaf nodes is reached, or the depth (the maximum number of rules/splits taken from the root node to any leaf node) reaches some chosen maximum. Given our understanding of how regularisation can be used to fit models with good generalisation, an alternative is to use a penalised objective function \\[ \\frac{1}{n}\\sum_{r=1}^R \\sum_{i:\\x_i \\in \\R_r} L(y_i, \\hat y_{\\R_r}) + \\lambda R. \\] In the above \\(R\\) is simply the number of splits/leaves/regions, and the penalty for increasing the complexity of the tree by adding an additional split is fixed equal to \\(\\lambda\\). That is, there is no fixed maximum number of regions, and new regions can be added provided they improve the fit (i.e. reduce the training error) by at least \\(\\lambda\\). But here we reach a potential problem which comes about as a result of the semi-optimal manner in which trees are fit. What if by adding a not-so-great split fairly high up in the tree one is able to find a subsequent split which massively improves the overall fit? It is certainly possible that the high quality second split is not possible until the not-so-great split is added. A way around this is known as pruning. First a very deep/complex tree is fit, and then some of the branches are pruned away leaving a simpler tree in its place which gives a better penalised objective value. In this way it is possible to add the combination of a not-so-great split followed by a fantastic split instead of two mediocre splits (and, of course, more complex combinations of splits of varying quality). In the context of what is known as cost-complexity pruning the penalty parameter \\(\\lambda\\) is often referred to as the complexity parameter, and this can simply be chosen using our ubiquitous cross validation. 9.3.1.1 Regression Trees Describing regression trees, given the above and what we already know about the standard regression problem, is relatively straightforward. As before a natural loss function to use when fitting regression trees is the squared error loss function, and we also learned when looking at likelihood based estimation in generalised linear models that minimising the squared error is equivalent to maximum likelihood when the response is normally distributed, i.e. when \\(Y = g^*(X) + \\epsilon\\) where \\(\\epsilon \\sim N(0, \\sigma_{\\epsilon}^2)\\). The squared error loss function is also computationally preferred in the context of decision trees since when scanning over the potential splits along one of the covariate axes calculation of the total loss can be done recursively. Regression Trees in R The caret package can be used to fit (and tune) decision trees, however we will also be making use of the rpart package. We will also use the package rpart.plot which provides nice visualisations of fitted decision trees. Lets fit a regression tree which models the salaries of baseball players based on various career statistics, using the Hitters data set in the ISLR2 package. ### Start by loading the libraries we need library(ISLR2) library(rpart) ### Let&#39;s quickly inspect str(Hitters) ## &#39;data.frame&#39;: 322 obs. of 20 variables: ## $ AtBat : int 293 315 479 496 321 594 185 298 323 401 ... ## $ Hits : int 66 81 130 141 87 169 37 73 81 92 ... ## $ HmRun : int 1 7 18 20 10 4 1 0 6 17 ... ## $ Runs : int 30 24 66 65 39 74 23 24 26 49 ... ## $ RBI : int 29 38 72 78 42 51 8 24 32 66 ... ## $ Walks : int 14 39 76 37 30 35 21 7 8 65 ... ## $ Years : int 1 14 3 11 2 11 2 3 2 13 ... ## $ CAtBat : int 293 3449 1624 5628 396 4408 214 509 341 5206 ... ## $ CHits : int 66 835 457 1575 101 1133 42 108 86 1332 ... ## $ CHmRun : int 1 69 63 225 12 19 1 0 6 253 ... ## $ CRuns : int 30 321 224 828 48 501 30 41 32 784 ... ## $ CRBI : int 29 414 266 838 46 336 9 37 34 890 ... ## $ CWalks : int 14 375 263 354 33 194 24 12 8 866 ... ## $ League : Factor w/ 2 levels &quot;A&quot;,&quot;N&quot;: 1 2 1 2 2 1 2 1 2 1 ... ## $ Division : Factor w/ 2 levels &quot;E&quot;,&quot;W&quot;: 1 2 2 1 1 2 1 2 2 1 ... ## $ PutOuts : int 446 632 880 200 805 282 76 121 143 0 ... ## $ Assists : int 33 43 82 11 40 421 127 283 290 0 ... ## $ Errors : int 20 10 14 3 4 25 7 9 19 0 ... ## $ Salary : num NA 475 480 500 91.5 750 70 100 75 1100 ... ## $ NewLeague: Factor w/ 2 levels &quot;A&quot;,&quot;N&quot;: 1 2 1 2 2 1 1 1 2 1 ... Similar to the function train in caret the rpart function uses a control object which tells it how to perform fitting and pruning. For starters we will simply use a fixed complexity parameter (cp) and set the minimum number of observations allowed in any terminal node (minbucket). ### Set up the control object contr &lt;- rpart.control(minbucket = 10, cp = 0.001) ### Now we can fit our model, setting the seed to ensure ### reproducibility ### We will regress the log salary since the raw salary ### values are very heavily skewed set.seed(123) tree_model &lt;- rpart(log(Salary)~., data = Hitters, control = contr) By default when calling the rpart function cross validation is performed and the results for all complexity parameters greater than the value provided are stored. The cross validation results can be seen by using the function plotcp: plotcp(tree_model) The presentation of the results is slightly different from what we see from caret, where with rpart the Relative Error is shown representing the estimated prediction error relative to the model with no splits (i.e. one which simply uses the average in the entire data set). The horizontal dotted line also indicates the one standard error rule threshold. All relevant information is stored in the field $cptable, where the column xerror is the cross validation estimate of relative error and rel error is the training relative error: tree_model$cptable ## CP nsplit rel error xerror xstd ## 1 0.568937909 0 1.0000000 1.0072170 0.06567495 ## 2 0.061287729 1 0.4310621 0.4722220 0.05346809 ## 3 0.057784443 2 0.3697744 0.4657209 0.05537989 ## 4 0.030786188 3 0.3119899 0.3964910 0.06113039 ## 5 0.013096781 4 0.2812037 0.3513748 0.06089676 ## 6 0.011700767 5 0.2681069 0.3745273 0.05898447 ## 7 0.010933909 6 0.2564062 0.3691529 0.05819627 ## 8 0.009209713 7 0.2454723 0.3698248 0.05808903 ## 9 0.008216401 8 0.2362626 0.3736554 0.05929000 ## 10 0.005492546 9 0.2280462 0.3898444 0.06430554 ## 11 0.005158254 10 0.2225536 0.3828877 0.06503018 ## 12 0.003910817 11 0.2173954 0.3901558 0.06991258 ## 13 0.003753955 12 0.2134845 0.3914650 0.06991046 ## 14 0.003390561 13 0.2097306 0.3914650 0.06991046 ## 15 0.001000000 14 0.2063400 0.3974447 0.07122065 To extract a model for a different setting of cp after fitting a first tree with rpart one can use the function prune. For example, for the values of cp which minimise the estimated prediction error and using the 1 standard error rule ### Minimum cv error ix_min &lt;- which.min(tree_model$cptable[,&#39;xerror&#39;]) cp_min &lt;- tree_model$cptable[ix_min,&#39;CP&#39;] pruned_tree_min &lt;- prune(tree_model, cp = cp_min) ### One standard error rule ix_1se &lt;- min(which(tree_model$cptable[,&#39;xerror&#39;] &lt;= tree_model$cptable[ix_min,&#39;xerror&#39;] + tree_model$cptable[ix_min,&#39;xstd&#39;])) cp_1se &lt;- tree_model$cptable[ix_1se,&#39;CP&#39;] pruned_tree_1se &lt;- prune(tree_model, cp = cp_1se) Visualising and Interpreting Decision Trees One of the reasons decision trees are favoured by practicioners is the fact that they can very clearly and intuitively be visualised, provided they are not very complex. Lets investigate the trees we fit above to the Hitters data set using the prp function in the rpart.plot package. ### Load the package library(rpart.plot) ### The two trees selected using cross validation par(mfrow = c(1, 2)) prp(pruned_tree_min, main = &quot;Tree selected by minimum CV error&quot;) prp(pruned_tree_1se, main = &quot;Tree selected by 1 SE rule&quot;) It is hopefully not surprising that the model selected using the one standard error rule is simpler than the one which gave the lowest cross validation error estimate. Both trees clearly indicate how the predictions from the two models come about. For example, in the left tree we can see that the key determining factors (as captured by this model) in achieving the highest predicted salary (bottom right leaf of the tree) is to have variable CAtBat (number of times batting in entire career) at least 1452, Hits (total number of hits in the year 1986) at least 118, and CRBI (total number of runs scored in career) at least 273. None of the other variables affects this particular prediction, but the number of career hits CHits is an important variable for prediction when CAtBat is less than 1452. The fact that different variables become more/less important depening on splits higher up the tree mean that decision trees are extremely well suited to capturing complex interactions between covariates. However, as alluded to above, being able to interpret the outputs of a tree model relies on its not being too complex. If we consider the original tree we fit, we see something less appealing prp(tree_model, main = &quot;Tree with default cp = 0.001&quot;) Although with only 320 observations and a minimum node size of 10 one cannot ever reach extreme levels of complexity, already in this model interpretation becomes more challenging than in the pruned models. 9.3.1.2 Classification Trees The main ideas in fitting and pruning decision trees, whether they are used for regression or for classification, are the same. However, whereas in fitting regression trees the common squared error loss if used, it is common when fitting classification trees to use either the Gini coefficient or the cross-entropy as the measure of impurity which implicitly defines the loss function. This is not to say that using a likelihood based loss function is inappropriate, but these measures of impurity generally perform better on multiclass classification problems. Suppose that we have a total of \\(K\\) classes, and let \\(\\hat p_1, \\hat p_2, ..., \\hat p_K\\) be the proportions of the observations in a given node from each of the classes. These values therefore define a probability distribution over the classes, and both the Gini coefficient and cross-entropy can be seen as measures of how much uncertainty there is in this distribution. If all of the observations are in a single class (say class \\(k\\)) then we would have \\(\\hat p_k = 1\\) and \\(\\hat p_j = 0\\) for \\(j \\not = k\\). Such a distribution can be seen as having no uncertainty because all outcomes will be in class \\(k\\). On the other hand if all \\(\\hat p_k; k = 1, ..., K\\) are equal then there is a maximum level of uncertainty since each outcome is equally likely. Formally we have \\[ Gini(\\hat p_1, ..., \\hat p_K) = \\sum_{j=1}^K \\hat p_j (1- \\hat p_j), \\] and \\[ CrossEntropy(\\hat p_1, ..., \\hat p_K) = -\\sum_{j=1}^K \\hat p_j \\log(\\hat p_j), \\] and we set \\(0\\log(0) = 0\\). It should be pointed out, however, that these quantities are not directly dependent on the number of observations in the node and so to quantify the total impurity of a node the Gini coefficient/cross entropy is multiplied by the number of observations in the node. Classification Trees in R Within the rpart function we can minimise the Gini impurity by specifying parms = list(split = \"gini\") or the cross-entropy by specifying parms = list(split = \"information\"), where minimising cross-entropy is equivalent to maximising information gain. Lets revisit the satimage data set we saw with the multinomial regression model in Chapter 7. ### Loading library library(pmlbr) ### The fetch_data function will download and load data sets by name satimage &lt;- fetch_data(&quot;satimage&quot;) ## Download successful. ### All data sets loaded using pmlbr have the response variable named &quot;target&quot; table(satimage$target) ## ## 1 2 3 4 5 7 ## 1533 703 1358 626 707 1508 Now lets begin by splitting the data into training and test sets using carets createDataPartition function. ### We must ensure the response is a factor variable satimage$target &lt;- as.factor(satimage$target) ### Recall that createDataPartition requires the response variable ### and will split the data to approximately respect the class ### proportions train_ix &lt;- createDataPartition(satimage$target, p = 0.7, list = FALSE) ### We can now index the satimage data set to produce train and test sets satimage.tr &lt;- satimage[train_ix,] satimage.te &lt;- satimage[-train_ix,] ### Now let&#39;s set up the control object. As this is a much bigger data set then ### we had with the regression problem we include a larger minbucket contr &lt;- rpart.control(minbucket = 25, cp = 0.0001) ### Now we can fit our models, using both impurity measures tree_gini &lt;- rpart(target~., data = satimage.tr, control = contr, parms = list(split = &quot;gini&quot;)) tree_info &lt;- rpart(target~., data = satimage.tr, control = contr, parms = list(split = &quot;information&quot;)) ### In both cases the cross validation error continues to decrease with ### the depth of the tree. We can, however, compare the full models ### with one another as well as with the pruned trees based on the ### 1 standard error rule par(mfrow = c(1, 2)) plotcp(tree_gini, main = &quot;Gini Coefficient&quot;) plotcp(tree_info, main = &quot;Cross-Entropy&quot;) ### For the tree fit with the Gini coefficient ix_min &lt;- which.min(tree_gini$cptable[,&#39;xerror&#39;]) ix_1se &lt;- min(which(tree_gini$cptable[,&#39;xerror&#39;] &lt;= tree_gini$cptable[ix_min,&#39;xerror&#39;] + tree_gini$cptable[ix_min,&#39;xstd&#39;])) cp_1se &lt;- tree_gini$cptable[ix_1se,&#39;CP&#39;] pruned_gini &lt;- prune(tree_gini, cp = cp_1se) ### For the tree fit with the information gain/cross-entropy ix_min &lt;- which.min(tree_info$cptable[,&#39;xerror&#39;]) ix_1se &lt;- min(which(tree_info$cptable[,&#39;xerror&#39;] &lt;= tree_info$cptable[ix_min,&#39;xerror&#39;] + tree_info$cptable[ix_min,&#39;xstd&#39;])) cp_1se &lt;- tree_info$cptable[ix_1se,&#39;CP&#39;] pruned_info &lt;- prune(tree_info, cp = cp_1se) confusionMatrix(predict(tree_gini, satimage.te, type = &quot;class&quot;), satimage.te$target)$overall[&#39;Accuracy&#39;] ## Accuracy ## 0.8323819 confusionMatrix(predict(pruned_gini, satimage.te, type = &quot;class&quot;), satimage.te$target)$overall[&#39;Accuracy&#39;] ## Accuracy ## 0.8354956 confusionMatrix(predict(tree_info, satimage.te, type = &quot;class&quot;), satimage.te$target)$overall[&#39;Accuracy&#39;] ## Accuracy ## 0.8417229 confusionMatrix(predict(pruned_info, satimage.te, type = &quot;class&quot;), satimage.te$target)$overall[&#39;Accuracy&#39;] ## Accuracy ## 0.8375714 9.3.2 Some further Comments on Decision Trees Before we conclude we will briefly discuss a few more points which are relevant to decision trees. Handling categorical variables with dummy variables can be cumbersome. Decision trees are able to get around this by splitting on categorical variables directly; simply separating all observations in one category from the rest and choosing which category to isolate based on the improvement in the fit. Similar to how they handle categorical variables decision trees can also neatly handle missing values. How this is achieved is that whenever a variable is being screened for potential split points, at each split the tree can check whether placing all the NAs above/below the split gives the better fit. In this way the missing entries are actually informative rather than a nuisance. The discontinuous splitting of decision trees means they are extremely high variance estimators, even for a given level of bias. Even a few changes to observations on one of the covariates can completely change the structure of the tree if it disrupts one of the splits fairly high up. Although this is in general detrimental, as we see in the next chapter this is actually a beneficial thing when it comes to ensemble models. 9.4 Summary Nonparametric smoothing is an alternative approach for fitting non-linear models Unlike the basis expansion approach, which the covariates are transformed and then a linear model is fit to these transformations, nonparametric smoothing is based on the principle of a local average The intuition is that if the function we are estimating is continuous then small changes in the values of the covariates should correspond with small changes in the value of the function If we average the values of the response, but only after filtering the data to select the observations which are close to the point of interest (in terms of the values of the covariates), then there should not be much bias If we choose to filter more strictly, only including those very near to the point of interest, then there should be less bias, but there will be fewer points which are being averaged, leading to higher variance Decision trees are adaptive nonparametric smoothing models, since they choose how the filtering should be done based on an optimisation algorithm Because the algortihm is greedy we typically will not find the globally optimal solution, however the fit to the data will usually be better than the non-adaptive (or lazy) approach of something like \\(k\\) nearest neighbours Decision trees also have the advantage of being fairly interpretable provided they are not overly complex. In addition since they are not based on distances like other nonparametric smoothing models, we do not have to concern ouselves with scaling of the covariates, and decision trees also naturally handle categorical covariates and missing data. 9.5 Exercises Refer to the SAHeart data set from 9.2.2. The task is to use (five fold) cross validation to select both an appropriate value of \\(k\\) and an appropriate classification threshold to maximise balanced accuracy when using a \\(k\\)NN classifier. There is a number of ways in which this could be done, with the simplest being to use your own cross validation code and loop over both \\(k\\) and threshold. Note that predict(model, newdata, type = \"prob\"), where model is an output from train(method = \"knn\"), will provide the probabilities of classification to each class. As always start by setting aside \\(30\\%\\) of the data for testing, and perform cross validation on the remaining \\(70\\%\\). Once an appropriate setting for \\(k\\) and the classification threshold has been found, train the model on all \\(70\\%\\) and report its performance on the test data. Refer to the Pima data sets (Pima.tr, Pima.tr2 and Pima.te) in the MASS package, which we saw in Chapter 7. The difference between Pima.tr and Pima.tr2 is that the latter includes an additional 100 observations which include missing values. The purpose of this task is to investigate the relative performance of (i) complete cases analysis, i.e. only fitting a model on the observations which dont have any missing values; (ii) imputation, i.e. replacing missing entries with plausibe values so that observations with missingness can be treated as normal; and (iii) using decision trees native ability to handle missing data automatically. Fit three decision tree classifiers to predict type from all other variables in the Pima data sets. The first model should only use Pima.tr for training, while the other two should use Pima.tr2 with one first imputing the missing entries and the other not (rpart will automatically handle the missing entries, so no special instructions need to be given when fitting the model). For all models set the argument control to rpart.control(minbucket = 10, cp = 0) and then prune each based on cost-complexity pruning. Compare the performance of the three pruned models on the test set. The process of splitting the complete data set into Pima.tr/Pima.tr2 and Pima.te could obviously have been different, and because the overall data set is not very large these results may be subject to considerable randomness. Start by combining Pima.tr2 and Pima.te into Pima. Then perform the following 500 times: Recreate Pima.tr, Pima.tr2 and Pima.te from Pima in such a way that Pima.te contains 200 observations, and only select these test cases from among the observations/rows with no missing entries. Then fit the three models as in a. and store the results obtained on Pima.te. Report the distributions of results across the 500 repeats of the experiment as you feel is most appropriate. "],["ensemble-models.html", "10 Ensemble Models 10.1 Bagging 10.2 Boosting 10.3 Variable Importance 10.4 Summary 10.5 Exercises", " 10 Ensemble Models \\[ \\def\\x{\\mathbf{x}} \\def\\hg{\\hat g} \\def\\F{\\mathcal{F}} \\def\\bbeta{\\boldsymbol{\\beta}} \\newcommand{\\argmin}{\\mathop{\\rm argmin}} \\newcommand{\\argmax}{\\mathop{\\rm argmax}} \\] In this final chapter we will be looking at some of the most influential groups of models of the last twenty to thirty years. Ensemble models are seen by many as the state of the art when it comes to predictive modelling on so-called tabular data. Although neural nets have risen to prominence in problems involving unstructured data, such as textual data, as well as highly structured data, such as images, in the vector of covariates aligned with response variables (this is what we mean by tabular data) context ensemble models are very often the most performant. The basic idea behind ensemble modelling is to combine the outputs/predictions of multiple (sometimes a very large number of) weaker models (sometimes called base learners) in order to produce a final prediction. We will look at what are very likely the two most fundamental ensemble approaches, known as bagging (bootstrap aggregating) and boosting. Whereas bagging fits the base learners independently of one another, boosting is a sequential approach in which each base learner in the ensemble is fit to improve on what the ensemble comprising the base learners fit so far are predicting. Although ensemble models are heralded for their performance in terms of accuracy, there is a sacrifice on the side of interpretability. In addition because a large number of individual models needs to be fit for each ensemble they are considered computationally intensive methods. 10.1 Bagging As alluded to above bagging is a bootstrap based procedure, but unlike in the standard bootstrap procedure we saw before where we used the bootstrap in order to understand the sampling distribution of an estimator, bagging is used to actually construct the estimator itself. Returning to the notation we used in Chapter 5 where \\(\\hg^T\\) refers to a particular model trained on a training set \\(T\\), the main steps in a bagging procedure are: For \\(b = 1, 2, ..., B\\) do: Resample from the training set \\(T = \\{(y_1, \\x_1), ..., (y_n, \\x_n)\\}\\), to obtain a bootstrap sample \\(T_b\\) Fit a base learner \\(\\hg^{T_b}\\) on the bootstrap sample Combine the base learners for final prediction by defining \\[ \\hg^T_{bag}(\\x) = \\frac{1}{B}\\sum_{b=1}^B \\hg^{T_b}(\\x) \\] A Quick Aside In the above we have expressed the bagged prediction as an average of the predictions from the base learners. This formulation makes the most immediate sense for the regression setting, however in the classification setting we know it does not make sense to average class labels (recall that the average of a cat and a goat is not a dog!). We will come back to this point a little later on, but for now we will work with the averaging framework above. 10.1.1 Variance Reduction through Model Averaging The fundamental reason for performing bagging is that averaging reduces variance. We are well aware that the variance and bias of a model are ultimately what drive its success as a predictive model, and strategies for reducing the variance (regularisation strategies) can be very beneficial if the increase in bias is much less than the reduction in variance. So lets examine these a little, and how they compare with simply fitting our base learner to the entire training set to obtain \\(\\hg^T\\). It is relatively straightforward to show that the variance of the bagged prediction (as it is expressed above as an average) satisfies \\[ Var\\left(\\hg^T_{bag}(\\x)\\right) = Var\\left(\\hg^{T_1}(\\x)\\right)\\left(\\rho + \\frac{1-\\rho}{B}\\right), \\] where the quantity \\(\\rho\\) is the correlation between \\(\\hg^{T_1}(\\x)\\) and \\(\\hg^{T_2}(\\x)\\). Why have we used specifically bootstrap samples \\(T_1\\) and \\(T_2\\) in the above? Those who have done some statistics before may be familiar with the reason, and it is simply because the joint distribution of \\(\\hg^{T_i}\\) and \\(\\hg^{T_j}\\) for any \\(i \\not = j\\) is the same. This means that \\(Var(\\hg^{T_i}(\\x))\\) is equal for all \\(i\\) and \\(Cor(\\hg^{T_i}(\\x), \\hg^{T_j}(\\x))\\) is the same for all \\(i, j\\) as long as \\(i \\not = j\\). So the answer is that we could have used any pair, not necessarily the first and second bootstrap samples. Lets take a minute to unpack the expression above. The first thing to notice is that in principle we can always increase \\(B\\), and so the term \\((1-\\rho)/B\\) doesnt matter much except that for \\(B\\) to be very large it may take a lot of computational effort to fit so many models. From a statistical perspective the dominant term is \\(\\rho Var(\\hg^{T_1}(\\x))\\). Typically the variance of \\(\\hg^{T_1}(\\x)\\) will be higher than that of \\(\\hg^T(\\x)\\) since there is less information in a bootstrap sample than there is in the training set, as some of the observations will be repeated and not all observations will be present. The amount by which \\(Var(\\hg^{T_1}(\\x))\\) exceeds \\(Var(\\hg^T(\\x))\\) will depend on the base learner, but typically it is not a massive difference. What is more crucial is whether or not \\(\\rho\\) is close to one or whether there are base learners for which it is actually much smaller. Firstly, the reason why \\(\\rho\\) is not simply zero, is that pairs bootstrap samples overlap, and a rough estimate is that about two thirds of the information in a bootstrap sample will overlap with another. Herein lies the beauty of bagging. At a high level we may think of \\(\\rho\\) as capturing how similar the predictions tend to be from base learners fit to two bootstrap samples. Inflexible (low variance) models will typically be fairly stable, in that the two-thirds overlap in two bootstrap samples will make the predictions from the associated base learners quite similar (i.e. high correlation). It is actually the more flexible, high variance models, which typically have lower correlation because they fit closer to the specific variations in their samples, and so the one-third non-overlapping part actually influences the predictions from the model more. So the reduction in variance from bagging is typically more pronounced from particularly flexible, high variance models. What this means is that we can exploit the low bias, high flexibility models with relative impunity when using them as base learners within a bagged ensemble. We benefit from their low bias, but dont suffer so much from their high variance. The kernelised linear models (including SVMs) as well as nonparametric smoothing models (plus some models we have not covered in this module) can all be made extremely flexible, however decision trees have undeniably become the most successful models to use within a bagged ensemble, to the extent that bagging is often categorised as a decision tree based approach. 10.1.2 Why Bag Decision Trees? There is a number of reasons why decision trees have become almost ubiquitous within bagging. Flexibility: They are universal approximators meaning that with enough splits that can fit arbitrarily close to any function. Although there are other models which are also universal approximators, decision trees are often able to capture extremely complex interactions between covariates with comparative ease. Computation: Fitting decision trees is extremely computationally efficient, especially compared with other highly flexible models. This is very important since within bagged ensembles we typically use of the order of hundreds of base learners. Instability: Whereas the comparatively high variance of deep decision trees, for a similarly low bias to other flexible models, is usually a disadvantage, within the context of bagging it actually leads to lower correlation between the predictions from trees fit on bootstrapped samples and is overall usually an advantage in this context. 10.1.2.1 Bagging Classifiers When it comes to making predictions from a combination of regression models, taking the average is sensible. When predicting combining the outputs of classification models, however, taking the average of the predicted labels is clearly not sensible. Typically instead we use what is called majority voting, and assign the final predicted class label to be that which is predicted most often by the ensemble. Specifically, we have \\[ \\hg^T_{bag}(\\x) = \\argmax_k \\sum_{b=1}^B I(\\hg^{T_b}(\\x) = k), \\] where the function \\(I\\) is called the indicator function and takes the value one if its argument is true and zero otherwise. It is worth noting that the majority voting formulation can be expressed as an average, and so if you find the discussion related to the variance reduction from averaging particularly persuasive, you can rest assured it applies here too. 10.1.2.2 Out-Of-Bag (OOB) Error One of the additional benefits of bagging is that we get a few things for free. Recall that within each bootstrap sample there will be some observations which are excluded. In fact each observation is out of bag in roughly one third of the bootstrap samples. What this means is that for, say the \\(i\\)-th observation, we actually have an ensemble of roughly \\(B/3\\) models which were not trained using \\((y_i, \\x_i)\\) and so this point can act sort of like a validation point for this ensemble. Perhaps more precisely, for each observation we can obtain a prediction from an ensemble of roughly \\(B/3\\) models which didnt have access to the observation for training. By comparing the actual responses in the training data with these out-of-bag predictions gives a fairly reliable estimate of the generalisation performance, and can be used as a much more computationally efficient alternative to cross validation in bagging. Bagged Decision Trees in R The bagging function in the ipred package allows us to fit bagged predictive models within R. Although standard bagged tree models can be linked using caret, this particular implementation does not allow for hyperparameter tuning. Lets use the satimage data set again, for a comparison with the single decision trees we fit in the previous chapter. ### load our libraries library(ipred) library(pmlbr) library(caret) library(rpart) ### Fetch the satimage data set and convert response to a factor variable satimage &lt;- fetch_data(&quot;satimage&quot;) ## Download successful. satimage$target &lt;- as.factor(satimage$target) ### Create training/test split train_ix &lt;- createDataPartition(satimage$target, p = 0.7, list = FALSE) satimage.tr &lt;- satimage[train_ix,] satimage.te &lt;- satimage[-train_ix,] ### The bagging function links to rpart and so we need to provide ### an rpart.control object. Since we know that bagging can effectively ### &quot;take care of&quot; overfitting we can use very deep trees. For instance ### it is common to set the minbucket parameter to a value between 1 and ### 5 control &lt;- rpart.control(minbucket = 3, cp = 0) ### Now we can fit the bagged model ### The coob argument tells ipred whether or not ### we want to estimate the prediction error using ### out of bag error tree_bag &lt;- bagging(target~., satimage.tr, nbagg = 100, method = &quot;standard&quot;, coob = TRUE, control = control) ### Recall that the multinomial regression model and the single ### decision tree had accuracy approximately 0.85 confusionMatrix(predict(tree_bag, satimage.te), satimage.te$target)$overall[&#39;Accuracy&#39;] ## Accuracy ## 0.9050337 ### The bagged model achieves roughly 5% more correct classifications ### which is very far from negligible. Although this will depend on the ### specific train/test split, on a data set of this size there is ### little variation in the preformance across training/test ### splits. We can also check the out of bag error from the model ### to see if it accurately represents the test error ### Printing the tree_bag object will show the out of bag error tree_bag ## ## Bagging classification trees with 100 bootstrap replications ## ## Call: bagging.data.frame(formula = target ~ ., data = satimage.tr, ## nbagg = 100, method = &quot;standard&quot;, coob = TRUE, control = control) ## ## Out-of-bag estimate of misclassification error: 0.1009 # Indeed the OOB error approximately of 0.1 closely matches # the test error of 1-accuracy 10.1.3 Random Forests Random Forests (RFs) have ultimately eclipsed the standard bagged decision trees. RFs are themselves bagged ensembles of decision trees, however there is a slight difference in how each of the individual trees is fit. At a high level the steps taken to fit each tree are the same, in that the greedy optimisation of the CART algorithm we saw in Chapter 9 is applied. However, whenever a node in a tree is being split, rather than scanning all of the covariates to find the best split, only a random subset of the covariates is scanned. This randomisation can be seen as disrupting the way in which the trees are fit, and leads to less similarity (and hence correlation) between the trees fit on different bootstrap samples. Although this additional randomisation slightly impacts on the quality (bias and variance) of each of the trees, the reduction in the correlation typically overall improves performance, and sometimes substantially so. The hyperparameter used to specify the number of covariates available at each split in a tree is typically called mtry, and any standard RF implementation can be turned into a standard bagged ensemble of trees by simply setting mtry equal to the number of covariates. Because of their remarkable success, RFs have been implemented in a large number of R packages. The caret package alone links to a plethora of implementations, but we will be using the randomForestSRC package, as a computationally efficient and accurate option. ### Load the randomForestSRC library library(randomForestSRC) ### Let&#39;s simply use the default settings rf &lt;- rfsrc(target~., data = satimage.tr) ### The predict function applied to an rfsrc is a list ### with fields $predicted which for regression is the final ### prediction of the response and for classification ### contains the proportion of trees which &quot;voted&quot; for ### each of the classes. To obtain the class predictions ### directly use the field $class rf_pred &lt;- predict(rf, satimage.te)$class ### Now let&#39;s check the test accuracy confusionMatrix(rf_pred, satimage.te$target)$overall[&#39;Accuracy&#39;] ## Accuracy ## 0.9159315 ### To obtain the OOB error we can extract the OOB class predictions ### from the model and compare these with the actual class labels confusionMatrix(rf$class.oob, satimage.tr$target)$overall[&#39;Accuracy&#39;] ## Accuracy ## 0.9099379 10.2 Boosting Boosting, like bagging, is an ensemble approach in that model predictions are obtained by combining the predictions from multiple smaller models (base learners). Like bagging, boosting can be achieved using base learners other than decision trees, but decision trees have nonetheless become a very popular choice in this context. Unlike in bagging, however, in the case of boosting the individual base learners are fit sequentially, and use information about the others which have been fit so far. Because of this sequential learning, the base learner added at the \\(b\\)-th iteration can be fit specifically to improve the performance of the ensemble comprising the first \\(b-1\\). The earliest boosting methods, like the Adaptive Boosting (AdaBoost) algorithm, were based on adaptively modifying the case weights so that each base learner would focus more effort on those observations on which the rest of the ensemble was less accurate. Starting with all observations having the same weight, after each base learner is added to the ensemble the weights for those observations on which it is inaccurate are increased and those on which it is accurate are decreased. In this way the subsequent base learner will focus more on those observations where this base learner failed. However, more recently formulations of the sequential ensemble boosting framework have mainly been based on what is known as gradient boosting. 10.2.1 Gradient Boosting Gradient boosting is appealing for its universality, in that it can be applied for any differentiable loss function. Although it has been shown that the reweighting strategy of AdaBoost can be framed as a gradient boosting algorithm, the loss function in that case is not the most natural loss function for the classification problem for which it was designed. Before describing the general gradient boosting approach, we will start with the standard regression problem as this is the most immediately intuitive. Suppose, as always, that our sample is given by \\(\\{(y_1, \\x_1), ..., (y_n, \\x_n)\\}\\). Then, for an initial estimate for \\(g^*\\), say \\(\\hg\\) (typically we just set \\(\\hg(\\x) = \\bar y\\) for all \\(\\x\\)), and for a tuning parameter \\(0 &lt; \\eta &lt; 1\\), do the following for \\(b = 1, ..., B\\): Find the residuals from the current model: \\(r_i \\gets y_i - \\hg(\\x_i); i = 1, ..., n\\). Fit a base learner (e.g. a decision tree), say \\(\\hg^{b}\\), to predict the \\(r_i\\)s from the \\(\\x_i\\)s. That is we fit a regression model where the responses are replaced with the residuals \\(r_1, ..., r_n\\) Update the estimate of \\(g^*\\) by adding a shrunken version of \\(\\hg^{b}\\): \\(\\hg \\gets \\hg + \\eta\\hg^{b}\\) Why should an approach like this work? We can think of the residuals at each iteration simply as the part of the response which is not yet predicted by the ensemble. The next base learner is therefore focused specifically on predicting what the rest of the ensemble is not, and so whose addition will most improve the overall model. So, why shrink the effect of the base learners (with the parameter \\(\\eta\\))? The simple answer to this is to avoid overfitting. If the base learners are able to fit too well to the data (or the modified data using the residuals rather than the raw \\(Y\\) values), then we face the standard problem of focusing too heavily on the precise manifest data observations and not the general trend in the relationships. By limiting the influence of each of the base learners, through this shrinkage, the risk of overfitting is substantially reduced. Using a larger number of base learners, each with a small contribution to the overall prediction, can also have a variance reducing effect similar to that obtained when bagging. In fact many of the popular implementations of boosting seek to further leverage this effect by using decorrelation tricks, like sub-sampling observations and variables (as in random forests) when fitting each base learner. Nonetheless, it is important to appropriately tune the value of \\(\\eta\\), the total number of base learners \\(B\\), as well as any additional hyperparameters of the individual base learners. Importantly, because the base learners act collaboratively, each can be far simpler than when a single decision tree (or bagged ensemble/random forest) is used. Functional Gradient Descent The approach of having the base learners try to predict the residuals from the current model, at each iteration, is intuitive. However, it is only in regression where the notion of a residual has obvious meaning. Fortunately we can rely on the principles of optimisation, and specifically gradient descent, to extend this boosting procedure to more general settings. First lets quickly recall the ideas of gradient descent, and apply them to the problem of training a model. Lets suppose that the class of functions from which we are choosing our estimate for \\(g^*\\), \\(\\F\\), is a parametric class of functions, meaning that each \\(g \\in \\F\\) can be written using some general form \\(g_\\F\\) plus specific settings for parameters, \\(\\theta\\). For example, if \\(\\F\\) is the collection of all linear (affine) functions then we could have \\(\\theta = \\bbeta\\) and \\[ g_\\F(\\x|\\bbeta) = \\beta_0 + \\sum_{j=1}^p \\beta_j x_j. \\] We know that we want to minimise our training error, that is, to find \\[ \\hat \\theta = \\argmin_{\\theta} \\frac{1}{n}\\sum_{i=1}^n (y_i - g_\\F(\\x_i|\\theta))^2 \\equiv \\argmin_\\theta L_{train}\\left(g_\\F(\\cdot | \\theta)\\right). \\] To find the best \\(\\hat \\theta\\) using gradient descent, we would start with an initial estimate and then proceed by iteratively updating; setting \\[ \\hat \\theta \\gets \\hat \\theta - \\eta \\frac{\\partial}{\\partial \\theta}\\frac{1}{n}\\sum_{i=1}^n (y_i - g_\\F(\\x_i|\\hat \\theta))^2. \\] That is, with each iteration we shift the parameters a small amount (\\(\\eta\\)) in the direction which decreases the training error the most (i.e. in the direction of the negative gradient). This sequential updating is analogous to boosting, since the current value for \\(\\hat \\theta\\) is what has been achieved so far by previous iterations, and we are seeking to improve this incrementally. The idea of functional gradient descent is to imagine that every single value of the function is its own parameter. In other words we would start with an initial setting of \\(\\hg\\) and then iteratively update this by setting \\[ \\hg \\gets \\hg - \\eta \\frac{\\partial}{\\partial g}\\frac{1}{n}\\sum_{i=1}^n (y_i - \\hg(\\x_i))^2. \\] But notice that the training error \\(\\frac{1}{n}\\sum_{i=1}^n (y_i - \\hg(\\x_i))^2\\) depends on \\(\\hg\\) only through the values of \\(\\hg\\) evaluated at the \\(\\x_i\\)s. That is, the function \\(\\frac{\\partial L_{train}(\\hg)}{\\partial g}:= \\frac{\\partial}{\\partial g} \\frac{1}{n}\\sum_{i=1}^n (y_i - \\hg(\\x_i))^2\\) can be anything which satisfies \\(\\frac{\\partial L_{train}(\\hg)}{\\partial g}(\\x_i) = \\frac{1}{n}\\frac{\\partial}{\\partial \\hat y}(y_i - \\hat y)^2|_{\\hat y = \\hg(\\x_i)}\\) for all \\(i = 1, ..., n\\). But what is \\(\\frac{\\partial}{\\partial \\hat y}(y_i-\\hat y_i)^2\\)? It is simply equal to \\(-2(y_i - \\hat y_i) = -2 r_i\\), where \\(r_i\\) is the \\(i\\)-th residual. In other words, to update our model using functional gradient descent we could simplify this as \\[ \\hg \\gets \\hg + \\tilde \\eta \\tilde g, \\] where \\(\\tilde \\eta = \\frac{2}{n}\\eta\\) and \\(\\tilde g\\) is any function satisfying \\(\\tilde g(\\x_i) = r_i\\). Okay, so this is a lot to take in, so lets quickly take stock: We know that gradient descent can be used to perform optimisation We know that model training is an optimisation problem Although we have seen a lot about how only minimising the training error may not be a good idea, and that some form of regularisation may be needed to improve generalisation, lets park that point for now. We dont want to be restricted to parametric \\(\\F\\), and so we look at nonparametric gradient descent where we actually treat the function values themselves as though they are the parameters were shifting to improve our objective Then we see finally that actually doing this is the same as iteratively updating our estimate \\(\\hg\\) by adding a shrunken version of a function \\(\\tilde g\\) which predicts the current residuals. In other words, we found that the boosting idea we described earlier for regression is actually very closely connected to training a model by functional gradient descent. Now, many of you may be thinking how do we actually find \\(\\tilde g\\)?, and the truth is that we dont. Not only do we not, but actually it is beneficial only to use an approximation of \\(\\tilde g\\), as this this will reduce the risk of overfitting. Moreover, we have learnt a lot in this module about how to approximate (or estimate) functions using regression. So replacing \\(\\tilde g\\) with a regression function fit to predict the residuals, which is what we described initially in this section anyway, is likely preferable. So, more correctly than it is stated above, the regression boosting approach described at the start of this section may be seen as performing approximate functional gradient descent, based on minimising the regression training error. With this description it is far easier to generalise the procedure to other problems, since we could equally apply the same principles to any loss function, \\(L\\). The general gradient boosting method therefore works as follows: Starting with some initial model \\(\\hg\\), we do the following for \\(b = 1, ..., B\\): Find the current gradients of the loss: \\(\\delta_i \\gets \\frac{\\partial}{\\partial \\hat y} L(y_i, \\hat y)|_{\\hat y = \\hg(\\x_i)}; i = 1, ..., n\\) Fit a base learner (regression model, e.g. a regression tree), say \\(\\hg^b\\) to predict the negative gradients \\(-\\delta_1, ..., -\\delta_n\\) from \\(\\x_1, ..., \\x_n\\). Update \\(\\hg\\) to include a shrunken version of \\(\\hg^b\\): \\(\\hg \\gets \\hg + \\eta \\hg^b\\). Link Functions As in the case of generalised linear models, when performing gradient boosting we may require a link function \\(h\\) to connect the outputs of the function \\(\\hg\\) with the predictions for the mean of the response. In this case we evaluate the loss function on \\(y_i\\) and \\(h^{-1}(\\hg(\\x_i))\\), and not on \\(y_i\\) and \\(\\hg(\\x_i)\\). The same overall procedure is applied, however when computing the gradients \\(\\delta_i\\) with respect to \\(\\hg\\) we need to apply the chain rule in evaluating \\(\\delta_i = \\frac{\\partial}{\\partial \\hat y} L(y_i, h^{-1}(\\hat y))|_{\\hat y = \\hg(\\x_i)}\\). Gradient Boosting in R Gradient boosting can lead to extremely flexible models, and so careful tuning of both \\(\\eta\\) (sometimes called either the learning rate, or the shrinkage parameter) and \\(B\\) (the total number of boosting iterations), is vital. At a high level we may think of the product of \\(\\eta\\) and \\(B\\) as the total amount of learning which is achieved. However this does not always mean that halving \\(\\eta\\) and doubling \\(B\\) would lead to the same solution, nor the same quality of solution. The reason for this is that when \\(\\eta\\) is relatively large some of the learning is non-productive, since taking large steps in gradient based optimisation can overshoot and lead to oscillation around where good solutions lie. Generally speaking it is preferable from the point of view of accuracy, to use very small \\(\\eta\\) and larger \\(B\\). However this comes at the cost of increased computation since we need to fit \\(B\\) individual base learners. As a general rule of thumb for the standard gradient boosting algorithm setting \\(\\eta\\) to \\(0.1\\) or less is reasonable, and then tuning over \\(B\\) can be done using cross validation. One important point to note is that when tuning \\(B\\), if for example we want to consider models with both \\(100\\) and \\(200\\) base learners/boosting iterations then we dont have to fit two separate models. We can simply fit the model with \\(B=200\\) and then compare the solutions after the first \\(100\\) and \\(200\\). In fact we can compare the solutions after each iteration. However, the implementation in caret does not allow for this and the gbm package is preferable in this case. In addition to \\(\\eta\\) and \\(B\\), each of the base learners themselves will have their own tuning parameters. In the case of decision trees, we can tune any of the parameters which determine their complexity, such as their maximum depth and the minimum size of any of the leaf nodes. It is arguable that tuning all of these is overkill, however, and that only tuning \\(\\eta, B\\) and any one of the tuning parameters of the trees is sufficient. ### Load library library(caret) ### Set up to perform five fold cross validation ### As the data set is reasonably large five folds is very reasonable trControl &lt;- trainControl(method = &quot;cv&quot;, number = 5) ### We will only vary B (called n.trees) and tree depth (called interaction.depth) ### However as the gbm method has additional hyperparameters eta (called shrinkage) ### and minimum leaf size (called n.minobsinnode) we need to set fixed values for ### these as well tune_grid &lt;- expand.grid(n.trees = c(100, 200, 300), shrinkage = 0.1, n.minobsinnode = 10, interaction.depth = c(1, 2, 3)) ### We now tune and train the model as always gb_model &lt;- train(target~., data = satimage.tr, method = &#39;gbm&#39;, tuneGrid = tune_grid, trControl = trControl, verbose = FALSE) # stops printing of training progress ### We can inspect the results of the cross validation gb_model$results ## shrinkage interaction.depth n.minobsinnode n.trees Accuracy Kappa AccuracySD ## 1 0.1 1 10 100 0.8704567 0.8396749 0.005147918 ## 4 0.1 2 10 100 0.8848772 0.8576698 0.007625226 ## 7 0.1 3 10 100 0.8968583 0.8725216 0.009026825 ## 2 0.1 1 10 200 0.8779977 0.8492478 0.005164187 ## 5 0.1 2 10 200 0.8979637 0.8738788 0.007562932 ## 8 0.1 3 10 200 0.9024032 0.8793976 0.008225978 ## 3 0.1 1 10 300 0.8839891 0.8566643 0.008871559 ## 6 0.1 2 10 300 0.8997413 0.8760905 0.011100185 ## 9 0.1 3 10 300 0.9026225 0.8796597 0.006896950 ## KappaSD ## 1 0.006359900 ## 4 0.009347375 ## 7 0.011066750 ## 2 0.006369456 ## 5 0.009314294 ## 8 0.010157932 ## 3 0.010935541 ## 6 0.013643020 ## 9 0.008477130 ### Finally we can make predictions on the test set and the assess the ### test accuracy. We see that the performance is similar to the ### bagging models gb_pred &lt;- predict(gb_model, satimage.te) confusionMatrix(gb_pred, satimage.te$target)$overall[&#39;Accuracy&#39;] ## Accuracy ## 0.9060716 ### library(gbm) ### Using the gbm package directly allows us to perform cross validation over ### a broad range of values for B, since it tracks the validation error ### for each iteration ### It should be noted that the multinomial model in gbm has bugs, and a ### warning is printed to the screen. For a real modelling problem care should ### therefore be taken, and if you happen to love this implementation of ### gbm then you can always fit K-1 binary models as described in the ### section on generalised linear models, and combine the outputs for ### conducting multinomial regression. For illustrative purposes ### on how to use the package for tuning, however, we will accept the ### risk of inappropriate results gb_model2 &lt;- gbm(target~., data = satimage.tr, cv.folds = 5, n.trees = 300, interaction.depth = 6) ## Distribution not specified, assuming multinomial ... ### The predict method for gbm objects does not directly produce ### a class prediction, but rather the probabilities gb_predict &lt;- predict(gb_model2, satimage.te, type = &quot;response&quot;) ## Using 114 trees... ### To convert these to class predictions we can take the maximum ### probability, and to ensure the correct class label (factor level) ### is applied we can find these in the names of the columns gb_pred2 &lt;- as.factor(colnames(gb_predict[,,1])[apply(gb_predict[,,1], 1, which.max)]) ### Again we see the accuracy very similar to the other ensembles confusionMatrix(gb_pred2, satimage.te$target)$overall[&#39;Accuracy&#39;] ## Accuracy ## 0.9128179 10.2.1.1 eXtreme Gradient Boosting Because of its success gradient boosting has had a number of improvements, with one of the most successful being what is known as eXtreme Gradient Boosting (XGBoost). XGBoost uses a few principled modifications of the standard gradient boosting approach. These include the addition of a penalty to the training error in order to regularise the estimation and reduce the risk of overfitting, as well as a more sophisticated optimisation strategy which uses both the first and second functional gradients. Because of these XGBoost is often able to achieve equal or better accuracy than standard gradient boosting using a larger \\(\\eta\\) and smaller \\(B\\), and hence less overall computation. XGBoost in R XGBoost can be implemented using either the caret package or the xgboost package (as well as some others, which include variations on the standard XGBoost framework). Because of the additional options in terms of how the regularisation is applied, XGBoost has more tuning parameters than standard gradient boosting. We will only vary the learning rate \\(\\eta\\) and the number of iterations \\(B\\) (nrounds below), however details of the other tuning parameters can be accessed by using help(xgboost) after loading the xgboost library. ### Let&#39;s start by setting up our trainControl and tuning grid objects trControl &lt;- trainControl(method = &quot;cv&quot;, number = 5) tune_grid &lt;- expand.grid(nrounds = c(100, 200, 300), max_depth = 6, eta = c(0.1, 0.2, 0.3), gamma = 0, colsample_bytree = 0.8, subsample = 0.5, min_child_weight = 1) ### Now we can tune and train the model xgb_model &lt;- train(target~., data = satimage.tr, trControl = trControl, tuneGrid = tune_grid, method = &quot;xgbTree&quot;) ### The best tuning parameters are xgb_model$bestTune ## nrounds max_depth eta gamma colsample_bytree min_child_weight subsample ## 4 100 6 0.2 0 0.8 1 0.5 xgb_pred &lt;- predict(xgb_model, satimage.te) confusionMatrix(xgb_pred, satimage.te$target)$overall[&#39;Accuracy&#39;] ## Accuracy ## 0.9216399 Using the xgboost package directly requires that we do some of the hard-coding of objects and arguments ourselves. First, the data need to be provided as a matrix of covariates and a vector of labels, and the labels need to be numeric valued from \\(0\\) to \\(K-1\\). We also need to be specific about the loss function, where either the setting objective = \"multi:softmax\" or objective = \"multi:softprob\" can be used for multiclass classification. In addition cross validation is not implemented in the same way as in gbm, but we can use a simple validation approach by providing a so-called watchlist, which contains data sets on which evaluation should be performed during training. In this way xgboost is able to stop updating once the validation error stops improving, by providing an argument early_sopping_rounds. Specifically, if the validation error does not improve for early_stopping_rounds iterations then the optimisation terminates. ### Load the library library library(xgboost) ### Let&#39;s split the training data into 70% training and 30% validation valid_ix &lt;- createDataPartition(satimage.tr$target, p = 0.3, list = FALSE) X_tr &lt;- as.matrix(satimage.tr[-valid_ix,names(satimage.tr)!=&#39;target&#39;]) X_val &lt;- as.matrix(satimage.tr[valid_ix,names(satimage.tr)!=&#39;target&#39;]) ### Applying as.numeric(as.factor(vector)) will transform the values ### in a vector to the values 1, 2, ..., up to the number of unique ### entries. Since satimage.tr$target is already a factor we can ### use as.numeric(satimage.tr$target)-1 to convert the labels ### to 0, 1, ..., K-1 y_tr &lt;- as.numeric(satimage.tr$target[-valid_ix])-1 y_val &lt;- as.numeric(satimage.tr$target[valid_ix])-1 ### The function xgb.DMatrix allows us to create data objects which xgboost ### will be happy with train_data &lt;- xgb.DMatrix(X_tr, label = y_tr) val_data &lt;- xgb.DMatrix(X_val, label = y_val) watchlist &lt;- list(train = train_data, eval = val_data) xgb_validation &lt;- xgb.train(data = train_data, watchlist = watchlist, nrounds = 200, early_stopping_rounds = 10, num_class = 6, verbose = 0) ### We now need to train the final model using the number of rounds ### after which this model terminated, since this was based on the ### validation error, but we only used some of the training data to ### actually fit the model Bopt &lt;- xgb_validation$best_iteration xgb_model2 &lt;- xgb.train(data = xgb.DMatrix(rbind(X_tr, X_val), label = c(y_tr, y_val)), nrounds = Bopt, num_class = 6, verbose = 0) ### Finally we can predict on the test set, and convert to an appropriate ### factor variable to evaluate test performance xgb_pred2 &lt;- levels(satimage.te$target)[predict(xgb_model2, as.matrix(satimage.te[,names(satimage.te)!=&#39;target&#39;]))+1] confusionMatrix(as.factor(xgb_pred2), satimage.te$target)$overall[&#39;Accuracy&#39;] ## Accuracy ## 0.9107421 10.3 Variable Importance The ensemble nature of these models makes their predictions difficult to interpret. Model free diagnostics like Shapley Values and partial dependence plots are always an option. We will not go into these model free approaches, but rather briefly discuss the concept of variable importance. During the fitting of each decision tree in an ensemble models, each time a variable is used for splitting statistics on the improvement in the impurity due to the split can be tallied. Aggregating this information over the entire ensemble gives an indication of which are seen to be the more/less important variables for determining the predictions from the model. Although these do not give any indication of how modifying the value of one of the covariates will impact on its predicted value, they nonetheless allow us to gain a high level picture of the most important variables as they are captured by the models. 10.4 Summary Ensemble models combine the predictions from multiple (often very many) base learners in order to make a final prediction Ensemble models can be extremely accurate if well tuned, but are hard to interpret Bagging is a general purpose framework which averages/aggregates the predictions from base learners fit independently on bootstrap samples Bagging can have a remarkable effect in reducing the variance of highly flexible models Bagging inflexible models is typically not advised, and the highly flexible decision trees are by far the most popular Random Forests are a simple modification of bagged decision trees which add randomisation to the fitting procedure which decorrelates the predictions from different trees and typically leads to overall better prediction Out-of-bag statistics can significantly reduce the time needed to tune hyperparameters Boosting is a sequential procedure where base learners focus on improving specifically where the ensemble fit so far is less accurate Boosting is also a general purpose framework, but decision trees are the most popular base learners Each boosting iteration reduces the bias of the overall model since it improves the fit to the training data To combat the increase in variance, slow learning by using a small learning rate (\\(\\eta\\)) and adding randomness to the fitting of each tree can be beneficial 10.5 Exercises Write a function which will select values of mtry and nodesize for use within a Random Forest classification model. The function should select the pair which gives the best estimated AUC score based on Out-Of-Bag (OOB) estimates. In the output from a predictive model fit using the function rfsrc() the fields $predicted.oob (regression and classification) and $class.oob (classification) can be used to compute the OOB estimates of any chosen performance metric. Apply your function from Q 1. to the SAheart data set which we saw in the previous chapter. Use values of nodesize in 1:5 and of mtry in c(3, 6, 9). As always start by setting aside \\(30\\%\\) of the data for testing and use the other \\(70\\%\\) for training/model selection. Compare the test performance of the selected model with that obtained using the default settings for rfsrc(). Download and load the US_communities_and_crimes.RData file in the R Scripts folder in Moodle. There is a total of 127 covariates relating to socio-economic and law enforcement statistics, as well as some additional state and city information, pertaining to various cities in the USA, and the response variable is the per capita violent crimes from 1990 (variable 128). Details of the data set can be found at https://archive.ics.uci.edu/dataset/183/communities+and+crime. Set aside \\(30\\%\\) of the data for testing, and then fit two boosting based models on the remaining \\(70\\%\\). It is up to you how you choose to fit and select these two models. Compare their performance on the test set. "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
